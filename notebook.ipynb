{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn   \n",
    "from functorch import jacrev, vmap\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv,ResGatedGraphConv,SAGEConv, GatedGraphConv\n",
    "import torch.nn as nn \n",
    "from OnlineAdaptation.modules.vq_torch.vq_quantize import VectorQuantize\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(act_name):\n",
    "    if act_name == \"elu\":\n",
    "        return nn.ELU()\n",
    "    elif act_name == \"selu\":\n",
    "        return nn.SELU()\n",
    "    elif act_name == \"relu\":\n",
    "        return nn.ReLU()\n",
    "    elif act_name == \"crelu\":\n",
    "        return nn.ReLU()\n",
    "    elif act_name == \"lrelu\":\n",
    "        return nn.LeakyReLU()\n",
    "    elif act_name == \"tanh\":\n",
    "        return nn.Tanh()\n",
    "    elif act_name == \"sigmoid\":\n",
    "        return nn.Sigmoid()\n",
    "    elif act_name == \"identity\":\n",
    "        return nn.Identity()\n",
    "    else:\n",
    "        print(\"invalid activation function!\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_index = {\n",
    "    '0': [0,1,2,3,4,5,6,7,8,9,10,11,48,49,50,51],\n",
    "}\n",
    "\n",
    "hip_index = {\n",
    "    '1': [12,24,36,48],\n",
    "    '2': [15,27,39,49],\n",
    "    '3': [18,30,42,50],\n",
    "    '4': [21,33,45,51],\n",
    "}\n",
    "thigh_index = {\n",
    "    '5': [13,25,37,48],\n",
    "    '6': [16,28,40,49],\n",
    "    '7': [19,31,43,50],\n",
    "    '8': [22,34,46,51],\n",
    "}\n",
    "calf_index = {\n",
    "    '9': [14,26,38,48],\n",
    "    '10': [17,29,41,49],\n",
    "    '11': [20,32,44,50],\n",
    "    '12': [23,35,47,51],\n",
    "}\n",
    "\n",
    "edge_index = [\n",
    "    [0,1],[0,2],[0,3],[0,4],[0,5],[0,6],[0,7],[0,8],[0,9],[0,10],[0,11],[0,12],\n",
    "    [1,5],[5,9],\n",
    "    [2,6],[6,10],\n",
    "    [3,7],[7,11],\n",
    "    [4,8],[8,12],\n",
    "]\n",
    "\n",
    "def mlp(input_dim, out_dim, hidden_sizes, activations):\n",
    "    layers = []\n",
    "    prev_h = input_dim\n",
    "    for h in hidden_sizes:\n",
    "        layers.append(nn.Linear(prev_h, h))\n",
    "        layers.append(activations)\n",
    "        prev_h = h\n",
    "    layers.append(nn.Linear(prev_h, out_dim))\n",
    "    return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_obs,\n",
    "                 num_history,\n",
    "                 num_latent,\n",
    "                 activation = 'relu',):\n",
    "        super(GraphEncoder, self).__init__()\n",
    "        self.num_obs = num_obs\n",
    "        self.num_latent = num_latent\n",
    "        activation_fn = get_activation(activation)\n",
    "        # graph info\n",
    "        node_base = torch.tensor(list(body_index.values()),dtype=torch.long).squeeze()\n",
    "        node_hip = torch.stack([torch.tensor(list(hip_index.values()),dtype=torch.long)],dim=0).squeeze()\n",
    "        node_thigh = torch.stack([torch.tensor(list(thigh_index.values()),dtype=torch.long)],dim=0).squeeze()\n",
    "        node_calf = torch.stack([torch.tensor(list(calf_index.values()),dtype=torch.long)],dim=0).squeeze()\n",
    "\n",
    "        self.node_base = nn.Parameter(node_base, requires_grad=False)\n",
    "        self.node_hip = nn.Parameter(node_hip, requires_grad=False)\n",
    "        self.node_thigh = nn.Parameter(node_thigh, requires_grad=False)\n",
    "        self.node_calf = nn.Parameter(node_calf, requires_grad=False) \n",
    "\n",
    "        self.edge = torch.as_tensor(edge_index, dtype=torch.long).contiguous().t()\n",
    "\n",
    "        # build feature extractor for base, hip, thigh and calf\n",
    "        base_input_size = num_history * len(list(body_index.values())[0])\n",
    "        hip_input_size = num_history * len(list(hip_index.values())[0])\n",
    "        thigh_input_size = num_history * len(list(thigh_index.values())[0])\n",
    "        calf_input_size = num_history * len(list(calf_index.values())[0])\n",
    "        self.base_net = mlp(base_input_size, 2 * num_latent, [128, 64], activation_fn)\n",
    "        self.hip_net = mlp(hip_input_size, 2 * num_latent, [128, 64], activation_fn)\n",
    "        self.thigh_net = mlp(thigh_input_size, 2 * num_latent, [128, 64], activation_fn)\n",
    "        self.calf_net = mlp(calf_input_size, 2* num_latent, [128, 64], activation_fn)\n",
    "\n",
    "        # build graph net \n",
    "        self.gn = ResGatedGraphConv(in_channels= 2* num_latent,out_channels=2* num_latent)\n",
    "        self.gn2 = ResGatedGraphConv(in_channels= 2* num_latent,out_channels= num_latent)\n",
    "        self.act = activation_fn\n",
    "    \n",
    "    def _history2node(self,obs_history):\n",
    "        # obs_history.shape = (bz, n_histroy, n_obs)\n",
    "        base = obs_history[:,:,self.node_base].unsqueeze(1) # (bz, n_history, n_base)\n",
    "        hip = obs_history[:,:,self.node_hip].permute(0,2,1,3) # (bz, n_history, 4, 4)\n",
    "        thigh = obs_history[:,:,self.node_thigh].permute(0,2,1,3)\n",
    "        calf = obs_history[:,:,self.node_calf].permute(0,2,1,3) \n",
    "        base = self.base_net(base.flatten(-2,-1))\n",
    "        hip = self.hip_net(hip.flatten(-2,-1))\n",
    "        thigh = self.thigh_net(thigh.flatten(-2,-1))\n",
    "        calf = self.calf_net(calf.flatten(-2,-1))\n",
    "        return torch.cat([base,hip,thigh,calf],dim=1) # (bz, n_node,num_latent)\n",
    "    \n",
    "    def forward(self,obs_history):\n",
    "        nodes = self._history2node(obs_history)\n",
    "        nodes = self.gn(nodes,self.edge)\n",
    "        nodes = self.act(nodes)\n",
    "        nodes = self.gn2(nodes,self.edge)\n",
    "        return nodes\n",
    "\n",
    "    \n",
    "class GraphActor(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_obs,\n",
    "                 num_latent,\n",
    "                 num_actions,\n",
    "                 activation = 'elu',\n",
    "                 actor_hidden_dims = [512, 256, 128]):\n",
    "        super().__init__()\n",
    "        self.num_latent = num_latent\n",
    "        # graph info\n",
    "        node_base = torch.tensor(list(body_index.values()),dtype=torch.long).squeeze()\n",
    "        node_hip = torch.stack([torch.tensor(list(hip_index.values()),dtype=torch.long)],dim=0).squeeze()\n",
    "        node_thigh = torch.stack([torch.tensor(list(thigh_index.values()),dtype=torch.long)],dim=0).squeeze()\n",
    "        node_calf = torch.stack([torch.tensor(list(calf_index.values()),dtype=torch.long)],dim=0).squeeze()\n",
    "        activation_fn = get_activation(activation)\n",
    "        self.node_base = nn.Parameter(node_base, requires_grad=False)\n",
    "        self.node_hip = nn.Parameter(node_hip, requires_grad=False)\n",
    "        self.node_thigh = nn.Parameter(node_thigh, requires_grad=False)\n",
    "        self.node_calf = nn.Parameter(node_calf, requires_grad=False) \n",
    "        self.edge = torch.as_tensor(edge_index, dtype=torch.long).contiguous().t()\n",
    "        # Pipeline\n",
    "        # obs-> node, concat with latent -> node latent\n",
    "        # node latent -> node level policy -> node action\n",
    "        base_input_size =  len(list(body_index.values())[0])\n",
    "        hip_input_size = len(list(hip_index.values())[0])\n",
    "        thigh_input_size =len(list(thigh_index.values())[0])\n",
    "        calf_input_size = len(list(calf_index.values())[0])\n",
    "        self.base_net = mlp(base_input_size, num_latent, [128], activation_fn)\n",
    "        self.hip_net = mlp(hip_input_size, num_latent, [128], activation_fn)\n",
    "        self.thigh_net = mlp(thigh_input_size, num_latent, [128], activation_fn)\n",
    "        self.calf_net = mlp(calf_input_size, num_latent, [128], activation_fn)\n",
    "\n",
    "        # graph neural network \n",
    "        self.gn = ResGatedGraphConv(in_channels=2 * num_latent,\n",
    "                                    out_channels=2 * num_latent)\n",
    "        self.act = activation_fn\n",
    "        self.gn2 = ResGatedGraphConv(in_channels=2 * num_latent,\n",
    "                                    out_channels=num_latent)\n",
    "        \n",
    "        # graph policy : (base, hip, thigh, calf) -> leg_control \n",
    "        self.leg_policy = mlp(4*num_latent, 3, [256,128], activation_fn)\n",
    "        self.FL_Leg = nn.Parameter(torch.tensor([0,1,5,9],dtype=torch.long), requires_grad=False)\n",
    "        self.FR_Leg = nn.Parameter(torch.tensor([0,2,6,10],dtype=torch.long), requires_grad=False)\n",
    "        self.RL_Leg = nn.Parameter(torch.tensor([0,3,7,11],dtype=torch.long), requires_grad=False)\n",
    "        self.RR_Leg = nn.Parameter(torch.tensor([0,4,8,12],dtype=torch.long), requires_grad=False)\n",
    "\n",
    "    def _obs2node(self, obs):\n",
    "        # obs.shape = (bz, n_obs)\n",
    "        base = obs[:,self.node_base].unsqueeze(1) # (bz, 1, n_base)\n",
    "        hip = obs[:,self.node_hip]# (bz, 4, 4)\n",
    "        thigh = obs[:,self.node_thigh]\n",
    "        calf = obs[:,self.node_calf]\n",
    "        base = self.base_net(base)\n",
    "        hip = self.hip_net(hip)\n",
    "        thigh = self.thigh_net(thigh)\n",
    "        calf = self.calf_net(calf)\n",
    "        return torch.cat([base,hip,thigh,calf],dim=1) # (bz, n_node,num_latent)\n",
    "    def forward(self, obs, latent):\n",
    "        # obs.shape = (bz, n_obs)\n",
    "        # latent.shape = (bz,n_node, num_latent)\n",
    "        obs_nodes = self._obs2node(obs) # (bz, n_node, num_latent) \n",
    "        nodes_latent = torch.cat([obs_nodes,latent],dim=-1) # (bz, n_node, 2*num_latent)\n",
    "        nodes_latent = self.gn(nodes_latent, self.edge) # (bz, n_node, 2*num_latent)\n",
    "        nodes_latent = self.act(nodes_latent)\n",
    "        nodes_latent = self.gn2(nodes_latent, self.edge) # (bz, n_node, num_latent) \n",
    "        FL_Leg_latent = nodes_latent[:,self.FL_Leg,:].reshape(-1,4*self.num_latent)\n",
    "        FR_Leg_latent = nodes_latent[:,self.FR_Leg,:].reshape(-1,4*self.num_latent)\n",
    "        RL_Leg_latent = nodes_latent[:,self.RL_Leg,:].reshape(-1,4*self.num_latent)\n",
    "        RR_Leg_latent = nodes_latent[:,self.RR_Leg,:].reshape(-1,4*self.num_latent)\n",
    "        FL_Leg_action = self.leg_policy(FL_Leg_latent)\n",
    "        FR_Leg_action = self.leg_policy(FR_Leg_latent)\n",
    "        RL_Leg_action = self.leg_policy(RL_Leg_latent)\n",
    "        RR_Leg_action = self.leg_policy(RR_Leg_latent)\n",
    "        return torch.cat([FL_Leg_action,FR_Leg_action,RL_Leg_action,RR_Leg_action],dim=1) # (bz, 12)\n",
    "\n",
    "class GraphForward(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_obs,\n",
    "                 num_latent,\n",
    "                 num_actions,\n",
    "                 activation = 'elu',\n",
    "                 actor_hidden_dims = [512, 256, 128]):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        obs + action + latent -> next_obs \n",
    "        obs -> node \n",
    "        action -> node\n",
    "        \"\"\"\n",
    "        self.num_latent = num_latent\n",
    "        # graph info\n",
    "        node_base = torch.tensor(list(body_index.values()),dtype=torch.long).squeeze()\n",
    "        node_hip = torch.stack([torch.tensor(list(hip_index.values()),dtype=torch.long)],dim=0).squeeze()\n",
    "        node_thigh = torch.stack([torch.tensor(list(thigh_index.values()),dtype=torch.long)],dim=0).squeeze()\n",
    "        node_calf = torch.stack([torch.tensor(list(calf_index.values()),dtype=torch.long)],dim=0).squeeze()\n",
    "        \n",
    "        node_hip_action = torch.tensor([0, 3, 6, 9],dtype=torch.long)\n",
    "        node_thigh_action = torch.tensor([1, 4, 7, 10],dtype=torch.long)\n",
    "        node_calf_action = torch.tensor([2, 5, 8, 11],dtype=torch.long)\n",
    "        self.node_hip_action = nn.Parameter(node_hip_action, requires_grad=False)\n",
    "        self.node_thigh_action = nn.Parameter(node_thigh_action, requires_grad=False)\n",
    "        self.node_calf_action = nn.Parameter(node_calf_action, requires_grad=False)\n",
    "\n",
    "        activation_fn = get_activation(activation)\n",
    "        self.node_base = nn.Parameter(node_base, requires_grad=False)\n",
    "        self.node_hip = nn.Parameter(node_hip, requires_grad=False)\n",
    "        self.node_thigh = nn.Parameter(node_thigh, requires_grad=False)\n",
    "        self.node_calf = nn.Parameter(node_calf, requires_grad=False) \n",
    "        self.edge = nn.Parameter(torch.as_tensor(edge_index, dtype=torch.long).contiguous().t(),requires_grad=False)\n",
    "        # pipeline \n",
    "        base_input_size =  len(list(body_index.values())[0])\n",
    "        hip_input_size = len(list(hip_index.values())[0]) + 1\n",
    "        thigh_input_size =len(list(thigh_index.values())[0]) + 1 \n",
    "        calf_input_size = len(list(calf_index.values())[0]) + 1 \n",
    "        self.base_net = mlp(base_input_size, num_latent, [128], activation_fn)\n",
    "        self.hip_net = mlp(hip_input_size, num_latent, [128], activation_fn)\n",
    "        self.thigh_net = mlp(thigh_input_size, num_latent, [128], activation_fn)\n",
    "        self.calf_net = mlp(calf_input_size, num_latent, [128], activation_fn)\n",
    "        # graph neural network \n",
    "        self.gn = ResGatedGraphConv(in_channels=2 * num_latent,\n",
    "                                    out_channels=2 * num_latent)\n",
    "        self.act = activation_fn\n",
    "        self.gn2 = ResGatedGraphConv(in_channels=2 * num_latent,\n",
    "                                    out_channels=num_latent)\n",
    "        # decoder \n",
    "        ## base_vel, base_ang, project_gravity, cmd\n",
    "        ## dof_pos,vel,actions, contact\n",
    "        self.base_decoder = mlp(num_latent, 3 + 3 + 3 + 3, [128], activation_fn)\n",
    "        self.leg_decoder = mlp(num_latent * 4 , 3 + 3 + 3 + 1, [128], activation_fn)\n",
    "        self.FL_Leg = nn.Parameter(torch.tensor([0,1,5,9],dtype=torch.long), requires_grad=False)\n",
    "        self.FR_Leg = nn.Parameter(torch.tensor([0,2,6,10],dtype=torch.long), requires_grad=False)\n",
    "        self.RL_Leg = nn.Parameter(torch.tensor([0,3,7,11],dtype=torch.long), requires_grad=False)\n",
    "        self.RR_Leg = nn.Parameter(torch.tensor([0,4,8,12],dtype=torch.long), requires_grad=False)\n",
    "\n",
    "\n",
    "    def _obsaction2node(self,obs,action):\n",
    "        base = obs[:,self.node_base].unsqueeze(1) # (bz, 1, n_base)\n",
    "        hip = obs[:,self.node_hip]# (bz, 4, 4)\n",
    "        thigh = obs[:,self.node_thigh]\n",
    "        calf = obs[:,self.node_calf]\n",
    "        hip_action = action[:,self.node_hip_action].unsqueeze(-1)\n",
    "        thigh_action = action[:,self.node_thigh_action].unsqueeze(-1)\n",
    "        calf_action = action[:,self.node_calf_action].unsqueeze(-1)\n",
    "        base = self.base_net(base)\n",
    "        hip = self.hip_net(torch.cat([hip,hip_action],dim=-1))\n",
    "        thigh = self.hip_net(torch.cat([thigh,thigh_action],dim=-1))\n",
    "        calf = self.calf_net(torch.cat([calf,calf_action],dim=-1))\n",
    "        node = torch.cat([base,hip,thigh,calf],dim=1)\n",
    "        return node # shape (bz, 13, 4)\n",
    "\n",
    "    def forward(self,obs,action,latent):\n",
    "        obsaction_node = self._obsaction2node(obs,action) \n",
    "        nodes_latent = torch.cat([obsaction_node,latent],dim=-1) # (bz, n_node, 2*num_latent)\n",
    "        nodes_latent = self.gn(nodes_latent, self.edge) # (bz, n_node, 2*num_latent)\n",
    "        nodes_latent = self.act(nodes_latent)\n",
    "        nodes_latent = self.gn2(nodes_latent, self.edge) # (bz, n_node, num_latent) \n",
    "\n",
    "        Base_latent = nodes_latent[:,0:1,:].reshape(-1,self.num_latent)\n",
    "        FL_Leg_latent = nodes_latent[:,self.FL_Leg,:].reshape(-1,4*self.num_latent)\n",
    "        FR_Leg_latent = nodes_latent[:,self.FR_Leg,:].reshape(-1,4*self.num_latent)\n",
    "        RL_Leg_latent = nodes_latent[:,self.RL_Leg,:].reshape(-1,4*self.num_latent)\n",
    "        RR_Leg_latent = nodes_latent[:,self.RR_Leg,:].reshape(-1,4*self.num_latent)\n",
    "\n",
    "        base_decoded = self.base_decoder(Base_latent) # (bz,12)\n",
    "        FL_Leg_decoded = self.leg_decoder(FL_Leg_latent) # (bz, 4) \n",
    "        FR_Leg_decoded = self.leg_decoder(FR_Leg_latent)\n",
    "        RL_Leg_decoded = self.leg_decoder(RL_Leg_latent)\n",
    "        RR_Leg_decoded = self.leg_decoder(RR_Leg_latent)\n",
    "        decoded_pos = torch.cat([FL_Leg_decoded[:,0:3],FR_Leg_decoded[:,0:3],RL_Leg_decoded[:,0:3],RR_Leg_decoded[:,0:3]],dim=-1) # (bz,12)\n",
    "        decoded_vel = torch.cat([FL_Leg_decoded[:,3:6],FR_Leg_decoded[:,3:6],RL_Leg_decoded[:,3:6],RR_Leg_decoded[:,3:6]],dim=-1)\n",
    "        decoded_act = torch.cat([FL_Leg_decoded[:,6:9],FR_Leg_decoded[:,6:9],RL_Leg_decoded[:,6:9],RR_Leg_decoded[:,6:9]],dim=-1)\n",
    "        decoded_contact = torch.cat([FL_Leg_decoded[:,9:10],FR_Leg_decoded[:,9:10],RL_Leg_decoded[:,9:10],RR_Leg_decoded[:,9:10]],dim=-1)\n",
    "        decoded = torch.cat((base_decoded,decoded_pos,decoded_vel,decoded_act,decoded_contact),dim=-1)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_history = torch.randn(2,10,52)\n",
    "obs = torch.randn(2,52)\n",
    "\n",
    "obs_history_dim = 10 * 52 \n",
    "\n",
    "vq = VectorQuantize(\n",
    "    dim = 256,\n",
    "    codebook_dim = 32,                  # a number of papers have shown smaller codebook dimension to be acceptable\n",
    "    heads = 8,                          # number of heads to vector quantize, codebook shared across all heads\n",
    "    separate_codebook_per_head = True,  # whether to have a separate codebook per head. False would mean 1 shared codebook\n",
    "    codebook_size = 32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VQ Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as distributed\n",
    "from torch.optim import Optimizer\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from einops import rearrange, repeat, reduce, pack, unpack\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "def batched_embedding(indices, embeds):\n",
    "    batch, dim = indices.shape[1], embeds.shape[-1]\n",
    "    indices = repeat(indices, 'h b n -> h b n d', d = dim)\n",
    "    embeds = repeat(embeds, 'h c d -> h b c d', b = batch)\n",
    "    return embeds.gather(2, indices)\n",
    "\n",
    "def ema_inplace(old, new, decay):\n",
    "    old.mul_(decay).add_(new * (1 - decay))\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def uniform_init(*shape):\n",
    "    t = torch.empty(shape)\n",
    "    nn.init.kaiming_uniform_(t)\n",
    "    return t\n",
    "def pack_one(t, pattern):\n",
    "    return pack([t], pattern)\n",
    "\n",
    "def unpack_one(t, ps, pattern):\n",
    "    return unpack(t, ps, pattern)[0]\n",
    "def l2norm(t):\n",
    "    return F.normalize(t, p = 2, dim = -1)\n",
    "def sample_vectors(samples, num):\n",
    "    num_samples, device = samples.shape[0], samples.device\n",
    "    if num_samples >= num:\n",
    "        indices = torch.randperm(num_samples, device = device)[:num]\n",
    "    else:\n",
    "        indices = torch.randint(0, num_samples, (num,), device = device)\n",
    "\n",
    "    return samples[indices]\n",
    "\n",
    "def batched_sample_vectors(samples, num):\n",
    "    return torch.stack([sample_vectors(sample, num) for sample in samples.unbind(dim = 0)], dim = 0)\n",
    "\n",
    "def orthogonal_loss_fn(t):\n",
    "    # eq (2) from https://arxiv.org/abs/2112.00384\n",
    "    #! t.shape (n_book, n_codebook, dim)\n",
    "    h, n = t.shape[:2]\n",
    "    normed_codes = l2norm(t)\n",
    "    cosine_sim = einsum('h i d, h j d -> h i j', normed_codes, normed_codes)\n",
    "    return (cosine_sim ** 2).sum() / (h * n ** 2) - (1 / n)\n",
    "\n",
    "def orthogonal_loss_fn_with_mask(t, mask):\n",
    "    #! t.shape (n_book, n_codebook, dim)\n",
    "    #! mask.shape (n_book, n_codebook)\n",
    "    # perform othogonal loss on masked codes where mask==1\n",
    "    h, n = t.shape[:2]\n",
    "    normed_codes = l2norm(t)\n",
    "    cosine_sim = einsum('h i d, h j d -> h i j', normed_codes, normed_codes)\n",
    "    print(\"Check: \", cosine_sim, mask)\n",
    "    cosine_sim = cosine_sim * mask.unsqueeze(-1) * mask.unsqueeze(-2)\n",
    "    print(\"Check: \", cosine_sim)\n",
    "    n_effective = mask.sum(dim = -1)[0]\n",
    "    print(\"Check: \", n_effective)\n",
    "    return (cosine_sim ** 2).sum() / (h * n_effective ** 2) - (1 / n_effective)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def cdist(x, y):\n",
    "    x2 = reduce(x ** 2, 'b n d -> b n', 'sum')\n",
    "    y2 = reduce(y ** 2, 'b n d -> b n', 'sum')\n",
    "    xy = einsum('b i d, b j d -> b i j', x, y) * -2\n",
    "    return (rearrange(x2, 'b i -> b i 1') + rearrange(y2, 'b j -> b 1 j') + xy).sqrt()\n",
    "def log(t, eps = 1e-20):\n",
    "    return torch.log(t.clamp(min = eps))\n",
    "\n",
    "def gumbel_noise(t):\n",
    "    noise = torch.zeros_like(t).uniform_(0, 1)\n",
    "    return -log(-log(noise))\n",
    "\n",
    "def laplace_smoothing(x, n_categories, eps = 1e-5, dim = -1):\n",
    "    # x.shape = (n_book, n_code)\n",
    "    denom = x.sum(dim = dim, keepdim = True)\n",
    "    return (x + eps) / (denom + n_categories * eps) # (n_book,n_code), n_code 的概率\n",
    "\n",
    "def simple_sample(logits,mask,temperature=1,deterministic=True,dim = -1):\n",
    "    # loigits.shape = (n_head, bz, n_codebook)\n",
    "    # mask.shape = (n_head, n_codebook)\n",
    "    # sample logits with argmax from the mask==1 \n",
    "    dtype, size = logits.dtype, logits.shape[dim]\n",
    "    if deterministic:\n",
    "        sampling_logits = logits\n",
    "    else:\n",
    "        sampling_logits = (logits / temperature) + gumbel_noise(logits)\n",
    "    if mask is not None:\n",
    "        mask = repeat(mask, 'h c -> h b c', b = logits.shape[1])\n",
    "        sampling_logits[~mask] = -1e10\n",
    "    ind = sampling_logits.argmax(dim = dim)\n",
    "    one_hot = F.one_hot(ind, size).type(dtype)\n",
    "    return ind, one_hot\n",
    "\n",
    "def gumbel_sample(\n",
    "    logits,\n",
    "    temperature = 1.,\n",
    "    stochastic = False,\n",
    "    straight_through = False,\n",
    "    reinmax = False,\n",
    "    dim = -1,\n",
    "    training = True\n",
    "):\n",
    "    dtype, size = logits.dtype, logits.shape[dim]\n",
    "\n",
    "    if training and stochastic and temperature > 0:\n",
    "        sampling_logits = (logits / temperature) + gumbel_noise(logits)\n",
    "    else:\n",
    "        sampling_logits = logits\n",
    "\n",
    "    ind = sampling_logits.argmax(dim = dim)\n",
    "    one_hot = F.one_hot(ind, size).type(dtype)\n",
    "\n",
    "    assert not (reinmax and not straight_through), 'reinmax can only be turned on if using straight through gumbel softmax'\n",
    "\n",
    "    if not straight_through or temperature <= 0. or not training:\n",
    "        return ind, one_hot\n",
    "\n",
    "    # use reinmax for better second-order accuracy - https://arxiv.org/abs/2304.08612\n",
    "    # algorithm 2\n",
    "\n",
    "    if reinmax:\n",
    "        π0 = logits.softmax(dim = dim)\n",
    "        π1 = (one_hot + (logits / temperature).softmax(dim = dim)) / 2\n",
    "        π1 = ((log(π1) - logits).detach() + logits).softmax(dim = 1)\n",
    "        π2 = 2 * π1 - 0.5 * π0\n",
    "        one_hot = π2 - π2.detach() + one_hot\n",
    "    else:\n",
    "        π1 = (logits / temperature).softmax(dim = dim)\n",
    "        one_hot = one_hot + π1 - π1.detach()\n",
    "\n",
    "    return ind, one_hot\n",
    "class CodeBook(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim, # 这个 dim 就是分化好的\n",
    "                 num_codebooks,\n",
    "                 codebook_size,\n",
    "                    ema_update = True,\n",
    "                    decay = 0.8,\n",
    "                    eps = 1e-5,\n",
    "                    threshold_ema_dead_code = 2\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        self.codebook_size = codebook_size\n",
    "        self.num_codebooks = num_codebooks\n",
    "        init_fn = uniform_init # 可能可以尝试有 pretrain 的初始化 \n",
    "        self.gumbel_sample = gumbel_sample\n",
    "        embed = init_fn(num_codebooks,codebook_size, dim)\n",
    "        self.sample_codebook_temp = 1.0\n",
    "        self.ema_update = ema_update\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "        self.threshold_ema_dead_code = threshold_ema_dead_code \n",
    "        self.reset_cluster_size = threshold_ema_dead_code\n",
    "        self.sample_fn = batched_sample_vectors\n",
    "\n",
    "        #! 是不是统计一下每个 embed 的选取个数, 可以查看 dead code\n",
    "        self.register_buffer('cluster_size', torch.zeros(num_codebooks, codebook_size))\n",
    "        self.register_buffer('embed_avg', embed.clone())\n",
    "        self.embed = nn.Parameter(embed)\n",
    "\n",
    "        #! 算法需求\n",
    "        self.register_buffer('batch_mean', None)\n",
    "        self.register_buffer('batch_variance', None)\n",
    "\n",
    "        self.register_buffer('codebook_mean_needs_init', torch.Tensor([True]))\n",
    "        self.register_buffer('codebook_mean', torch.empty(num_codebooks, 1, dim))\n",
    "        self.register_buffer('codebook_variance_needs_init', torch.Tensor([True]))\n",
    "        self.register_buffer('codebook_variance', torch.empty(num_codebooks, 1, dim))\n",
    "    \n",
    "    def replace(self, batch_samples, batch_mask):\n",
    "        for ind, (samples, mask) in enumerate(zip(batch_samples.unbind(dim = 0), batch_mask.unbind(dim = 0))):\n",
    "            if not torch.any(mask):\n",
    "                continue\n",
    "\n",
    "            sampled = self.sample_fn(rearrange(samples, '... -> 1 ...'), mask.sum().item())\n",
    "            sampled = rearrange(sampled, '1 ... -> ...')\n",
    "            \n",
    "            self.embed.data[ind][mask] = sampled\n",
    "\n",
    "            self.cluster_size.data[ind][mask] = self.reset_cluster_size\n",
    "            self.embed_avg.data[ind][mask] = sampled * self.reset_cluster_size\n",
    "\n",
    "    def expire_codes_(self, batch_samples):\n",
    "        if self.threshold_ema_dead_code == 0:\n",
    "            return\n",
    "\n",
    "        expired_codes = self.cluster_size < self.threshold_ema_dead_code\n",
    "\n",
    "        if not torch.any(expired_codes):\n",
    "            return\n",
    "\n",
    "        batch_samples = rearrange(batch_samples, 'h ... d -> h (...) d')\n",
    "        self.replace(batch_samples, batch_mask = expired_codes)\n",
    "\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def update_with_decay(self, buffer_name, new_value, decay):\n",
    "        old_value = getattr(self, buffer_name)\n",
    "\n",
    "        needs_init = getattr(self, buffer_name + \"_needs_init\", False)\n",
    "\n",
    "        if needs_init:\n",
    "            self.register_buffer(buffer_name + \"_needs_init\", torch.Tensor([False]))\n",
    "\n",
    "        if not exists(old_value) or needs_init:\n",
    "            self.register_buffer(buffer_name, new_value.detach())\n",
    "\n",
    "            return\n",
    "\n",
    "        value = old_value * decay + new_value.detach() * (1 - decay)\n",
    "        self.register_buffer(buffer_name, value)\n",
    "    \n",
    "    @autocast(enabled = False)\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        sample_codebook_temp = None,\n",
    "        mask = None,\n",
    "        freeze_codebook = False\n",
    "    ):\n",
    "        # x.shape = 'h b n d', head 数量前置, b 是 batch size, n 感觉是辅助的 ?? d 是 dim\n",
    "        needs_codebook_dim = x.ndim < 4\n",
    "        x = x.float()\n",
    "        if needs_codebook_dim:\n",
    "            x = rearrange(x, 'h ... -> h 1 ...')\n",
    "        flatten,ps = pack_one(x,'h * d') # 这步的操作类似 从 h,b,n,d -> h, b*n,d\n",
    "        embed = self.embed\n",
    "        dist = -cdist(flatten, embed) # 越大相似度越高 \n",
    "\n",
    "        embed_ind, embed_onehot = self.gumbel_sample(dist, dim = -1, temperature = self.sample_codebook_temp, training = self.training)\n",
    "        embed_ind = unpack_one(embed_ind, ps, 'h *')\n",
    "        if self.training:\n",
    "            unpacked_onehot = unpack_one(embed_onehot, ps, 'h * c')\n",
    "            quantize = einsum('h b n c, h c d -> h b n d', unpacked_onehot, embed)\n",
    "        else:\n",
    "            quantize = batched_embedding(embed_ind, embed)\n",
    "\n",
    "        \n",
    "        if self.training and self.ema_update:\n",
    "            cluster_size = embed_onehot.sum(dim = 1) \n",
    "            ema_inplace(self.cluster_size, cluster_size, self.decay)\n",
    "            embed_sum = einsum('h n d, h n c -> h c d', flatten, embed_onehot) # 选择出来哪些 emb 被选中了 \n",
    "            ema_inplace(self.embed_avg, embed_sum, self.decay)\n",
    "            cluster_size = laplace_smoothing(self.cluster_size, self.codebook_size, self.eps) * self.cluster_size.sum(dim = -1, keepdim = True)\n",
    "            embed_normalized = self.embed_avg / rearrange(cluster_size, '... -> ... 1')\n",
    "            self.embed.data.copy_(embed_normalized)\n",
    "            self.expire_codes_(x)\n",
    "        \n",
    "        dist = unpack_one(dist, ps, 'h * d')\n",
    "        if needs_codebook_dim:\n",
    "            quantize = rearrange(quantize, 'h 1 n d -> h n d')\n",
    "            dist = rearrange(dist, 'h 1 n d -> h n d')\n",
    "            embed_ind = rearrange(embed_ind, 'h 1 n -> h n')\n",
    "        return quantize, embed_ind, dist\n",
    "        \n",
    "class SimpleCodeBook(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim, # 这个 dim 就是分化好的\n",
    "                 num_codebooks,\n",
    "                 codebook_size,\n",
    "                    ema_update = True,\n",
    "                    decay = 0.8,\n",
    "                    eps = 1e-5,\n",
    "                    threshold_ema_dead_code = 2\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        self.codebook_size = codebook_size\n",
    "        self.num_codebooks = num_codebooks\n",
    "        init_fn = uniform_init # 可能可以尝试有 pretrain 的初始化 \n",
    "        self.sample_method = simple_sample\n",
    "        embed = init_fn(num_codebooks,codebook_size, dim)\n",
    "        self.sample_codebook_temp = 1.0\n",
    "        self.ema_update = ema_update\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "        self.sample_fn = batched_sample_vectors\n",
    "\n",
    "        #! 是不是统计一下每个 embed 的选取个数, 可以查看 dead code\n",
    "        self.register_buffer('cluster_size', torch.zeros(num_codebooks, codebook_size))\n",
    "        self.register_buffer('valid_codebook', torch.ones(num_codebooks, codebook_size,dtype=torch.bool))\n",
    "        self.register_buffer('embed_avg', embed.clone())\n",
    "        self.embed = nn.Parameter(embed)\n",
    "\n",
    "        #! 算法需求\n",
    "        self.register_buffer('batch_mean', None)\n",
    "        self.register_buffer('batch_variance', None)\n",
    "\n",
    "        self.register_buffer('codebook_mean_needs_init', torch.Tensor([True]))\n",
    "        self.register_buffer('codebook_mean', torch.empty(num_codebooks, 1, dim))\n",
    "        self.register_buffer('codebook_variance_needs_init', torch.Tensor([True]))\n",
    "        self.register_buffer('codebook_variance', torch.empty(num_codebooks, 1, dim))\n",
    "    \n",
    "    def unmask_all(self):\n",
    "        self.valid_codebook.data.copy_(torch.ones_like(self.valid_codebook,dtype=torch.bool))\n",
    "    def mask_percentage(self,percentage):\n",
    "        assert percentage >= 0 and percentage <= 1\n",
    "        valid_num = int(self.codebook_size * (1-percentage))\n",
    "        self.valid_codebook.data.copy_(torch.ones_like(self.valid_codebook,dtype=torch.bool))\n",
    "        self.valid_codebook.data[:,valid_num:] = 0\n",
    "    def random_mask(self):\n",
    "        # 对每个头的 codebook 随机 mask 一些\n",
    "        mask = torch.randint(low=0,high = 2, size = (self.codebook_size,),dtype=torch.bool)\n",
    "        self.valid_codebook.data[:,mask] = 0\n",
    "    @torch.jit.ignore\n",
    "    def update_with_decay(self, buffer_name, new_value, decay):\n",
    "        old_value = getattr(self, buffer_name)\n",
    "\n",
    "        needs_init = getattr(self, buffer_name + \"_needs_init\", False)\n",
    "\n",
    "        if needs_init:\n",
    "            self.register_buffer(buffer_name + \"_needs_init\", torch.Tensor([False]))\n",
    "\n",
    "        if not exists(old_value) or needs_init:\n",
    "            self.register_buffer(buffer_name, new_value.detach())\n",
    "\n",
    "            return\n",
    "\n",
    "        value = old_value * decay + new_value.detach() * (1 - decay)\n",
    "        self.register_buffer(buffer_name, value)\n",
    "    \n",
    "    @autocast(enabled = False)\n",
    "    def forward(\n",
    "        self,\n",
    "        x\n",
    "    ):\n",
    "        # x.shape = 'h b d', head 数量前置, b 是 batch size, d 是 dim\n",
    "        x = x.float()\n",
    "        \n",
    "        embed = self.embed\n",
    "        dist = -cdist(x, embed) # 越大相似度越高 \n",
    "\n",
    "        embed_ind, embed_onehot = self.sample_method(logits = dist,\n",
    "                                                     mask = self.valid_codebook,\n",
    "                                                     temperature=1.0,\n",
    "                                                     deterministic=not self.training,\n",
    "                                                     dim = -1)\n",
    "        if self.training:\n",
    "            quantize = einsum('h b c, h c d -> h b d', embed_onehot, embed)\n",
    "        else:\n",
    "            batch, dim = embed_ind.shape[1], embed.shape[-1]\n",
    "            indices = repeat(embed_ind, 'h b -> h b d', d = dim)\n",
    "            repeated_embeds = repeat(embed, 'h c d -> h b c d', b = batch)\n",
    "            quantize = repeated_embeds.gather(2, indices)\n",
    "\n",
    "        \n",
    "        if self.training and self.ema_update:\n",
    "            #! onehot.shape = (n_book,bz,n_code)-> n_code 激活的数量\n",
    "            cluster_size = embed_onehot.sum(dim = 1) \n",
    "            ema_inplace(self.cluster_size, cluster_size, self.decay) #! 统计每个 code 的激活数量均值\n",
    "            embed_sum = einsum('h b d, h b c -> h c d', x, embed_onehot) # 选择出来哪些 emb 被选中了 \n",
    "            ema_inplace(self.embed_avg, embed_sum, self.decay) #! 统计用来激活 code 的emb 原来长什么样\n",
    "\n",
    "            #! 这个平滑挺有趣的, 因为有的 code 没有被采样到,但是不能直接 + 1, 所以先算概率(Laplace平滑), 再乘上总数, 相当于略微的平分\n",
    "            cluster_size = laplace_smoothing(self.cluster_size, self.codebook_size, self.eps) * self.cluster_size.sum(dim = -1, keepdim = True)\n",
    "            #! 这一步是缓慢更新 emb\n",
    "            embed_normalized = self.embed_avg / rearrange(cluster_size, '... -> ... 1')\n",
    "            self.embed.data.copy_(embed_normalized)\n",
    "        \n",
    "        return quantize, embed_ind, dist\n",
    "\n",
    "class VectorQuantize(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim, n_head, codebook_size,\n",
    "                 commitment_weight = 1.,\n",
    "                 orthogonal_reg_weight = 0.,\n",
    "                 orthogonal_reg_active_codes_only = False):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.n_head = n_head\n",
    "        self.codebook_size = codebook_size\n",
    "        self.commitment_weight = commitment_weight\n",
    "        self.orthogonal_reg_weight = orthogonal_reg_weight\n",
    "        self.orthogonal_reg_active_codes_only = orthogonal_reg_active_codes_only \n",
    "\n",
    "        codebook_dim = input_dim // n_head # 这个是每个 head 的维度\n",
    "        assert codebook_dim * n_head == input_dim, 'input_dim must be divisible by n_head'\n",
    "        self._codebook = SimpleCodeBook(\n",
    "            dim = codebook_dim,\n",
    "            num_codebooks = n_head,\n",
    "            codebook_size = codebook_size\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # x.shape = 'bz, d',\n",
    "        # x = rearrange(x,'bz d -> bz h d')\n",
    "        ein_rhs_eq = 'h b d'\n",
    "        x = rearrange(input, f\"b (h d) -> {ein_rhs_eq}\", h = self.n_head)\n",
    "        # quantize\n",
    "        quantize, embed_ind, distances = self._codebook(x)\n",
    "        if self.training:\n",
    "            # straight through\n",
    "            commit_quantize = quantize\n",
    "            quantize = x + (quantize - x).detach()\n",
    "\n",
    "        embed_ind = rearrange(embed_ind, 'h b -> b h', h = self.n_head)\n",
    "        \n",
    "\n",
    "        loss = torch.tensor([0.], device = x.device, requires_grad = self.training)\n",
    "\n",
    "        if self.training:\n",
    "            #! commit loss\n",
    "            commit_loss = F.mse_loss(commit_quantize, x)\n",
    "            loss = loss + commit_loss * self.commitment_weight\n",
    "            #! othogonal loss \n",
    "            if self.orthogonal_reg_weight > 0:\n",
    "                orthogonal_reg_loss = orthogonal_loss_fn_with_mask(self._codebook.embed, self._codebook.valid_codebook)\n",
    "                loss = loss + orthogonal_reg_loss * self.orthogonal_reg_weight\n",
    "\n",
    "\n",
    "        quantize = rearrange(quantize, 'h b d -> b (h d)', h = self.n_head)\n",
    "        \n",
    "        return quantize, embed_ind, loss\n",
    "\n",
    "    def get_info(self,x):\n",
    "        ein_rhs_eq = 'h b d'\n",
    "        x = rearrange(input, f\"b (h d) -> {ein_rhs_eq}\", h = self.n_head)\n",
    "        # quantize\n",
    "        quantize, embed_ind, distances = self._codebook(x)\n",
    "        embed_ind = rearrange(embed_ind, 'h b -> b h', h = self.n_head)\n",
    "        quantize = rearrange(quantize, 'h b d -> b (h d)', h = self.n_head)\n",
    "        distances = rearrange(distances, 'h b d -> b h d')\n",
    "        return quantize, embed_ind, distances\n",
    "\n",
    "\n",
    "        \n",
    "VQ = VectorQuantize(\n",
    "    input_dim=8,\n",
    "    n_head=4,\n",
    "    codebook_size=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_activation(act_name):\n",
    "    if act_name == \"elu\":\n",
    "        return nn.ELU()\n",
    "    elif act_name == \"selu\":\n",
    "        return nn.SELU()\n",
    "    elif act_name == \"relu\":\n",
    "        return nn.ReLU()\n",
    "    elif act_name == \"crelu\":\n",
    "        return nn.ReLU()\n",
    "    elif act_name == \"lrelu\":\n",
    "        return nn.LeakyReLU()\n",
    "    elif act_name == \"tanh\":\n",
    "        return nn.Tanh()\n",
    "    elif act_name == \"sigmoid\":\n",
    "        return nn.Sigmoid()\n",
    "    elif act_name == \"identity\":\n",
    "        return nn.Identity()\n",
    "    elif act_name == \"elephant\":\n",
    "        return Elephant(0.2,4)\n",
    "    else:\n",
    "        print(\"invalid activation function!\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(10, 10),\n",
    "    get_activation('elephant'),\n",
    "    nn.Linear(10, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x  = torch.randn(2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0133, -0.0153,  0.0105,  ..., -0.0596,  0.0052,  0.0633],\n",
      "         [ 0.0126, -0.0127,  0.0336,  ..., -0.0941,  0.0138,  0.0018],\n",
      "         [-0.0151, -0.0243,  0.0075,  ...,  0.0183, -0.0211, -0.0049],\n",
      "         ...,\n",
      "         [-0.0162,  0.0233, -0.0164,  ...,  0.0485,  0.0029,  0.0357],\n",
      "         [-0.0262,  0.0108,  0.0273,  ...,  0.0073, -0.0678,  0.0023],\n",
      "         [-0.0461, -0.0137, -0.0086,  ..., -0.0030, -0.0267,  0.0228]]])\n"
     ]
    }
   ],
   "source": [
    "sz = (1,32,32)\n",
    "t = torch.empty(sz)\n",
    "nn.init.orthogonal_(t)\n",
    "print(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_t = l2norm(t)\n",
    "res = einsum('h i d, h j d ->h i j', normed_t,normed_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0000e+00,  3.8754e-02, -2.2172e-02,  ...,  1.9489e-01,\n",
       "          -1.0454e-01,  1.5987e-04],\n",
       "         [ 3.8754e-02,  1.0000e+00, -2.6583e-01,  ..., -2.1387e-01,\n",
       "           8.9059e-02,  9.0151e-03],\n",
       "         [-2.2172e-02, -2.6583e-01,  1.0000e+00,  ..., -1.9104e-01,\n",
       "           7.0439e-02,  1.6044e-01],\n",
       "         ...,\n",
       "         [ 1.9489e-01, -2.1387e-01, -1.9104e-01,  ...,  1.0000e+00,\n",
       "           2.4896e-01, -1.2304e-01],\n",
       "         [-1.0454e-01,  8.9059e-02,  7.0439e-02,  ...,  2.4896e-01,\n",
       "           1.0000e+00,  2.1640e-01],\n",
       "         [ 1.5987e-04,  9.0151e-03,  1.6044e-01,  ..., -1.2304e-01,\n",
       "           2.1640e-01,  1.0000e+00]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = orthogonal_loss_fn(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PushConfig:\n",
    "    def __init__(self,\n",
    "                 id,\n",
    "                 body_index_list:list,\n",
    "                 change_interval:int,\n",
    "                 force_list:list, ) -> None:\n",
    "        self.id = id \n",
    "        self.body_index_list =body_index_list\n",
    "        self.change_interval = change_interval\n",
    "        self.force_list = force_list \n",
    "        assert len(self.body_index_list) > 0 and  len(self.force_list) > 0\n",
    "        self._force = self.force_list[0]\n",
    "        self._body_index = self.body_index_list[0] \n",
    "    \n",
    "    def _change(self):\n",
    "        self._force = np.random.choice(self.force_list)\n",
    "        self._body_index = np.random.choice(self.body_index_list) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config = PushConfig(id = 0, \n",
    "                         body_index_list=[1,2,3,4],\n",
    "                         change_interval=10,\n",
    "                         force_list=[1,2,3,4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.load('logs/Eval/VQ-STG_Forward-stationary_push-DebugEval.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "force (100, 1011, 17, 3)\n",
      "done (100, 1011)\n",
      "tracking_error (100, 1011)\n",
      "base_vel (100, 1011, 3)\n",
      "first_done (100,)\n",
      "Fall (100,)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/jijingtian/project/MetaRobotics/notebook.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c6162227d/home/jijingtian/project/MetaRobotics/notebook.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m tmp\u001b[39m.\u001b[39mitems():\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224c6162227d/home/jijingtian/project/MetaRobotics/notebook.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(k,v\u001b[39m.\u001b[39;49mshape)    \n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "for k,v in tmp.items():\n",
    "    print(k,v.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "terrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
