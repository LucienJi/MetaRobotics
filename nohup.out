Importing module 'gym_38' (/home/jijingtian/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_38.so)
Setting GYM_USD_PLUG_INFO_PATH to /home/jijingtian/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json
PyTorch version 2.0.1+cu117
Device count 4
/home/jijingtian/isaacgym/python/isaacgym/_bindings/src/gymtorch
Using /home/jijingtian/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Emitting ninja build file /home/jijingtian/.cache/torch_extensions/py38_cu117/gymtorch/build.ninja...
Building extension module gymtorch...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:0
GPU Pipeline: enabled
Loading extension module gymtorch...
/home/jijingtian/miniconda3/envs/terrain/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if not hasattr(tensorboard, "__version__") or LooseVersion(
/home/jijingtian/project/MetaRobotics/legged_gym/utils/terrain_lib.py:83: DeprecationWarning: `interp2d` is deprecated!
`interp2d` is deprecated in SciPy 1.10 and will be removed in SciPy 1.12.0.

For legacy code, nearly bug-for-bug compatible replacements are
`RectBivariateSpline` on regular grids, and `bisplrep`/`bisplev` for
scattered 2D data.

In new code, for regular grids use `RegularGridInterpolator` instead.
For scattered data, prefer `LinearNDInterpolator` or
`CloughTocher2DInterpolator`.

For more details see
`https://gist.github.com/ev-br/8544371b40f414b7eaf3fe6217209bff`

  f = interpolate.interp2d(y, x, height_field_downsampled, kind='linear')
/home/jijingtian/project/MetaRobotics/legged_gym/utils/terrain_lib.py:87: DeprecationWarning:         `interp2d` is deprecated!
        `interp2d` is deprecated in SciPy 1.10 and will be removed in SciPy 1.12.0.

        For legacy code, nearly bug-for-bug compatible replacements are
        `RectBivariateSpline` on regular grids, and `bisplrep`/`bisplev` for
        scattered 2D data.

        In new code, for regular grids use `RegularGridInterpolator` instead.
        For scattered data, prefer `LinearNDInterpolator` or
        `CloughTocher2DInterpolator`.

        For more details see
        `https://gist.github.com/ev-br/8544371b40f414b7eaf3fe6217209bff`

  z_upsampled = np.rint(f(y_upsampled, x_upsampled))
Check Body Names:  ['base', 'FL_hip', 'FL_thigh', 'FL_calf', 'FL_foot', 'FR_hip', 'FR_thigh', 'FR_calf', 'FR_foot', 'RL_hip', 'RL_thigh', 'RL_calf', 'RL_foot', 'RR_hip', 'RR_thigh', 'RR_calf', 'RR_foot']
Traceback (most recent call last):
  File "run_vq.py", line 75, in <module>
    runner.learn(num_learning_iterations=10000)
  File "/home/jijingtian/project/MetaRobotics/OnlineAdaptation/runners/vq_onpolicy_runner.py", line 101, in learn
    mean_value_loss, mean_surrogate_loss,mean_entropy_loss,mean_forward_loss,mean_cmt_loss = self.alg.update()
  File "/home/jijingtian/project/MetaRobotics/OnlineAdaptation/algorithms/vq_ppo.py", line 98, in update
    self.actor_critic._update_with_latent(obs_batch,latent)
  File "/home/jijingtian/project/MetaRobotics/OnlineAdaptation/modules/ac.py", line 108, in _update_with_latent
    mean = self.actor(obs, latent)
  File "/home/jijingtian/miniconda3/envs/terrain/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
TypeError: forward() takes 2 positional arguments but 3 were given
Importing module 'gym_38' (/home/jijingtian/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_38.so)
Setting GYM_USD_PLUG_INFO_PATH to /home/jijingtian/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json
PyTorch version 2.0.1+cu117
Device count 4
/home/jijingtian/isaacgym/python/isaacgym/_bindings/src/gymtorch
Using /home/jijingtian/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Emitting ninja build file /home/jijingtian/.cache/torch_extensions/py38_cu117/gymtorch/build.ninja...
Building extension module gymtorch...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:0
GPU Pipeline: enabled
Check Body Names:  ['base', 'FL_hip', 'FL_thigh', 'FL_calf', 'FL_foot', 'FR_hip', 'FR_thigh', 'FR_calf', 'FR_foot', 'RL_hip', 'RL_thigh', 'RL_calf', 'RL_foot', 'RR_hip', 'RR_thigh', 'RR_calf', 'RR_foot']
################################################################################
                      [1m Learning iteration 0/10000 [0m                      

                       Computation: 43485 steps/s (collection: 1.885s, learning 0.376s)
               Value function loss: 0.2673
           Forward prediction loss: 0.0000
                          CMT loss: 0.0317
                    Surrogate loss: -0.0029
                    Policy entropy: 17.0242
             Mean action noise std: 1.00
                       Mean reward: -0.69
               Mean episode length: 15.44
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0023
       Mean episode rew_ang_vel_xy: -0.0053
        Mean episode rew_collision: -0.0026
          Mean episode rew_dof_acc: -0.0050
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0073
          Mean episode rew_torques: -0.0010
 Mean episode rew_tracking_ang_vel: 0.0009
 Mean episode rew_tracking_lin_vel: 0.0017
            Mean episode rew_total: -0.0209
        Mean episode terrain_level: 3.9806
    Mean episode min_command_x_vel: -0.5994
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1665
  Mean episode max_command_yaw_vel: 0.1665
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 2.26s
                        Total time: 2.26s
                               ETA: 22606.4s

################################################################################
                      [1m Learning iteration 1/10000 [0m                      

                       Computation: 43511 steps/s (collection: 1.893s, learning 0.367s)
               Value function loss: 0.0465
           Forward prediction loss: 0.0000
                          CMT loss: 0.0192
                    Surrogate loss: -0.0091
                    Policy entropy: 16.9958
             Mean action noise std: 1.00
                       Mean reward: -1.90
               Mean episode length: 37.01
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0085
       Mean episode rew_ang_vel_xy: -0.0337
        Mean episode rew_collision: -0.0063
          Mean episode rew_dof_acc: -0.0211
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0284
          Mean episode rew_torques: -0.0026
 Mean episode rew_tracking_ang_vel: 0.0047
 Mean episode rew_tracking_lin_vel: 0.0088
            Mean episode rew_total: -0.0869
        Mean episode terrain_level: 3.9760
    Mean episode min_command_x_vel: -0.5994
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1665
  Mean episode max_command_yaw_vel: 0.1665
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 2.26s
                        Total time: 4.52s
                               ETA: 22597.3s

################################################################################
                      [1m Learning iteration 2/10000 [0m                      

                       Computation: 36859 steps/s (collection: 2.305s, learning 0.362s)
               Value function loss: 0.0323
           Forward prediction loss: 0.0000
                          CMT loss: 0.0094
                    Surrogate loss: -0.0111
                    Policy entropy: 16.9387
             Mean action noise std: 0.99
                       Mean reward: -2.41
               Mean episode length: 69.11
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0144
       Mean episode rew_ang_vel_xy: -0.0511
        Mean episode rew_collision: -0.0080
          Mean episode rew_dof_acc: -0.0362
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0374
          Mean episode rew_torques: -0.0037
 Mean episode rew_tracking_ang_vel: 0.0101
 Mean episode rew_tracking_lin_vel: 0.0206
            Mean episode rew_total: -0.1199
        Mean episode terrain_level: 3.9435
    Mean episode min_command_x_vel: -0.5994
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1665
  Mean episode max_command_yaw_vel: 0.1665
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 2.67s
                        Total time: 7.19s
                               ETA: 23951.6s

################################################################################
                      [1m Learning iteration 3/10000 [0m                      

                       Computation: 40300 steps/s (collection: 2.078s, learning 0.361s)
               Value function loss: 0.0221
           Forward prediction loss: 0.0000
                          CMT loss: 0.0064
                    Surrogate loss: -0.0126
                    Policy entropy: 16.8749
             Mean action noise std: 0.99
                       Mean reward: -2.51
               Mean episode length: 91.33
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0199
       Mean episode rew_ang_vel_xy: -0.0545
        Mean episode rew_collision: -0.0114
          Mean episode rew_dof_acc: -0.0465
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0357
          Mean episode rew_torques: -0.0052
 Mean episode rew_tracking_ang_vel: 0.0150
 Mean episode rew_tracking_lin_vel: 0.0341
            Mean episode rew_total: -0.1239
        Mean episode terrain_level: 3.8867
    Mean episode min_command_x_vel: -0.5994
    Mean episode max_command_x_vel: 0.5996
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1665
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 2.44s
                        Total time: 9.63s
                               ETA: 24058.2s

################################################################################
                      [1m Learning iteration 4/10000 [0m                      

                       Computation: 41022 steps/s (collection: 2.031s, learning 0.366s)
               Value function loss: 0.0196
           Forward prediction loss: 0.0000
                          CMT loss: 0.0059
                    Surrogate loss: -0.0181
                    Policy entropy: 16.8529
             Mean action noise std: 0.99
                       Mean reward: -2.64
               Mean episode length: 108.07
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0240
       Mean episode rew_ang_vel_xy: -0.0591
        Mean episode rew_collision: -0.0132
          Mean episode rew_dof_acc: -0.0532
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0353
          Mean episode rew_torques: -0.0064
 Mean episode rew_tracking_ang_vel: 0.0188
 Mean episode rew_tracking_lin_vel: 0.0414
            Mean episode rew_total: -0.1310
        Mean episode terrain_level: 3.8408
    Mean episode min_command_x_vel: -0.5996
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1665
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 2.40s
                        Total time: 12.02s
                               ETA: 24035.5s

################################################################################
                      [1m Learning iteration 5/10000 [0m                      

                       Computation: 39558 steps/s (collection: 2.119s, learning 0.366s)
               Value function loss: 0.0152
           Forward prediction loss: 0.0000
                          CMT loss: 0.0059
                    Surrogate loss: -0.0209
                    Policy entropy: 16.7875
             Mean action noise std: 0.98
                       Mean reward: -2.84
               Mean episode length: 125.70
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0278
       Mean episode rew_ang_vel_xy: -0.0639
        Mean episode rew_collision: -0.0157
          Mean episode rew_dof_acc: -0.0587
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0401
          Mean episode rew_torques: -0.0075
 Mean episode rew_tracking_ang_vel: 0.0218
 Mean episode rew_tracking_lin_vel: 0.0488
            Mean episode rew_total: -0.1431
        Mean episode terrain_level: 3.7986
    Mean episode min_command_x_vel: -0.5996
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1665
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 2.49s
                        Total time: 14.51s
                               ETA: 24167.2s

################################################################################
                      [1m Learning iteration 6/10000 [0m                      

                       Computation: 41990 steps/s (collection: 1.974s, learning 0.367s)
               Value function loss: 0.0143
           Forward prediction loss: 0.0000
                          CMT loss: 0.0055
                    Surrogate loss: -0.0233
                    Policy entropy: 16.7242
             Mean action noise std: 0.97
                       Mean reward: -2.71
               Mean episode length: 134.18
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0306
       Mean episode rew_ang_vel_xy: -0.0658
        Mean episode rew_collision: -0.0157
          Mean episode rew_dof_acc: -0.0618
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0376
          Mean episode rew_torques: -0.0082
 Mean episode rew_tracking_ang_vel: 0.0249
 Mean episode rew_tracking_lin_vel: 0.0607
            Mean episode rew_total: -0.1340
        Mean episode terrain_level: 3.7628
    Mean episode min_command_x_vel: -0.5995
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1665
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 2.34s
                        Total time: 16.85s
                               ETA: 24055.1s

################################################################################
                      [1m Learning iteration 7/10000 [0m                      

                       Computation: 41152 steps/s (collection: 2.021s, learning 0.368s)
               Value function loss: 0.0146
           Forward prediction loss: 0.0000
                          CMT loss: 0.0054
                    Surrogate loss: -0.0216
                    Policy entropy: 16.6500
             Mean action noise std: 0.97
                       Mean reward: -2.80
               Mean episode length: 140.95
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0333
       Mean episode rew_ang_vel_xy: -0.0685
        Mean episode rew_collision: -0.0211
          Mean episode rew_dof_acc: -0.0656
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0384
          Mean episode rew_torques: -0.0090
 Mean episode rew_tracking_ang_vel: 0.0270
 Mean episode rew_tracking_lin_vel: 0.0658
            Mean episode rew_total: -0.1430
        Mean episode terrain_level: 3.7346
    Mean episode min_command_x_vel: -0.5994
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1665
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 2.39s
                        Total time: 19.24s
                               ETA: 24030.0s

################################################################################
                      [1m Learning iteration 8/10000 [0m                      

                       Computation: 39333 steps/s (collection: 2.133s, learning 0.367s)
               Value function loss: 0.0135
           Forward prediction loss: 0.0000
                          CMT loss: 0.0051
                    Surrogate loss: -0.0183
                    Policy entropy: 16.5653
             Mean action noise std: 0.96
                       Mean reward: -2.80
               Mean episode length: 177.00
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0395
       Mean episode rew_ang_vel_xy: -0.0730
        Mean episode rew_collision: -0.0192
          Mean episode rew_dof_acc: -0.0724
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0375
          Mean episode rew_torques: -0.0109
 Mean episode rew_tracking_ang_vel: 0.0310
 Mean episode rew_tracking_lin_vel: 0.0800
            Mean episode rew_total: -0.1415
        Mean episode terrain_level: 3.7068
    Mean episode min_command_x_vel: -0.5994
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1665
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 2.50s
                        Total time: 21.74s
                               ETA: 24132.6s

################################################################################
                      [1m Learning iteration 9/10000 [0m                      

                       Computation: 42686 steps/s (collection: 1.935s, learning 0.368s)
               Value function loss: 0.0119
           Forward prediction loss: 0.0000
                          CMT loss: 0.0047
                    Surrogate loss: -0.0209
                    Policy entropy: 16.4189
             Mean action noise std: 0.95
                       Mean reward: -2.95
               Mean episode length: 184.38
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0408
       Mean episode rew_ang_vel_xy: -0.0734
        Mean episode rew_collision: -0.0185
          Mean episode rew_dof_acc: -0.0771
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0374
          Mean episode rew_torques: -0.0110
 Mean episode rew_tracking_ang_vel: 0.0331
 Mean episode rew_tracking_lin_vel: 0.0832
            Mean episode rew_total: -0.1420
        Mean episode terrain_level: 3.6773
    Mean episode min_command_x_vel: -0.5994
    Mean episode max_command_x_vel: 0.5988
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1665
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 2.30s
                        Total time: 24.04s
                               ETA: 24018.0s

################################################################################
                     [1m Learning iteration 10/10000 [0m                      

                       Computation: 45245 steps/s (collection: 1.804s, learning 0.369s)
               Value function loss: 0.0105
           Forward prediction loss: 0.0000
                          CMT loss: 0.0039
                    Surrogate loss: -0.0158
                    Policy entropy: 16.3436
             Mean action noise std: 0.94
                       Mean reward: -2.85
               Mean episode length: 185.18
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0401
       Mean episode rew_ang_vel_xy: -0.0711
        Mean episode rew_collision: -0.0221
          Mean episode rew_dof_acc: -0.0739
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0337
          Mean episode rew_torques: -0.0111
 Mean episode rew_tracking_ang_vel: 0.0338
 Mean episode rew_tracking_lin_vel: 0.0844
            Mean episode rew_total: -0.1338
        Mean episode terrain_level: 3.6552
    Mean episode min_command_x_vel: -0.5994
    Mean episode max_command_x_vel: 0.5985
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 2.17s
                        Total time: 26.21s
                               ETA: 23805.5s

################################################################################
                     [1m Learning iteration 11/10000 [0m                      

                       Computation: 43865 steps/s (collection: 1.875s, learning 0.366s)
               Value function loss: 0.0107
           Forward prediction loss: 0.0000
                          CMT loss: 0.0037
                    Surrogate loss: -0.0205
                    Policy entropy: 16.1659
             Mean action noise std: 0.93
                       Mean reward: -2.45
               Mean episode length: 188.95
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0421
       Mean episode rew_ang_vel_xy: -0.0696
        Mean episode rew_collision: -0.0218
          Mean episode rew_dof_acc: -0.0727
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0314
          Mean episode rew_torques: -0.0118
 Mean episode rew_tracking_ang_vel: 0.0353
 Mean episode rew_tracking_lin_vel: 0.0873
            Mean episode rew_total: -0.1267
        Mean episode terrain_level: 3.6347
    Mean episode min_command_x_vel: -0.5994
    Mean episode max_command_x_vel: 0.5985
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 2.24s
                        Total time: 28.45s
                               ETA: 23685.0s

################################################################################
                     [1m Learning iteration 12/10000 [0m                      

                       Computation: 44762 steps/s (collection: 1.828s, learning 0.369s)
               Value function loss: 0.0103
           Forward prediction loss: 0.0000
                          CMT loss: 0.0035
                    Surrogate loss: -0.0123
                    Policy entropy: 16.0992
             Mean action noise std: 0.92
                       Mean reward: -2.39
               Mean episode length: 202.09
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0464
       Mean episode rew_ang_vel_xy: -0.0684
        Mean episode rew_collision: -0.0214
          Mean episode rew_dof_acc: -0.0763
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0305
          Mean episode rew_torques: -0.0134
 Mean episode rew_tracking_ang_vel: 0.0413
 Mean episode rew_tracking_lin_vel: 0.0880
            Mean episode rew_total: -0.1272
        Mean episode terrain_level: 3.6141
    Mean episode min_command_x_vel: -0.5994
    Mean episode max_command_x_vel: 0.5985
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 2.20s
                        Total time: 30.65s
                               ETA: 23548.2s

################################################################################
                     [1m Learning iteration 13/10000 [0m                      

                       Computation: 48432 steps/s (collection: 1.662s, learning 0.368s)
               Value function loss: 0.0105
           Forward prediction loss: 0.0000
                          CMT loss: 0.0038
                    Surrogate loss: -0.0184
                    Policy entropy: 15.9666
             Mean action noise std: 0.91
                       Mean reward: -2.32
               Mean episode length: 230.71
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0511
       Mean episode rew_ang_vel_xy: -0.0742
        Mean episode rew_collision: -0.0172
          Mean episode rew_dof_acc: -0.0820
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0296
          Mean episode rew_torques: -0.0148
 Mean episode rew_tracking_ang_vel: 0.0460
 Mean episode rew_tracking_lin_vel: 0.1018
            Mean episode rew_total: -0.1211
        Mean episode terrain_level: 3.5896
    Mean episode min_command_x_vel: -0.5994
    Mean episode max_command_x_vel: 0.5985
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 2.03s
                        Total time: 32.68s
                               ETA: 23311.9s

################################################################################
                     [1m Learning iteration 14/10000 [0m                      

                       Computation: 47074 steps/s (collection: 1.720s, learning 0.368s)
               Value function loss: 0.0103
           Forward prediction loss: 0.0000
                          CMT loss: 0.0037
                    Surrogate loss: -0.0175
                    Policy entropy: 15.8747
             Mean action noise std: 0.91
                       Mean reward: -2.28
               Mean episode length: 260.32
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0588
       Mean episode rew_ang_vel_xy: -0.0783
        Mean episode rew_collision: -0.0120
          Mean episode rew_dof_acc: -0.0912
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0299
          Mean episode rew_torques: -0.0169
 Mean episode rew_tracking_ang_vel: 0.0528
 Mean episode rew_tracking_lin_vel: 0.1175
            Mean episode rew_total: -0.1169
        Mean episode terrain_level: 3.5656
    Mean episode min_command_x_vel: -0.5994
    Mean episode max_command_x_vel: 0.5985
    Mean episode min_command_y_vel: -0.1665
    Mean episode max_command_y_vel: 0.1664
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 2.09s
                        Total time: 34.77s
                               ETA: 23145.8s

################################################################################
                     [1m Learning iteration 15/10000 [0m                      

                       Computation: 43905 steps/s (collection: 1.872s, learning 0.367s)
               Value function loss: 0.0101
           Forward prediction loss: 0.0000
                          CMT loss: 0.0038
                    Surrogate loss: -0.0168
                    Policy entropy: 15.7641
             Mean action noise std: 0.90
                       Mean reward: -2.21
               Mean episode length: 270.99
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0609
       Mean episode rew_ang_vel_xy: -0.0805
        Mean episode rew_collision: -0.0180
          Mean episode rew_dof_acc: -0.0912
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0295
          Mean episode rew_torques: -0.0175
 Mean episode rew_tracking_ang_vel: 0.0558
 Mean episode rew_tracking_lin_vel: 0.1234
            Mean episode rew_total: -0.1184
        Mean episode terrain_level: 3.5371
    Mean episode min_command_x_vel: -0.5994
    Mean episode max_command_x_vel: 0.5983
    Mean episode min_command_y_vel: -0.1665
    Mean episode max_command_y_vel: 0.1664
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 2.24s
                        Total time: 37.01s
                               ETA: 23094.3s

################################################################################
                     [1m Learning iteration 16/10000 [0m                      

                       Computation: 45353 steps/s (collection: 1.799s, learning 0.369s)
               Value function loss: 0.0095
           Forward prediction loss: 0.0000
                          CMT loss: 0.0037
                    Surrogate loss: -0.0181
                    Policy entropy: 15.6832
             Mean action noise std: 0.89
                       Mean reward: -2.33
               Mean episode length: 286.04
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0621
       Mean episode rew_ang_vel_xy: -0.0830
        Mean episode rew_collision: -0.0234
          Mean episode rew_dof_acc: -0.0954
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0309
          Mean episode rew_torques: -0.0181
 Mean episode rew_tracking_ang_vel: 0.0581
 Mean episode rew_tracking_lin_vel: 0.1355
            Mean episode rew_total: -0.1192
        Mean episode terrain_level: 3.5092
    Mean episode min_command_x_vel: -0.5994
    Mean episode max_command_x_vel: 0.5979
    Mean episode min_command_y_vel: -0.1665
    Mean episode max_command_y_vel: 0.1664
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 2.17s
                        Total time: 39.17s
                               ETA: 23006.6s

################################################################################
                     [1m Learning iteration 17/10000 [0m                      

                       Computation: 46729 steps/s (collection: 1.735s, learning 0.369s)
               Value function loss: 0.0091
           Forward prediction loss: 0.0000
                          CMT loss: 0.0037
                    Surrogate loss: -0.0128
                    Policy entropy: 15.6095
             Mean action noise std: 0.89
                       Mean reward: -2.11
               Mean episode length: 278.98
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0607
       Mean episode rew_ang_vel_xy: -0.0833
        Mean episode rew_collision: -0.0124
          Mean episode rew_dof_acc: -0.0932
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0304
          Mean episode rew_torques: -0.0172
 Mean episode rew_tracking_ang_vel: 0.0571
 Mean episode rew_tracking_lin_vel: 0.1384
            Mean episode rew_total: -0.1017
        Mean episode terrain_level: 3.4823
    Mean episode min_command_x_vel: -0.5994
    Mean episode max_command_x_vel: 0.5989
    Mean episode min_command_y_vel: -0.1665
    Mean episode max_command_y_vel: 0.1664
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 2.10s
                        Total time: 41.28s
                               ETA: 22893.0s

################################################################################
                     [1m Learning iteration 18/10000 [0m                      

                       Computation: 45402 steps/s (collection: 1.797s, learning 0.369s)
               Value function loss: 0.0085
           Forward prediction loss: 0.0000
                          CMT loss: 0.0035
                    Surrogate loss: -0.0151
                    Policy entropy: 15.5109
             Mean action noise std: 0.88
                       Mean reward: -1.81
               Mean episode length: 266.50
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0564
       Mean episode rew_ang_vel_xy: -0.0772
        Mean episode rew_collision: -0.0106
          Mean episode rew_dof_acc: -0.0825
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0280
          Mean episode rew_torques: -0.0166
 Mean episode rew_tracking_ang_vel: 0.0530
 Mean episode rew_tracking_lin_vel: 0.1271
            Mean episode rew_total: -0.0913
        Mean episode terrain_level: 3.4556
    Mean episode min_command_x_vel: -0.5994
    Mean episode max_command_x_vel: 0.5992
    Mean episode min_command_y_vel: -0.1665
    Mean episode max_command_y_vel: 0.1664
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 2.17s
                        Total time: 43.44s
                               ETA: 22823.4s

################################################################################
                     [1m Learning iteration 19/10000 [0m                      

                       Computation: 46064 steps/s (collection: 1.766s, learning 0.368s)
               Value function loss: 0.0079
           Forward prediction loss: 0.0000
                          CMT loss: 0.0034
                    Surrogate loss: -0.0170
                    Policy entropy: 15.4285
             Mean action noise std: 0.87
                       Mean reward: -2.19
               Mean episode length: 299.51
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0577
       Mean episode rew_ang_vel_xy: -0.0757
        Mean episode rew_collision: -0.0179
          Mean episode rew_dof_acc: -0.0817
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0250
          Mean episode rew_torques: -0.0170
 Mean episode rew_tracking_ang_vel: 0.0555
 Mean episode rew_tracking_lin_vel: 0.1343
            Mean episode rew_total: -0.0852
        Mean episode terrain_level: 3.4315
    Mean episode min_command_x_vel: -0.5994
    Mean episode max_command_x_vel: 0.5987
    Mean episode min_command_y_vel: -0.1665
    Mean episode max_command_y_vel: 0.1664
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 2.13s
                        Total time: 45.58s
                               ETA: 22745.1s

################################################################################
                     [1m Learning iteration 20/10000 [0m                      

                       Computation: 33934 steps/s (collection: 2.532s, learning 0.364s)
               Value function loss: 0.0228
           Forward prediction loss: 0.0000
                          CMT loss: 0.0031
                    Surrogate loss: -0.0120
                    Policy entropy: 15.3576
             Mean action noise std: 0.87
                       Mean reward: -1.79
               Mean episode length: 327.02
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0701
       Mean episode rew_ang_vel_xy: -0.0870
        Mean episode rew_collision: -0.0153
          Mean episode rew_dof_acc: -0.1000
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0290
          Mean episode rew_torques: -0.0205
 Mean episode rew_tracking_ang_vel: 0.0689
 Mean episode rew_tracking_lin_vel: 0.1612
            Mean episode rew_total: -0.0917
        Mean episode terrain_level: 3.4099
    Mean episode min_command_x_vel: -0.5994
    Mean episode max_command_x_vel: 0.5985
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1664
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 2.90s
                        Total time: 48.47s
                               ETA: 23036.5s

################################################################################
                     [1m Learning iteration 21/10000 [0m                      

                       Computation: 47018 steps/s (collection: 1.727s, learning 0.364s)
               Value function loss: 0.0076
           Forward prediction loss: 0.0000
                          CMT loss: 0.0031
                    Surrogate loss: -0.0151
                    Policy entropy: 15.2884
             Mean action noise std: 0.86
                       Mean reward: -1.44
               Mean episode length: 325.16
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0677
       Mean episode rew_ang_vel_xy: -0.0858
        Mean episode rew_collision: -0.0141
          Mean episode rew_dof_acc: -0.0928
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0280
          Mean episode rew_torques: -0.0198
 Mean episode rew_tracking_ang_vel: 0.0667
 Mean episode rew_tracking_lin_vel: 0.1690
            Mean episode rew_total: -0.0725
        Mean episode terrain_level: 3.3925
    Mean episode min_command_x_vel: -0.5994
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1664
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 2.09s
                        Total time: 50.56s
                               ETA: 22935.6s

################################################################################
                     [1m Learning iteration 22/10000 [0m                      

                       Computation: 46602 steps/s (collection: 1.747s, learning 0.363s)
               Value function loss: 0.0083
           Forward prediction loss: 0.0000
                          CMT loss: 0.0034
                    Surrogate loss: -0.0137
                    Policy entropy: 15.2146
             Mean action noise std: 0.86
                       Mean reward: -1.03
               Mean episode length: 341.87
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0714
       Mean episode rew_ang_vel_xy: -0.0868
        Mean episode rew_collision: -0.0260
          Mean episode rew_dof_acc: -0.0964
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0269
          Mean episode rew_torques: -0.0214
 Mean episode rew_tracking_ang_vel: 0.0703
 Mean episode rew_tracking_lin_vel: 0.1967
            Mean episode rew_total: -0.0618
        Mean episode terrain_level: 3.3734
    Mean episode min_command_x_vel: -0.5994
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1664
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 2.11s
                        Total time: 52.67s
                               ETA: 22851.3s

################################################################################
                     [1m Learning iteration 23/10000 [0m                      

                       Computation: 44594 steps/s (collection: 1.841s, learning 0.364s)
               Value function loss: 0.0098
           Forward prediction loss: 0.0000
                          CMT loss: 0.0040
                    Surrogate loss: -0.0140
                    Policy entropy: 15.1282
             Mean action noise std: 0.85
                       Mean reward: -0.55
               Mean episode length: 441.50
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0785
       Mean episode rew_ang_vel_xy: -0.0960
        Mean episode rew_collision: -0.0169
          Mean episode rew_dof_acc: -0.1037
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0289
          Mean episode rew_torques: -0.0236
 Mean episode rew_tracking_ang_vel: 0.0798
 Mean episode rew_tracking_lin_vel: 0.2340
            Mean episode rew_total: -0.0339
        Mean episode terrain_level: 3.3491
    Mean episode min_command_x_vel: -0.5994
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1664
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 2.20s
                        Total time: 54.88s
                               ETA: 22813.3s

################################################################################
                     [1m Learning iteration 24/10000 [0m                      

                       Computation: 45180 steps/s (collection: 1.812s, learning 0.364s)
               Value function loss: 0.0097
           Forward prediction loss: 0.0000
                          CMT loss: 0.0040
                    Surrogate loss: -0.0134
                    Policy entropy: 15.0696
             Mean action noise std: 0.85
                       Mean reward: -0.78
               Mean episode length: 439.13
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0902
       Mean episode rew_ang_vel_xy: -0.1075
        Mean episode rew_collision: -0.0350
          Mean episode rew_dof_acc: -0.1173
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0329
          Mean episode rew_torques: -0.0280
 Mean episode rew_tracking_ang_vel: 0.0945
 Mean episode rew_tracking_lin_vel: 0.2933
            Mean episode rew_total: -0.0230
        Mean episode terrain_level: 3.3150
    Mean episode min_command_x_vel: -0.5994
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1665
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 2.18s
                        Total time: 57.05s
                               ETA: 22766.8s

################################################################################
                     [1m Learning iteration 25/10000 [0m                      

                       Computation: 44642 steps/s (collection: 1.837s, learning 0.365s)
               Value function loss: 0.0092
           Forward prediction loss: 0.0000
                          CMT loss: 0.0037
                    Surrogate loss: -0.0138
                    Policy entropy: 14.9742
             Mean action noise std: 0.84
                       Mean reward: -0.40
               Mean episode length: 462.66
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0935
       Mean episode rew_ang_vel_xy: -0.1087
        Mean episode rew_collision: -0.0304
          Mean episode rew_dof_acc: -0.1227
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0328
          Mean episode rew_torques: -0.0297
 Mean episode rew_tracking_ang_vel: 0.0989
 Mean episode rew_tracking_lin_vel: 0.3111
            Mean episode rew_total: -0.0078
        Mean episode terrain_level: 3.2828
    Mean episode min_command_x_vel: -0.5994
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 2.20s
                        Total time: 59.26s
                               ETA: 22733.8s

################################################################################
                     [1m Learning iteration 26/10000 [0m                      

                       Computation: 44841 steps/s (collection: 1.828s, learning 0.364s)
               Value function loss: 0.0090
           Forward prediction loss: 0.0000
                          CMT loss: 0.0034
                    Surrogate loss: -0.0081
                    Policy entropy: 14.9179
             Mean action noise std: 0.84
                       Mean reward: -0.34
               Mean episode length: 365.90
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0814
       Mean episode rew_ang_vel_xy: -0.0968
        Mean episode rew_collision: -0.0333
          Mean episode rew_dof_acc: -0.1030
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0294
          Mean episode rew_torques: -0.0255
 Mean episode rew_tracking_ang_vel: 0.0853
 Mean episode rew_tracking_lin_vel: 0.2526
            Mean episode rew_total: -0.0315
        Mean episode terrain_level: 3.2491
    Mean episode min_command_x_vel: -0.5994
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1665
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 2.19s
                        Total time: 61.45s
                               ETA: 22699.5s

################################################################################
                     [1m Learning iteration 27/10000 [0m                      

                       Computation: 45653 steps/s (collection: 1.790s, learning 0.364s)
               Value function loss: 0.0088
           Forward prediction loss: 0.0000
                          CMT loss: 0.0032
                    Surrogate loss: -0.0133
                    Policy entropy: 14.8411
             Mean action noise std: 0.83
                       Mean reward: -0.09
               Mean episode length: 391.83
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0788
       Mean episode rew_ang_vel_xy: -0.0931
        Mean episode rew_collision: -0.0288
          Mean episode rew_dof_acc: -0.1024
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0271
          Mean episode rew_torques: -0.0252
 Mean episode rew_tracking_ang_vel: 0.0844
 Mean episode rew_tracking_lin_vel: 0.2609
            Mean episode rew_total: -0.0102
        Mean episode terrain_level: 3.2216
    Mean episode min_command_x_vel: -0.5995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1664
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 2.15s
                        Total time: 63.60s
                               ETA: 22653.5s

################################################################################
                     [1m Learning iteration 28/10000 [0m                      

                       Computation: 46432 steps/s (collection: 1.752s, learning 0.365s)
               Value function loss: 0.0078
           Forward prediction loss: 0.0000
                          CMT loss: 0.0030
                    Surrogate loss: -0.0127
                    Policy entropy: 14.7660
             Mean action noise std: 0.83
                       Mean reward: -0.16
               Mean episode length: 391.09
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0720
       Mean episode rew_ang_vel_xy: -0.0869
        Mean episode rew_collision: -0.0275
          Mean episode rew_dof_acc: -0.0928
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0264
          Mean episode rew_torques: -0.0241
 Mean episode rew_tracking_ang_vel: 0.0803
 Mean episode rew_tracking_lin_vel: 0.2501
            Mean episode rew_total: 0.0007
        Mean episode terrain_level: 3.1952
    Mean episode min_command_x_vel: -0.5995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1664
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 2.12s
                        Total time: 65.72s
                               ETA: 22598.2s

################################################################################
                     [1m Learning iteration 29/10000 [0m                      

                       Computation: 45858 steps/s (collection: 1.780s, learning 0.364s)
               Value function loss: 0.0077
           Forward prediction loss: 0.0000
                          CMT loss: 0.0029
                    Surrogate loss: -0.0083
                    Policy entropy: 14.7022
             Mean action noise std: 0.82
                       Mean reward: 0.18
               Mean episode length: 383.63
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0760
       Mean episode rew_ang_vel_xy: -0.0915
        Mean episode rew_collision: -0.0355
          Mean episode rew_dof_acc: -0.0975
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0280
          Mean episode rew_torques: -0.0246
 Mean episode rew_tracking_ang_vel: 0.0876
 Mean episode rew_tracking_lin_vel: 0.2662
            Mean episode rew_total: 0.0008
        Mean episode terrain_level: 3.1733
    Mean episode min_command_x_vel: -0.5995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1664
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 2.14s
                        Total time: 67.86s
                               ETA: 22555.2s

################################################################################
                     [1m Learning iteration 30/10000 [0m                      

                       Computation: 48885 steps/s (collection: 1.648s, learning 0.363s)
               Value function loss: 0.0073
           Forward prediction loss: 0.0000
                          CMT loss: 0.0026
                    Surrogate loss: -0.0074
                    Policy entropy: 14.6436
             Mean action noise std: 0.82
                       Mean reward: 0.48
               Mean episode length: 313.26
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0544
       Mean episode rew_ang_vel_xy: -0.0684
        Mean episode rew_collision: -0.0152
          Mean episode rew_dof_acc: -0.0687
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0212
          Mean episode rew_torques: -0.0176
 Mean episode rew_tracking_ang_vel: 0.0630
 Mean episode rew_tracking_lin_vel: 0.2022
            Mean episode rew_total: 0.0197
        Mean episode terrain_level: 3.1552
    Mean episode min_command_x_vel: -0.5995
    Mean episode max_command_x_vel: 0.5984
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1664
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 2.01s
                        Total time: 69.87s
                               ETA: 22472.2s

################################################################################
                     [1m Learning iteration 31/10000 [0m                      

                       Computation: 48135 steps/s (collection: 1.678s, learning 0.364s)
               Value function loss: 0.0071
           Forward prediction loss: 0.0000
                          CMT loss: 0.0026
                    Surrogate loss: -0.0085
                    Policy entropy: 14.5870
             Mean action noise std: 0.82
                       Mean reward: 0.46
               Mean episode length: 341.98
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0637
       Mean episode rew_ang_vel_xy: -0.0760
        Mean episode rew_collision: -0.0291
          Mean episode rew_dof_acc: -0.0793
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0233
          Mean episode rew_torques: -0.0209
 Mean episode rew_tracking_ang_vel: 0.0744
 Mean episode rew_tracking_lin_vel: 0.2309
            Mean episode rew_total: 0.0131
        Mean episode terrain_level: 3.1418
    Mean episode min_command_x_vel: -0.5995
    Mean episode max_command_x_vel: 0.5981
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1664
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 2.04s
                        Total time: 71.92s
                               ETA: 22403.9s

################################################################################
                     [1m Learning iteration 32/10000 [0m                      

                       Computation: 46612 steps/s (collection: 1.744s, learning 0.365s)
               Value function loss: 0.0073
           Forward prediction loss: 0.0000
                          CMT loss: 0.0025
                    Surrogate loss: -0.0109
                    Policy entropy: 14.5146
             Mean action noise std: 0.81
                       Mean reward: 0.70
               Mean episode length: 362.73
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0683
       Mean episode rew_ang_vel_xy: -0.0818
        Mean episode rew_collision: -0.0212
          Mean episode rew_dof_acc: -0.0837
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0227
          Mean episode rew_torques: -0.0230
 Mean episode rew_tracking_ang_vel: 0.0814
 Mean episode rew_tracking_lin_vel: 0.2579
            Mean episode rew_total: 0.0386
        Mean episode terrain_level: 3.1263
    Mean episode min_command_x_vel: -0.5995
    Mean episode max_command_x_vel: 0.5976
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1664
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 2.11s
                        Total time: 74.02s
                               ETA: 22359.9s

################################################################################
                     [1m Learning iteration 33/10000 [0m                      

                       Computation: 48617 steps/s (collection: 1.657s, learning 0.365s)
               Value function loss: 0.0065
           Forward prediction loss: 0.0000
                          CMT loss: 0.0023
                    Surrogate loss: -0.0099
                    Policy entropy: 14.4311
             Mean action noise std: 0.81
                       Mean reward: 0.07
               Mean episode length: 352.32
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0613
       Mean episode rew_ang_vel_xy: -0.0760
        Mean episode rew_collision: -0.0310
          Mean episode rew_dof_acc: -0.0761
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0221
          Mean episode rew_torques: -0.0209
 Mean episode rew_tracking_ang_vel: 0.0765
 Mean episode rew_tracking_lin_vel: 0.2078
            Mean episode rew_total: -0.0030
        Mean episode terrain_level: 3.1132
    Mean episode min_command_x_vel: -0.5995
    Mean episode max_command_x_vel: 0.5975
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1664
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 2.02s
                        Total time: 76.05s
                               ETA: 22292.8s

################################################################################
                     [1m Learning iteration 34/10000 [0m                      

                       Computation: 50031 steps/s (collection: 1.601s, learning 0.364s)
               Value function loss: 0.0065
           Forward prediction loss: 0.0000
                          CMT loss: 0.0022
                    Surrogate loss: -0.0097
                    Policy entropy: 14.3603
             Mean action noise std: 0.80
                       Mean reward: 0.07
               Mean episode length: 356.48
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0699
       Mean episode rew_ang_vel_xy: -0.0870
        Mean episode rew_collision: -0.0317
          Mean episode rew_dof_acc: -0.0862
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0253
          Mean episode rew_torques: -0.0221
 Mean episode rew_tracking_ang_vel: 0.0861
 Mean episode rew_tracking_lin_vel: 0.2427
            Mean episode rew_total: 0.0064
        Mean episode terrain_level: 3.1027
    Mean episode min_command_x_vel: -0.5995
    Mean episode max_command_x_vel: 0.5979
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1664
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 1.96s
                        Total time: 78.01s
                               ETA: 22213.2s

################################################################################
                     [1m Learning iteration 35/10000 [0m                      

                       Computation: 48351 steps/s (collection: 1.669s, learning 0.364s)
               Value function loss: 0.0067
           Forward prediction loss: 0.0000
                          CMT loss: 0.0022
                    Surrogate loss: -0.0095
                    Policy entropy: 14.2857
             Mean action noise std: 0.79
                       Mean reward: 0.45
               Mean episode length: 382.25
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0710
       Mean episode rew_ang_vel_xy: -0.0812
        Mean episode rew_collision: -0.0423
          Mean episode rew_dof_acc: -0.0828
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0225
          Mean episode rew_torques: -0.0241
 Mean episode rew_tracking_ang_vel: 0.0916
 Mean episode rew_tracking_lin_vel: 0.2578
            Mean episode rew_total: 0.0255
        Mean episode terrain_level: 3.0929
    Mean episode min_command_x_vel: -0.5995
    Mean episode max_command_x_vel: 0.5923
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1664
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 2.03s
                        Total time: 80.04s
                               ETA: 22156.7s

################################################################################
                     [1m Learning iteration 36/10000 [0m                      

                       Computation: 48166 steps/s (collection: 1.676s, learning 0.365s)
               Value function loss: 0.0067
           Forward prediction loss: 0.0000
                          CMT loss: 0.0020
                    Surrogate loss: -0.0096
                    Policy entropy: 14.1898
             Mean action noise std: 0.79
                       Mean reward: 0.58
               Mean episode length: 420.06
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0762
       Mean episode rew_ang_vel_xy: -0.0920
        Mean episode rew_collision: -0.0294
          Mean episode rew_dof_acc: -0.0974
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0241
          Mean episode rew_torques: -0.0257
 Mean episode rew_tracking_ang_vel: 0.0963
 Mean episode rew_tracking_lin_vel: 0.2813
            Mean episode rew_total: 0.0328
        Mean episode terrain_level: 3.0814
    Mean episode min_command_x_vel: -0.5995
    Mean episode max_command_x_vel: 0.5916
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1664
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 2.04s
                        Total time: 82.09s
                               ETA: 22105.4s

################################################################################
                     [1m Learning iteration 37/10000 [0m                      

                       Computation: 49530 steps/s (collection: 1.619s, learning 0.366s)
               Value function loss: 0.0057
           Forward prediction loss: 0.0000
                          CMT loss: 0.0020
                    Surrogate loss: -0.0075
                    Policy entropy: 14.1201
             Mean action noise std: 0.78
                       Mean reward: 0.84
               Mean episode length: 434.67
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0699
       Mean episode rew_ang_vel_xy: -0.0839
        Mean episode rew_collision: -0.0602
          Mean episode rew_dof_acc: -0.0856
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0232
          Mean episode rew_torques: -0.0228
 Mean episode rew_tracking_ang_vel: 0.0951
 Mean episode rew_tracking_lin_vel: 0.2774
            Mean episode rew_total: 0.0267
        Mean episode terrain_level: 3.0731
    Mean episode min_command_x_vel: -0.5995
    Mean episode max_command_x_vel: 0.5946
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1664
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1665
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 1.98s
                        Total time: 84.07s
                               ETA: 22041.8s

################################################################################
                     [1m Learning iteration 38/10000 [0m                      

                       Computation: 49847 steps/s (collection: 1.607s, learning 0.365s)
               Value function loss: 0.0058
           Forward prediction loss: 0.0000
                          CMT loss: 0.0020
                    Surrogate loss: -0.0086
                    Policy entropy: 14.0285
             Mean action noise std: 0.78
                       Mean reward: 1.00
               Mean episode length: 399.86
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0655
       Mean episode rew_ang_vel_xy: -0.0741
        Mean episode rew_collision: -0.0333
          Mean episode rew_dof_acc: -0.0746
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0193
          Mean episode rew_torques: -0.0231
 Mean episode rew_tracking_ang_vel: 0.0906
 Mean episode rew_tracking_lin_vel: 0.2606
            Mean episode rew_total: 0.0612
        Mean episode terrain_level: 3.0655
    Mean episode min_command_x_vel: -0.5995
    Mean episode max_command_x_vel: 0.5968
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1665
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1665
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 1.97s
                        Total time: 86.04s
                               ETA: 21978.3s

################################################################################
                     [1m Learning iteration 39/10000 [0m                      

                       Computation: 47854 steps/s (collection: 1.687s, learning 0.367s)
               Value function loss: 0.0055
           Forward prediction loss: 0.0000
                          CMT loss: 0.0019
                    Surrogate loss: -0.0070
                    Policy entropy: 13.9713
             Mean action noise std: 0.77
                       Mean reward: 1.41
               Mean episode length: 408.96
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0780
       Mean episode rew_ang_vel_xy: -0.0901
        Mean episode rew_collision: -0.0431
          Mean episode rew_dof_acc: -0.0876
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0221
          Mean episode rew_torques: -0.0252
 Mean episode rew_tracking_ang_vel: 0.1108
 Mean episode rew_tracking_lin_vel: 0.3271
            Mean episode rew_total: 0.0918
        Mean episode terrain_level: 3.0590
    Mean episode min_command_x_vel: -0.5995
    Mean episode max_command_x_vel: 0.5979
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1665
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1665
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 2.05s
                        Total time: 88.10s
                               ETA: 21938.2s

################################################################################
                     [1m Learning iteration 40/10000 [0m                      

                       Computation: 48756 steps/s (collection: 1.652s, learning 0.365s)
               Value function loss: 0.0049
           Forward prediction loss: 0.0000
                          CMT loss: 0.0019
                    Surrogate loss: -0.0072
                    Policy entropy: 13.8948
             Mean action noise std: 0.77
                       Mean reward: 1.95
               Mean episode length: 421.92
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0764
       Mean episode rew_ang_vel_xy: -0.0854
        Mean episode rew_collision: -0.0152
          Mean episode rew_dof_acc: -0.0838
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0231
          Mean episode rew_torques: -0.0259
 Mean episode rew_tracking_ang_vel: 0.1059
 Mean episode rew_tracking_lin_vel: 0.3201
            Mean episode rew_total: 0.1162
        Mean episode terrain_level: 3.0532
    Mean episode min_command_x_vel: -0.5995
    Mean episode max_command_x_vel: 0.5982
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1665
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1665
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 2.02s
                        Total time: 90.11s
                               ETA: 21890.8s

################################################################################
                     [1m Learning iteration 41/10000 [0m                      

                       Computation: 30061 steps/s (collection: 2.903s, learning 0.367s)
               Value function loss: 0.0223
           Forward prediction loss: 0.0000
                          CMT loss: 0.0033
                    Surrogate loss: -0.0039
                    Policy entropy: 13.8309
             Mean action noise std: 0.77
                       Mean reward: 3.87
               Mean episode length: 968.44
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0891
       Mean episode rew_ang_vel_xy: -0.1010
        Mean episode rew_collision: -0.0408
          Mean episode rew_dof_acc: -0.1105
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0234
          Mean episode rew_torques: -0.0304
 Mean episode rew_tracking_ang_vel: 0.1339
 Mean episode rew_tracking_lin_vel: 0.3625
            Mean episode rew_total: 0.1011
        Mean episode terrain_level: 2.9921
    Mean episode min_command_x_vel: -0.5995
    Mean episode max_command_x_vel: 0.5989
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1665
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 3.27s
                        Total time: 93.38s
                               ETA: 22142.8s

################################################################################
                     [1m Learning iteration 42/10000 [0m                      

                       Computation: 47468 steps/s (collection: 1.705s, learning 0.366s)
               Value function loss: 0.0065
           Forward prediction loss: 0.0000
                          CMT loss: 0.0025
                    Surrogate loss: -0.0019
                    Policy entropy: 13.8147
             Mean action noise std: 0.77
                       Mean reward: 3.09
               Mean episode length: 780.99
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0756
       Mean episode rew_ang_vel_xy: -0.0831
        Mean episode rew_collision: -0.0348
          Mean episode rew_dof_acc: -0.0799
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0214
          Mean episode rew_torques: -0.0244
 Mean episode rew_tracking_ang_vel: 0.1101
 Mean episode rew_tracking_lin_vel: 0.3202
            Mean episode rew_total: 0.1110
        Mean episode terrain_level: 2.8798
    Mean episode min_command_x_vel: -0.5995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1665
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 2.07s
                        Total time: 95.45s
                               ETA: 22105.3s

################################################################################
                     [1m Learning iteration 43/10000 [0m                      

                       Computation: 45525 steps/s (collection: 1.796s, learning 0.364s)
               Value function loss: 0.0062
           Forward prediction loss: 0.0000
                          CMT loss: 0.0022
                    Surrogate loss: -0.0046
                    Policy entropy: 13.7790
             Mean action noise std: 0.76
                       Mean reward: 2.60
               Mean episode length: 525.62
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0874
       Mean episode rew_ang_vel_xy: -0.0920
        Mean episode rew_collision: -0.0374
          Mean episode rew_dof_acc: -0.0915
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0219
          Mean episode rew_torques: -0.0305
 Mean episode rew_tracking_ang_vel: 0.1201
 Mean episode rew_tracking_lin_vel: 0.3524
            Mean episode rew_total: 0.1118
        Mean episode terrain_level: 2.8712
    Mean episode min_command_x_vel: -0.5995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1665
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 2.16s
                        Total time: 97.61s
                               ETA: 22089.4s

################################################################################
                     [1m Learning iteration 44/10000 [0m                      

                       Computation: 44312 steps/s (collection: 1.854s, learning 0.364s)
               Value function loss: 0.0069
           Forward prediction loss: 0.0000
                          CMT loss: 0.0025
                    Surrogate loss: -0.0068
                    Policy entropy: 13.7239
             Mean action noise std: 0.76
                       Mean reward: 3.72
               Mean episode length: 665.58
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1229
       Mean episode rew_ang_vel_xy: -0.1307
        Mean episode rew_collision: -0.0348
          Mean episode rew_dof_acc: -0.1292
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0299
          Mean episode rew_torques: -0.0417
 Mean episode rew_tracking_ang_vel: 0.1658
 Mean episode rew_tracking_lin_vel: 0.5084
            Mean episode rew_total: 0.1851
        Mean episode terrain_level: 2.8542
    Mean episode min_command_x_vel: -0.5995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1665
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 2.22s
                        Total time: 99.83s
                               ETA: 22087.1s

################################################################################
                     [1m Learning iteration 45/10000 [0m                      

                       Computation: 43999 steps/s (collection: 1.870s, learning 0.365s)
               Value function loss: 0.0064
           Forward prediction loss: 0.0000
                          CMT loss: 0.0024
                    Surrogate loss: -0.0071
                    Policy entropy: 13.6684
             Mean action noise std: 0.76
                       Mean reward: 4.14
               Mean episode length: 705.50
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1296
       Mean episode rew_ang_vel_xy: -0.1361
        Mean episode rew_collision: -0.0413
          Mean episode rew_dof_acc: -0.1329
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0306
          Mean episode rew_torques: -0.0446
 Mean episode rew_tracking_ang_vel: 0.1803
 Mean episode rew_tracking_lin_vel: 0.5346
            Mean episode rew_total: 0.1999
        Mean episode terrain_level: 2.8302
    Mean episode min_command_x_vel: -0.5995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1665
  Mean episode min_command_yaw_vel: -0.1666
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 2.23s
                        Total time: 102.07s
                               ETA: 22088.3s

################################################################################
                     [1m Learning iteration 46/10000 [0m                      

                       Computation: 42763 steps/s (collection: 1.934s, learning 0.364s)
               Value function loss: 0.0068
           Forward prediction loss: 0.0000
                          CMT loss: 0.0024
                    Surrogate loss: -0.0080
                    Policy entropy: 13.6233
             Mean action noise std: 0.75
                       Mean reward: 4.68
               Mean episode length: 796.47
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1349
       Mean episode rew_ang_vel_xy: -0.1402
        Mean episode rew_collision: -0.0501
          Mean episode rew_dof_acc: -0.1354
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0293
          Mean episode rew_torques: -0.0467
 Mean episode rew_tracking_ang_vel: 0.1896
 Mean episode rew_tracking_lin_vel: 0.5515
            Mean episode rew_total: 0.2046
        Mean episode terrain_level: 2.8094
    Mean episode min_command_x_vel: -0.5995
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1665
  Mean episode min_command_yaw_vel: -0.1667
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 2.30s
                        Total time: 104.36s
                               ETA: 22103.0s

################################################################################
                     [1m Learning iteration 47/10000 [0m                      

                       Computation: 43517 steps/s (collection: 1.895s, learning 0.363s)
               Value function loss: 0.0062
           Forward prediction loss: 0.0000
                          CMT loss: 0.0024
                    Surrogate loss: -0.0054
                    Policy entropy: 13.5627
             Mean action noise std: 0.75
                       Mean reward: 4.44
               Mean episode length: 739.39
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1327
       Mean episode rew_ang_vel_xy: -0.1376
        Mean episode rew_collision: -0.0380
          Mean episode rew_dof_acc: -0.1326
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0295
          Mean episode rew_torques: -0.0469
 Mean episode rew_tracking_ang_vel: 0.1933
 Mean episode rew_tracking_lin_vel: 0.5509
            Mean episode rew_total: 0.2269
        Mean episode terrain_level: 2.7918
    Mean episode min_command_x_vel: -0.5995
    Mean episode max_command_x_vel: 0.5988
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1665
  Mean episode min_command_yaw_vel: -0.1667
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 2.26s
                        Total time: 106.62s
                               ETA: 22108.8s

################################################################################
                     [1m Learning iteration 48/10000 [0m                      

                       Computation: 44546 steps/s (collection: 1.842s, learning 0.365s)
               Value function loss: 0.0059
           Forward prediction loss: 0.0000
                          CMT loss: 0.0024
                    Surrogate loss: -0.0066
                    Policy entropy: 13.5184
             Mean action noise std: 0.75
                       Mean reward: 4.62
               Mean episode length: 742.17
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1302
       Mean episode rew_ang_vel_xy: -0.1377
        Mean episode rew_collision: -0.0384
          Mean episode rew_dof_acc: -0.1317
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0291
          Mean episode rew_torques: -0.0456
 Mean episode rew_tracking_ang_vel: 0.1918
 Mean episode rew_tracking_lin_vel: 0.5533
            Mean episode rew_total: 0.2325
        Mean episode terrain_level: 2.7757
    Mean episode min_command_x_vel: -0.5993
    Mean episode max_command_x_vel: 0.5992
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1665
  Mean episode min_command_yaw_vel: -0.1667
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 2.21s
                        Total time: 108.83s
                               ETA: 22103.6s

################################################################################
                     [1m Learning iteration 49/10000 [0m                      

                       Computation: 43818 steps/s (collection: 1.879s, learning 0.364s)
               Value function loss: 0.0056
           Forward prediction loss: 0.0000
                          CMT loss: 0.0022
                    Surrogate loss: -0.0056
                    Policy entropy: 13.4446
             Mean action noise std: 0.74
                       Mean reward: 4.78
               Mean episode length: 746.37
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1202
       Mean episode rew_ang_vel_xy: -0.1230
        Mean episode rew_collision: -0.0584
          Mean episode rew_dof_acc: -0.1183
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0259
          Mean episode rew_torques: -0.0432
 Mean episode rew_tracking_ang_vel: 0.1843
 Mean episode rew_tracking_lin_vel: 0.5237
            Mean episode rew_total: 0.2190
        Mean episode terrain_level: 2.7628
    Mean episode min_command_x_vel: -0.5995
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1665
  Mean episode min_command_yaw_vel: -0.1667
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 2.24s
                        Total time: 111.07s
                               ETA: 22105.9s

################################################################################
                     [1m Learning iteration 50/10000 [0m                      

                       Computation: 45116 steps/s (collection: 1.815s, learning 0.364s)
               Value function loss: 0.0052
           Forward prediction loss: 0.0000
                          CMT loss: 0.0022
                    Surrogate loss: -0.0063
                    Policy entropy: 13.3922
             Mean action noise std: 0.74
                       Mean reward: 5.44
               Mean episode length: 782.87
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1286
       Mean episode rew_ang_vel_xy: -0.1332
        Mean episode rew_collision: -0.0382
          Mean episode rew_dof_acc: -0.1272
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0273
          Mean episode rew_torques: -0.0450
 Mean episode rew_tracking_ang_vel: 0.1953
 Mean episode rew_tracking_lin_vel: 0.5709
            Mean episode rew_total: 0.2668
        Mean episode terrain_level: 2.7484
    Mean episode min_command_x_vel: -0.5998
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1665
  Mean episode min_command_yaw_vel: -0.1667
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 2.18s
                        Total time: 113.25s
                               ETA: 22095.3s

################################################################################
                     [1m Learning iteration 51/10000 [0m                      

                       Computation: 46682 steps/s (collection: 1.739s, learning 0.367s)
               Value function loss: 0.0045
           Forward prediction loss: 0.0000
                          CMT loss: 0.0020
                    Surrogate loss: -0.0073
                    Policy entropy: 13.3150
             Mean action noise std: 0.73
                       Mean reward: 6.50
               Mean episode length: 798.47
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1378
       Mean episode rew_ang_vel_xy: -0.1391
        Mean episode rew_collision: -0.0333
          Mean episode rew_dof_acc: -0.1307
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0275
          Mean episode rew_torques: -0.0476
 Mean episode rew_tracking_ang_vel: 0.2219
 Mean episode rew_tracking_lin_vel: 0.6310
            Mean episode rew_total: 0.3368
        Mean episode terrain_level: 2.7350
    Mean episode min_command_x_vel: -0.5998
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1665
  Mean episode min_command_yaw_vel: -0.1667
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 2.11s
                        Total time: 115.36s
                               ETA: 22071.1s

################################################################################
                     [1m Learning iteration 52/10000 [0m                      

                       Computation: 46148 steps/s (collection: 1.766s, learning 0.365s)
               Value function loss: 0.0049
           Forward prediction loss: 0.0000
                          CMT loss: 0.0019
                    Surrogate loss: -0.0062
                    Policy entropy: 13.2648
             Mean action noise std: 0.73
                       Mean reward: 6.48
               Mean episode length: 845.62
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1428
       Mean episode rew_ang_vel_xy: -0.1420
        Mean episode rew_collision: -0.0451
          Mean episode rew_dof_acc: -0.1329
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0270
          Mean episode rew_torques: -0.0519
 Mean episode rew_tracking_ang_vel: 0.2228
 Mean episode rew_tracking_lin_vel: 0.6394
            Mean episode rew_total: 0.3207
        Mean episode terrain_level: 2.7255
    Mean episode min_command_x_vel: -0.5998
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1667
    Mean episode max_command_y_vel: 0.1665
  Mean episode min_command_yaw_vel: -0.1667
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 2.13s
                        Total time: 117.49s
                               ETA: 22052.3s

################################################################################
                     [1m Learning iteration 53/10000 [0m                      

                       Computation: 47051 steps/s (collection: 1.722s, learning 0.367s)
               Value function loss: 0.0044
           Forward prediction loss: 0.0000
                          CMT loss: 0.0020
                    Surrogate loss: -0.0067
                    Policy entropy: 13.1966
             Mean action noise std: 0.73
                       Mean reward: 6.50
               Mean episode length: 842.15
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1219
       Mean episode rew_ang_vel_xy: -0.1249
        Mean episode rew_collision: -0.0523
          Mean episode rew_dof_acc: -0.1157
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0240
          Mean episode rew_torques: -0.0445
 Mean episode rew_tracking_ang_vel: 0.2050
 Mean episode rew_tracking_lin_vel: 0.5628
            Mean episode rew_total: 0.2844
        Mean episode terrain_level: 2.7155
    Mean episode min_command_x_vel: -0.5998
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1664
    Mean episode max_command_y_vel: 0.1665
  Mean episode min_command_yaw_vel: -0.1667
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 2.09s
                        Total time: 119.58s
                               ETA: 22026.7s

################################################################################
                     [1m Learning iteration 54/10000 [0m                      

                       Computation: 46558 steps/s (collection: 1.746s, learning 0.365s)
               Value function loss: 0.0043
           Forward prediction loss: 0.0000
                          CMT loss: 0.0020
                    Surrogate loss: -0.0038
                    Policy entropy: 13.1556
             Mean action noise std: 0.72
                       Mean reward: 7.27
               Mean episode length: 863.77
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1381
       Mean episode rew_ang_vel_xy: -0.1377
        Mean episode rew_collision: -0.0360
          Mean episode rew_dof_acc: -0.1237
   Mean episode rew_dof_pos_limits: 0.0000
        Mean episode rew_lin_vel_z: -0.0264
          Mean episode rew_torques: -0.0496
 Mean episode rew_tracking_ang_vel: 0.2273
 Mean episode rew_tracking_lin_vel: 0.6439
            Mean episode rew_total: 0.3597
        Mean episode terrain_level: 2.7066
    Mean episode min_command_x_vel: -0.5998
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1664
    Mean episode max_command_y_vel: 0.1665
  Mean episode min_command_yaw_vel: -0.1667
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 2.11s
                        Total time: 121.69s
                               ETA: 22005.8s

################################################################################
                     [1m Learning iteration 55/10000 [0m                      

                       Computation: 46664 steps/s (collection: 1.743s, learning 0.364s)
               Value function loss: 0.0042
           Forward prediction loss: 0.0000
                          CMT loss: 0.0020
                    Surrogate loss: -0.0016
                    Policy entropy: 13.1183
             Mean action noise std: 0.72
                       Mean reward: 8.23
               Mean episode length: 942.69
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1529
       Mean episode rew_ang_vel_xy: -0.1527
        Mean episode rew_collision: -0.0378
          Mean episode rew_dof_acc: -0.1374
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0276
          Mean episode rew_torques: -0.0565
 Mean episode rew_tracking_ang_vel: 0.2571
 Mean episode rew_tracking_lin_vel: 0.7196
            Mean episode rew_total: 0.4117
        Mean episode terrain_level: 2.6971
    Mean episode min_command_x_vel: -0.5998
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1664
    Mean episode max_command_y_vel: 0.1665
  Mean episode min_command_yaw_vel: -0.1667
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 2.11s
                        Total time: 123.80s
                               ETA: 21984.8s

################################################################################
                     [1m Learning iteration 56/10000 [0m                      

                       Computation: 46029 steps/s (collection: 1.770s, learning 0.365s)
               Value function loss: 0.0049
           Forward prediction loss: 0.0000
                          CMT loss: 0.0020
                    Surrogate loss: -0.0047
                    Policy entropy: 13.0809
             Mean action noise std: 0.72
                       Mean reward: 8.29
               Mean episode length: 939.97
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1479
       Mean episode rew_ang_vel_xy: -0.1474
        Mean episode rew_collision: -0.0327
          Mean episode rew_dof_acc: -0.1376
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0265
          Mean episode rew_torques: -0.0537
 Mean episode rew_tracking_ang_vel: 0.2497
 Mean episode rew_tracking_lin_vel: 0.6941
            Mean episode rew_total: 0.3980
        Mean episode terrain_level: 2.6866
    Mean episode min_command_x_vel: -0.5998
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1664
    Mean episode max_command_y_vel: 0.1665
  Mean episode min_command_yaw_vel: -0.1667
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 2.14s
                        Total time: 125.93s
                               ETA: 21969.5s

################################################################################
                     [1m Learning iteration 57/10000 [0m                      

                       Computation: 45627 steps/s (collection: 1.790s, learning 0.365s)
               Value function loss: 0.0049
           Forward prediction loss: 0.0000
                          CMT loss: 0.0020
                    Surrogate loss: -0.0038
                    Policy entropy: 13.0331
             Mean action noise std: 0.72
                       Mean reward: 7.52
               Mean episode length: 904.86
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1367
       Mean episode rew_ang_vel_xy: -0.1395
        Mean episode rew_collision: -0.0518
          Mean episode rew_dof_acc: -0.1220
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0260
          Mean episode rew_torques: -0.0499
 Mean episode rew_tracking_ang_vel: 0.2367
 Mean episode rew_tracking_lin_vel: 0.6496
            Mean episode rew_total: 0.3605
        Mean episode terrain_level: 2.6764
    Mean episode min_command_x_vel: -0.5998
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1664
    Mean episode max_command_y_vel: 0.1665
  Mean episode min_command_yaw_vel: -0.1667
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 2.15s
                        Total time: 128.09s
                               ETA: 21957.9s

################################################################################
                     [1m Learning iteration 58/10000 [0m                      

                       Computation: 47014 steps/s (collection: 1.727s, learning 0.364s)
               Value function loss: 0.0042
           Forward prediction loss: 0.0000
                          CMT loss: 0.0021
                    Surrogate loss: -0.0018
                    Policy entropy: 13.0014
             Mean action noise std: 0.72
                       Mean reward: 8.12
               Mean episode length: 902.96
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1421
       Mean episode rew_ang_vel_xy: -0.1399
        Mean episode rew_collision: -0.0327
          Mean episode rew_dof_acc: -0.1247
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0251
          Mean episode rew_torques: -0.0529
 Mean episode rew_tracking_ang_vel: 0.2482
 Mean episode rew_tracking_lin_vel: 0.6695
            Mean episode rew_total: 0.4003
        Mean episode terrain_level: 2.6652
    Mean episode min_command_x_vel: -0.5998
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1664
    Mean episode max_command_y_vel: 0.1665
  Mean episode min_command_yaw_vel: -0.1667
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 2.09s
                        Total time: 130.18s
                               ETA: 21935.9s

################################################################################
                     [1m Learning iteration 59/10000 [0m                      

                       Computation: 45701 steps/s (collection: 1.786s, learning 0.365s)
               Value function loss: 0.0040
           Forward prediction loss: 0.0000
                          CMT loss: 0.0021
                    Surrogate loss: -0.0043
                    Policy entropy: 12.9640
             Mean action noise std: 0.71
                       Mean reward: 8.70
               Mean episode length: 950.09
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1481
       Mean episode rew_ang_vel_xy: -0.1462
        Mean episode rew_collision: -0.0465
          Mean episode rew_dof_acc: -0.1287
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0262
          Mean episode rew_torques: -0.0548
 Mean episode rew_tracking_ang_vel: 0.2655
 Mean episode rew_tracking_lin_vel: 0.7094
            Mean episode rew_total: 0.4244
        Mean episode terrain_level: 2.6558
    Mean episode min_command_x_vel: -0.5998
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1664
    Mean episode max_command_y_vel: 0.1665
  Mean episode min_command_yaw_vel: -0.1667
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 2.15s
                        Total time: 132.33s
                               ETA: 21924.5s

################################################################################
                     [1m Learning iteration 60/10000 [0m                      

                       Computation: 47769 steps/s (collection: 1.690s, learning 0.368s)
               Value function loss: 0.0047
           Forward prediction loss: 0.0000
                          CMT loss: 0.0021
                    Surrogate loss: -0.0053
                    Policy entropy: 12.8999
             Mean action noise std: 0.71
                       Mean reward: 8.45
               Mean episode length: 929.00
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1435
       Mean episode rew_ang_vel_xy: -0.1404
        Mean episode rew_collision: -0.0572
          Mean episode rew_dof_acc: -0.1222
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0242
          Mean episode rew_torques: -0.0534
 Mean episode rew_tracking_ang_vel: 0.2647
 Mean episode rew_tracking_lin_vel: 0.7122
            Mean episode rew_total: 0.4361
        Mean episode terrain_level: 2.6460
    Mean episode min_command_x_vel: -0.5998
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1664
    Mean episode max_command_y_vel: 0.1665
  Mean episode min_command_yaw_vel: -0.1667
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 2.06s
                        Total time: 134.39s
                               ETA: 21898.3s

################################################################################
                     [1m Learning iteration 61/10000 [0m                      

                       Computation: 49155 steps/s (collection: 1.634s, learning 0.366s)
               Value function loss: 0.0044
           Forward prediction loss: 0.0000
                          CMT loss: 0.0022
                    Surrogate loss: -0.0030
                    Policy entropy: 12.8437
             Mean action noise std: 0.71
                       Mean reward: 8.78
               Mean episode length: 949.13
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1446
       Mean episode rew_ang_vel_xy: -0.1450
        Mean episode rew_collision: -0.0221
          Mean episode rew_dof_acc: -0.1285
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0251
          Mean episode rew_torques: -0.0527
 Mean episode rew_tracking_ang_vel: 0.2666
 Mean episode rew_tracking_lin_vel: 0.7067
            Mean episode rew_total: 0.4553
        Mean episode terrain_level: 2.6361
    Mean episode min_command_x_vel: -0.5998
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1664
    Mean episode max_command_y_vel: 0.1665
  Mean episode min_command_yaw_vel: -0.1667
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 2.00s
                        Total time: 136.39s
                               ETA: 21863.5s

################################################################################
                     [1m Learning iteration 62/10000 [0m                      

                       Computation: 39501 steps/s (collection: 2.124s, learning 0.365s)
               Value function loss: 0.0186
           Forward prediction loss: 0.0000
                          CMT loss: 0.0021
                    Surrogate loss: -0.0030
                    Policy entropy: 12.8178
             Mean action noise std: 0.71
                       Mean reward: 8.84
               Mean episode length: 920.40
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1372
       Mean episode rew_ang_vel_xy: -0.1349
        Mean episode rew_collision: -0.0519
          Mean episode rew_dof_acc: -0.1221
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0238
          Mean episode rew_torques: -0.0508
 Mean episode rew_tracking_ang_vel: 0.2600
 Mean episode rew_tracking_lin_vel: 0.6871
            Mean episode rew_total: 0.4264
        Mean episode terrain_level: 2.6258
    Mean episode min_command_x_vel: -0.5998
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1665
    Mean episode max_command_y_vel: 0.1665
  Mean episode min_command_yaw_vel: -0.1667
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 2.49s
                        Total time: 138.87s
                               ETA: 21906.9s

################################################################################
                     [1m Learning iteration 63/10000 [0m                      

                       Computation: 47263 steps/s (collection: 1.712s, learning 0.368s)
               Value function loss: 0.0041
           Forward prediction loss: 0.0000
                          CMT loss: 0.0021
                    Surrogate loss: -0.0043
                    Policy entropy: 12.7861
             Mean action noise std: 0.70
                       Mean reward: 8.84
               Mean episode length: 936.40
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1307
       Mean episode rew_ang_vel_xy: -0.1258
        Mean episode rew_collision: -0.0490
          Mean episode rew_dof_acc: -0.1058
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0227
          Mean episode rew_torques: -0.0498
 Mean episode rew_tracking_ang_vel: 0.2512
 Mean episode rew_tracking_lin_vel: 0.6191
            Mean episode rew_total: 0.3866
        Mean episode terrain_level: 2.6193
    Mean episode min_command_x_vel: -0.5999
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1667
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 2.08s
                        Total time: 140.95s
                               ETA: 21885.3s

################################################################################
                     [1m Learning iteration 64/10000 [0m                      

                       Computation: 47792 steps/s (collection: 1.691s, learning 0.366s)
               Value function loss: 0.0046
           Forward prediction loss: 0.0000
                          CMT loss: 0.0022
                    Surrogate loss: -0.0046
                    Policy entropy: 12.7259
             Mean action noise std: 0.70
                       Mean reward: 9.33
               Mean episode length: 966.35
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1448
       Mean episode rew_ang_vel_xy: -0.1413
        Mean episode rew_collision: -0.0354
          Mean episode rew_dof_acc: -0.1226
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0243
          Mean episode rew_torques: -0.0541
 Mean episode rew_tracking_ang_vel: 0.2790
 Mean episode rew_tracking_lin_vel: 0.7170
            Mean episode rew_total: 0.4736
        Mean episode terrain_level: 2.6113
    Mean episode min_command_x_vel: -0.5999
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1667
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 2.06s
                        Total time: 143.01s
                               ETA: 21860.9s

################################################################################
                     [1m Learning iteration 65/10000 [0m                      

                       Computation: 45813 steps/s (collection: 1.783s, learning 0.363s)
               Value function loss: 0.0057
           Forward prediction loss: 0.0000
                          CMT loss: 0.0022
                    Surrogate loss: -0.0010
                    Policy entropy: 12.6909
             Mean action noise std: 0.70
                       Mean reward: 9.51
               Mean episode length: 982.88
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1452
       Mean episode rew_ang_vel_xy: -0.1429
        Mean episode rew_collision: -0.0333
          Mean episode rew_dof_acc: -0.1241
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0247
          Mean episode rew_torques: -0.0546
 Mean episode rew_tracking_ang_vel: 0.2853
 Mean episode rew_tracking_lin_vel: 0.7265
            Mean episode rew_total: 0.4870
        Mean episode terrain_level: 2.6009
    Mean episode min_command_x_vel: -0.5999
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1667
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 2.15s
                        Total time: 145.16s
                               ETA: 21850.5s

################################################################################
                     [1m Learning iteration 66/10000 [0m                      

                       Computation: 44070 steps/s (collection: 1.868s, learning 0.363s)
               Value function loss: 0.0057
           Forward prediction loss: 0.0000
                          CMT loss: 0.0022
                    Surrogate loss: -0.0021
                    Policy entropy: 12.6626
             Mean action noise std: 0.70
                       Mean reward: 10.63
               Mean episode length: 1002.00
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1463
       Mean episode rew_ang_vel_xy: -0.1420
        Mean episode rew_collision: -0.0297
          Mean episode rew_dof_acc: -0.1237
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0242
          Mean episode rew_torques: -0.0546
 Mean episode rew_tracking_ang_vel: 0.2902
 Mean episode rew_tracking_lin_vel: 0.7677
            Mean episode rew_total: 0.5373
        Mean episode terrain_level: 2.5902
    Mean episode min_command_x_vel: -0.5999
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1667
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 2.23s
                        Total time: 147.39s
                               ETA: 21852.9s

################################################################################
                     [1m Learning iteration 67/10000 [0m                      

                       Computation: 43441 steps/s (collection: 1.899s, learning 0.364s)
               Value function loss: 0.0055
           Forward prediction loss: 0.0000
                          CMT loss: 0.0023
                    Surrogate loss: -0.0044
                    Policy entropy: 12.6153
             Mean action noise std: 0.69
                       Mean reward: 10.53
               Mean episode length: 978.13
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1416
       Mean episode rew_ang_vel_xy: -0.1381
        Mean episode rew_collision: -0.0214
          Mean episode rew_dof_acc: -0.1189
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0235
          Mean episode rew_torques: -0.0529
 Mean episode rew_tracking_ang_vel: 0.2869
 Mean episode rew_tracking_lin_vel: 0.7296
            Mean episode rew_total: 0.5200
        Mean episode terrain_level: 2.5790
    Mean episode min_command_x_vel: -0.5999
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1665
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 2.26s
                        Total time: 149.65s
                               ETA: 21859.9s

################################################################################
                     [1m Learning iteration 68/10000 [0m                      

                       Computation: 45326 steps/s (collection: 1.805s, learning 0.364s)
               Value function loss: 0.0051
           Forward prediction loss: 0.0000
                          CMT loss: 0.0022
                    Surrogate loss: -0.0032
                    Policy entropy: 12.5734
             Mean action noise std: 0.69
                       Mean reward: 9.95
               Mean episode length: 973.13
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1414
       Mean episode rew_ang_vel_xy: -0.1367
        Mean episode rew_collision: -0.0218
          Mean episode rew_dof_acc: -0.1231
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0236
          Mean episode rew_torques: -0.0538
 Mean episode rew_tracking_ang_vel: 0.2869
 Mean episode rew_tracking_lin_vel: 0.7172
            Mean episode rew_total: 0.5037
        Mean episode terrain_level: 2.5665
    Mean episode min_command_x_vel: -0.5999
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1665
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 2.17s
                        Total time: 151.82s
                               ETA: 21853.1s

################################################################################
                     [1m Learning iteration 69/10000 [0m                      

                       Computation: 45377 steps/s (collection: 1.801s, learning 0.365s)
               Value function loss: 0.0058
           Forward prediction loss: 0.0000
                          CMT loss: 0.0023
                    Surrogate loss: -0.0031
                    Policy entropy: 12.5193
             Mean action noise std: 0.69
                       Mean reward: 9.88
               Mean episode length: 983.55
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1403
       Mean episode rew_ang_vel_xy: -0.1362
        Mean episode rew_collision: -0.0381
          Mean episode rew_dof_acc: -0.1172
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0234
          Mean episode rew_torques: -0.0543
 Mean episode rew_tracking_ang_vel: 0.2907
 Mean episode rew_tracking_lin_vel: 0.7096
            Mean episode rew_total: 0.4909
        Mean episode terrain_level: 2.5551
    Mean episode min_command_x_vel: -0.5999
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1665
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 2.17s
                        Total time: 153.99s
                               ETA: 21846.1s

################################################################################
                     [1m Learning iteration 70/10000 [0m                      

                       Computation: 44102 steps/s (collection: 1.866s, learning 0.363s)
               Value function loss: 0.0056
           Forward prediction loss: 0.0000
                          CMT loss: 0.0022
                    Surrogate loss: -0.0030
                    Policy entropy: 12.4783
             Mean action noise std: 0.69
                       Mean reward: 10.39
               Mean episode length: 1002.00
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1414
       Mean episode rew_ang_vel_xy: -0.1363
        Mean episode rew_collision: -0.0452
          Mean episode rew_dof_acc: -0.1145
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0229
          Mean episode rew_torques: -0.0535
 Mean episode rew_tracking_ang_vel: 0.2983
 Mean episode rew_tracking_lin_vel: 0.7544
            Mean episode rew_total: 0.5389
        Mean episode terrain_level: 2.5452
    Mean episode min_command_x_vel: -0.5999
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1665
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 2.23s
                        Total time: 156.21s
                               ETA: 21848.0s

################################################################################
                     [1m Learning iteration 71/10000 [0m                      

                       Computation: 45568 steps/s (collection: 1.793s, learning 0.364s)
               Value function loss: 0.0053
           Forward prediction loss: 0.0000
                          CMT loss: 0.0023
                    Surrogate loss: -0.0042
                    Policy entropy: 12.4369
             Mean action noise std: 0.68
                       Mean reward: 10.38
               Mean episode length: 992.50
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1342
       Mean episode rew_ang_vel_xy: -0.1309
        Mean episode rew_collision: -0.0378
          Mean episode rew_dof_acc: -0.1089
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0234
          Mean episode rew_torques: -0.0512
 Mean episode rew_tracking_ang_vel: 0.2871
 Mean episode rew_tracking_lin_vel: 0.7139
            Mean episode rew_total: 0.5147
        Mean episode terrain_level: 2.5348
    Mean episode min_command_x_vel: -0.5999
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1665
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 2.16s
                        Total time: 158.37s
                               ETA: 21839.9s

################################################################################
                     [1m Learning iteration 72/10000 [0m                      

                       Computation: 46811 steps/s (collection: 1.736s, learning 0.364s)
               Value function loss: 0.0044
           Forward prediction loss: 0.0000
                          CMT loss: 0.0022
                    Surrogate loss: -0.0043
                    Policy entropy: 12.3811
             Mean action noise std: 0.68
                       Mean reward: 10.09
               Mean episode length: 969.02
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1355
       Mean episode rew_ang_vel_xy: -0.1306
        Mean episode rew_collision: -0.0766
          Mean episode rew_dof_acc: -0.1151
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0217
          Mean episode rew_torques: -0.0535
 Mean episode rew_tracking_ang_vel: 0.2973
 Mean episode rew_tracking_lin_vel: 0.7044
            Mean episode rew_total: 0.4687
        Mean episode terrain_level: 2.5283
    Mean episode min_command_x_vel: -0.5999
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1665
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 2.10s
                        Total time: 160.47s
                               ETA: 21824.2s

################################################################################
                     [1m Learning iteration 73/10000 [0m                      

                       Computation: 46007 steps/s (collection: 1.774s, learning 0.363s)
               Value function loss: 0.0050
           Forward prediction loss: 0.0000
                          CMT loss: 0.0023
                    Surrogate loss: -0.0020
                    Policy entropy: 12.3430
             Mean action noise std: 0.68
                       Mean reward: 9.33
               Mean episode length: 962.36
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1323
       Mean episode rew_ang_vel_xy: -0.1287
        Mean episode rew_collision: -0.0949
          Mean episode rew_dof_acc: -0.1109
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0227
          Mean episode rew_torques: -0.0526
 Mean episode rew_tracking_ang_vel: 0.2859
 Mean episode rew_tracking_lin_vel: 0.6841
            Mean episode rew_total: 0.4278
        Mean episode terrain_level: 2.5226
    Mean episode min_command_x_vel: -0.5999
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1665
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 2.14s
                        Total time: 162.61s
                               ETA: 21813.7s

################################################################################
                     [1m Learning iteration 74/10000 [0m                      

                       Computation: 45401 steps/s (collection: 1.802s, learning 0.363s)
               Value function loss: 0.0045
           Forward prediction loss: 0.0000
                          CMT loss: 0.0025
                    Surrogate loss: -0.0006
                    Policy entropy: 12.3150
             Mean action noise std: 0.68
                       Mean reward: 10.20
               Mean episode length: 975.25
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1315
       Mean episode rew_ang_vel_xy: -0.1258
        Mean episode rew_collision: -0.0369
          Mean episode rew_dof_acc: -0.1087
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0218
          Mean episode rew_torques: -0.0523
 Mean episode rew_tracking_ang_vel: 0.2892
 Mean episode rew_tracking_lin_vel: 0.6826
            Mean episode rew_total: 0.4948
        Mean episode terrain_level: 2.5156
    Mean episode min_command_x_vel: -0.5999
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1665
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 2.17s
                        Total time: 164.77s
                               ETA: 21807.2s

################################################################################
                     [1m Learning iteration 75/10000 [0m                      

                       Computation: 46307 steps/s (collection: 1.757s, learning 0.366s)
               Value function loss: 0.0047
           Forward prediction loss: 0.0000
                          CMT loss: 0.0024
                    Surrogate loss: 0.0013
                    Policy entropy: 12.2936
             Mean action noise std: 0.68
                       Mean reward: 10.09
               Mean episode length: 956.39
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1280
       Mean episode rew_ang_vel_xy: -0.1226
        Mean episode rew_collision: -0.0357
          Mean episode rew_dof_acc: -0.1032
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0215
          Mean episode rew_torques: -0.0511
 Mean episode rew_tracking_ang_vel: 0.2857
 Mean episode rew_tracking_lin_vel: 0.7160
            Mean episode rew_total: 0.5396
        Mean episode terrain_level: 2.5097
    Mean episode min_command_x_vel: -0.5999
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1665
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 2.12s
                        Total time: 166.90s
                               ETA: 21795.4s

################################################################################
                     [1m Learning iteration 76/10000 [0m                      

                       Computation: 46878 steps/s (collection: 1.729s, learning 0.368s)
               Value function loss: 0.0044
           Forward prediction loss: 0.0000
                          CMT loss: 0.0024
                    Surrogate loss: -0.0031
                    Policy entropy: 12.2694
             Mean action noise std: 0.67
                       Mean reward: 10.34
               Mean episode length: 938.71
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1250
       Mean episode rew_ang_vel_xy: -0.1252
        Mean episode rew_collision: -0.0396
          Mean episode rew_dof_acc: -0.1060
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0238
          Mean episode rew_torques: -0.0495
 Mean episode rew_tracking_ang_vel: 0.2783
 Mean episode rew_tracking_lin_vel: 0.6762
            Mean episode rew_total: 0.4853
        Mean episode terrain_level: 2.5037
    Mean episode min_command_x_vel: -0.5999
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1665
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 2.10s
                        Total time: 168.99s
                               ETA: 21780.4s

################################################################################
                     [1m Learning iteration 77/10000 [0m                      

                       Computation: 45680 steps/s (collection: 1.785s, learning 0.367s)
               Value function loss: 0.0047
           Forward prediction loss: 0.0000
                          CMT loss: 0.0025
                    Surrogate loss: 0.0006
                    Policy entropy: 12.2343
             Mean action noise std: 0.67
                       Mean reward: 10.25
               Mean episode length: 963.76
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1336
       Mean episode rew_ang_vel_xy: -0.1275
        Mean episode rew_collision: -0.0675
          Mean episode rew_dof_acc: -0.1096
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0214
          Mean episode rew_torques: -0.0533
 Mean episode rew_tracking_ang_vel: 0.3061
 Mean episode rew_tracking_lin_vel: 0.7614
            Mean episode rew_total: 0.5545
        Mean episode terrain_level: 2.4979
    Mean episode min_command_x_vel: -0.5999
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1665
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 2.15s
                        Total time: 171.15s
                               ETA: 21772.8s

################################################################################
                     [1m Learning iteration 78/10000 [0m                      

                       Computation: 46559 steps/s (collection: 1.748s, learning 0.363s)
               Value function loss: 0.0044
           Forward prediction loss: 0.0000
                          CMT loss: 0.0025
                    Surrogate loss: -0.0007
                    Policy entropy: 12.2089
             Mean action noise std: 0.67
                       Mean reward: 10.53
               Mean episode length: 993.09
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1304
       Mean episode rew_ang_vel_xy: -0.1239
        Mean episode rew_collision: -0.0513
          Mean episode rew_dof_acc: -0.0979
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0215
          Mean episode rew_torques: -0.0532
 Mean episode rew_tracking_ang_vel: 0.3032
 Mean episode rew_tracking_lin_vel: 0.7405
            Mean episode rew_total: 0.5655
        Mean episode terrain_level: 2.4942
    Mean episode min_command_x_vel: -0.5999
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1665
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 2.11s
                        Total time: 173.26s
                               ETA: 21760.2s

################################################################################
                     [1m Learning iteration 79/10000 [0m                      

                       Computation: 46303 steps/s (collection: 1.756s, learning 0.367s)
               Value function loss: 0.0046
           Forward prediction loss: 0.0000
                          CMT loss: 0.0025
                    Surrogate loss: -0.0014
                    Policy entropy: 12.1726
             Mean action noise std: 0.67
                       Mean reward: 10.35
               Mean episode length: 975.46
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1280
       Mean episode rew_ang_vel_xy: -0.1209
        Mean episode rew_collision: -0.0622
          Mean episode rew_dof_acc: -0.1046
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0211
          Mean episode rew_torques: -0.0523
 Mean episode rew_tracking_ang_vel: 0.2983
 Mean episode rew_tracking_lin_vel: 0.6571
            Mean episode rew_total: 0.4662
        Mean episode terrain_level: 2.4907
    Mean episode min_command_x_vel: -0.5999
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1665
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 2.12s
                        Total time: 175.38s
                               ETA: 21749.3s

################################################################################
                     [1m Learning iteration 80/10000 [0m                      

                       Computation: 47313 steps/s (collection: 1.713s, learning 0.364s)
               Value function loss: 0.0048
           Forward prediction loss: 0.0000
                          CMT loss: 0.0024
                    Surrogate loss: -0.0016
                    Policy entropy: 12.1357
             Mean action noise std: 0.67
                       Mean reward: 9.81
               Mean episode length: 962.33
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1257
       Mean episode rew_ang_vel_xy: -0.1207
        Mean episode rew_collision: -0.0521
          Mean episode rew_dof_acc: -0.1078
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0228
          Mean episode rew_torques: -0.0493
 Mean episode rew_tracking_ang_vel: 0.2912
 Mean episode rew_tracking_lin_vel: 0.7041
            Mean episode rew_total: 0.5169
        Mean episode terrain_level: 2.4868
    Mean episode min_command_x_vel: -0.5999
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1665
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 2.08s
                        Total time: 177.46s
                               ETA: 21733.1s

################################################################################
                     [1m Learning iteration 81/10000 [0m                      

                       Computation: 48120 steps/s (collection: 1.679s, learning 0.364s)
               Value function loss: 0.0044
           Forward prediction loss: 0.0000
                          CMT loss: 0.0024
                    Surrogate loss: -0.0038
                    Policy entropy: 12.0839
             Mean action noise std: 0.66
                       Mean reward: 9.73
               Mean episode length: 959.15
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1264
       Mean episode rew_ang_vel_xy: -0.1201
        Mean episode rew_collision: -0.0776
          Mean episode rew_dof_acc: -0.1001
   Mean episode rew_dof_pos_limits: 0.0000
        Mean episode rew_lin_vel_z: -0.0210
          Mean episode rew_torques: -0.0506
 Mean episode rew_tracking_ang_vel: 0.3046
 Mean episode rew_tracking_lin_vel: 0.7173
            Mean episode rew_total: 0.5261
        Mean episode terrain_level: 2.4828
    Mean episode min_command_x_vel: -0.5999
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1665
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 2.04s
                        Total time: 179.50s
                               ETA: 21713.0s

################################################################################
                     [1m Learning iteration 82/10000 [0m                      

                       Computation: 46893 steps/s (collection: 1.728s, learning 0.368s)
               Value function loss: 0.0041
           Forward prediction loss: 0.0000
                          CMT loss: 0.0024
                    Surrogate loss: -0.0010
                    Policy entropy: 12.0388
             Mean action noise std: 0.66
                       Mean reward: 9.97
               Mean episode length: 980.28
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1285
       Mean episode rew_ang_vel_xy: -0.1217
        Mean episode rew_collision: -0.0693
          Mean episode rew_dof_acc: -0.1006
   Mean episode rew_dof_pos_limits: 0.0000
        Mean episode rew_lin_vel_z: -0.0206
          Mean episode rew_torques: -0.0527
 Mean episode rew_tracking_ang_vel: 0.3079
 Mean episode rew_tracking_lin_vel: 0.7177
            Mean episode rew_total: 0.5322
        Mean episode terrain_level: 2.4791
    Mean episode min_command_x_vel: -0.5999
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.1666
    Mean episode max_command_y_vel: 0.1666
  Mean episode min_command_yaw_vel: -0.1665
  Mean episode max_command_yaw_vel: 0.1666
         Mean episode command_area: 0.0667
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 2.10s
                        Total time: 181.60s
                               ETA: 21699.7s

################################################################################
                     [1m Learning iteration 83/10000 [0m                      

                       Computation: 31535 steps/s (collection: 2.749s, learning 0.368s)
               Value function loss: 0.0195
           Forward prediction loss: 0.0000
                          CMT loss: 0.0045
                    Surrogate loss: 0.0056
                    Policy entropy: 12.0122
             Mean action noise std: 0.66
                       Mean reward: 11.02
               Mean episode length: 992.80
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1222
       Mean episode rew_ang_vel_xy: -0.1128
        Mean episode rew_collision: -0.0405
          Mean episode rew_dof_acc: -0.0968
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0206
          Mean episode rew_torques: -0.0489
 Mean episode rew_tracking_ang_vel: 0.2934
 Mean episode rew_tracking_lin_vel: 0.7065
            Mean episode rew_total: 0.5579
        Mean episode terrain_level: 2.4022
    Mean episode min_command_x_vel: -0.5999
    Mean episode max_command_x_vel: 0.5992
    Mean episode min_command_y_vel: -0.3610
    Mean episode max_command_y_vel: 0.3761
  Mean episode min_command_yaw_vel: -0.3706
  Mean episode max_command_yaw_vel: 0.3778
         Mean episode command_area: 0.1378
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 3.12s
                        Total time: 184.71s
                               ETA: 21807.3s

################################################################################
                     [1m Learning iteration 84/10000 [0m                      

                       Computation: 46052 steps/s (collection: 1.767s, learning 0.367s)
               Value function loss: 0.0053
           Forward prediction loss: 0.0000
                          CMT loss: 0.0029
                    Surrogate loss: 0.0007
                    Policy entropy: 12.0062
             Mean action noise std: 0.66
                       Mean reward: 10.46
               Mean episode length: 969.97
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1211
       Mean episode rew_ang_vel_xy: -0.1152
        Mean episode rew_collision: -0.0494
          Mean episode rew_dof_acc: -0.1054
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0210
          Mean episode rew_torques: -0.0497
 Mean episode rew_tracking_ang_vel: 0.2941
 Mean episode rew_tracking_lin_vel: 0.6533
            Mean episode rew_total: 0.4856
        Mean episode terrain_level: 2.3466
    Mean episode min_command_x_vel: -0.5998
    Mean episode max_command_x_vel: 0.5991
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4991
  Mean episode max_command_yaw_vel: 0.4988
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 2.13s
                        Total time: 186.85s
                               ETA: 21797.5s

################################################################################
                     [1m Learning iteration 85/10000 [0m                      

                       Computation: 44868 steps/s (collection: 1.828s, learning 0.363s)
               Value function loss: 0.0060
           Forward prediction loss: 0.0000
                          CMT loss: 0.0031
                    Surrogate loss: -0.0001
                    Policy entropy: 11.9865
             Mean action noise std: 0.66
                       Mean reward: 10.84
               Mean episode length: 954.33
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1210
       Mean episode rew_ang_vel_xy: -0.1154
        Mean episode rew_collision: -0.0495
          Mean episode rew_dof_acc: -0.0990
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0210
          Mean episode rew_torques: -0.0504
 Mean episode rew_tracking_ang_vel: 0.3002
 Mean episode rew_tracking_lin_vel: 0.7268
            Mean episode rew_total: 0.5705
        Mean episode terrain_level: 2.3388
    Mean episode min_command_x_vel: -0.5998
    Mean episode max_command_x_vel: 0.5992
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4991
  Mean episode max_command_yaw_vel: 0.4988
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 2.19s
                        Total time: 189.04s
                               ETA: 21794.5s

################################################################################
                     [1m Learning iteration 86/10000 [0m                      

                       Computation: 41562 steps/s (collection: 2.002s, learning 0.363s)
               Value function loss: 0.0070
           Forward prediction loss: 0.0000
                          CMT loss: 0.0032
                    Surrogate loss: -0.0012
                    Policy entropy: 11.9618
             Mean action noise std: 0.66
                       Mean reward: 11.32
               Mean episode length: 982.93
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1244
       Mean episode rew_ang_vel_xy: -0.1163
        Mean episode rew_collision: -0.0384
          Mean episode rew_dof_acc: -0.1002
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0209
          Mean episode rew_torques: -0.0516
 Mean episode rew_tracking_ang_vel: 0.3101
 Mean episode rew_tracking_lin_vel: 0.6993
            Mean episode rew_total: 0.5575
        Mean episode terrain_level: 2.3234
    Mean episode min_command_x_vel: -0.5998
    Mean episode max_command_x_vel: 0.5993
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4991
  Mean episode max_command_yaw_vel: 0.4993
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 2.37s
                        Total time: 191.40s
                               ETA: 21811.3s

################################################################################
                     [1m Learning iteration 87/10000 [0m                      

                       Computation: 41703 steps/s (collection: 1.992s, learning 0.365s)
               Value function loss: 0.0062
           Forward prediction loss: 0.0000
                          CMT loss: 0.0032
                    Surrogate loss: -0.0020
                    Policy entropy: 11.9338
             Mean action noise std: 0.65
                       Mean reward: 11.30
               Mean episode length: 974.52
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1221
       Mean episode rew_ang_vel_xy: -0.1142
        Mean episode rew_collision: -0.0521
          Mean episode rew_dof_acc: -0.0990
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0205
          Mean episode rew_torques: -0.0509
 Mean episode rew_tracking_ang_vel: 0.3101
 Mean episode rew_tracking_lin_vel: 0.6977
            Mean episode rew_total: 0.5490
        Mean episode terrain_level: 2.3040
    Mean episode min_command_x_vel: -0.5998
    Mean episode max_command_x_vel: 0.5993
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4991
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 2.36s
                        Total time: 193.76s
                               ETA: 21826.8s

################################################################################
                     [1m Learning iteration 88/10000 [0m                      

                       Computation: 42201 steps/s (collection: 1.967s, learning 0.363s)
               Value function loss: 0.0070
           Forward prediction loss: 0.0000
                          CMT loss: 0.0033
                    Surrogate loss: -0.0012
                    Policy entropy: 11.9090
             Mean action noise std: 0.65
                       Mean reward: 10.77
               Mean episode length: 948.44
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1189
       Mean episode rew_ang_vel_xy: -0.1113
        Mean episode rew_collision: -0.0584
          Mean episode rew_dof_acc: -0.0956
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0207
          Mean episode rew_torques: -0.0495
 Mean episode rew_tracking_ang_vel: 0.2991
 Mean episode rew_tracking_lin_vel: 0.6795
            Mean episode rew_total: 0.5242
        Mean episode terrain_level: 2.2874
    Mean episode min_command_x_vel: -0.5998
    Mean episode max_command_x_vel: 0.5993
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4991
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 2.33s
                        Total time: 196.09s
                               ETA: 21838.8s

################################################################################
                     [1m Learning iteration 89/10000 [0m                      

                       Computation: 44123 steps/s (collection: 1.863s, learning 0.365s)
               Value function loss: 0.0067
           Forward prediction loss: 0.0000
                          CMT loss: 0.0033
                    Surrogate loss: -0.0013
                    Policy entropy: 11.8905
             Mean action noise std: 0.65
                       Mean reward: 11.62
               Mean episode length: 993.12
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1221
       Mean episode rew_ang_vel_xy: -0.1120
        Mean episode rew_collision: -0.0360
          Mean episode rew_dof_acc: -0.0957
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0201
          Mean episode rew_torques: -0.0512
 Mean episode rew_tracking_ang_vel: 0.3131
 Mean episode rew_tracking_lin_vel: 0.7053
            Mean episode rew_total: 0.5813
        Mean episode terrain_level: 2.2715
    Mean episode min_command_x_vel: -0.5998
    Mean episode max_command_x_vel: 0.5993
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4991
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 2.23s
                        Total time: 198.32s
                               ETA: 21839.4s

################################################################################
                     [1m Learning iteration 90/10000 [0m                      

                       Computation: 42653 steps/s (collection: 1.939s, learning 0.366s)
               Value function loss: 0.0063
           Forward prediction loss: 0.0000
                          CMT loss: 0.0033
                    Surrogate loss: -0.0007
                    Policy entropy: 11.8629
             Mean action noise std: 0.65
                       Mean reward: 11.06
               Mean episode length: 967.86
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1191
       Mean episode rew_ang_vel_xy: -0.1101
        Mean episode rew_collision: -0.0499
          Mean episode rew_dof_acc: -0.0948
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0202
          Mean episode rew_torques: -0.0501
 Mean episode rew_tracking_ang_vel: 0.3068
 Mean episode rew_tracking_lin_vel: 0.6974
            Mean episode rew_total: 0.5600
        Mean episode terrain_level: 2.2568
    Mean episode min_command_x_vel: -0.5998
    Mean episode max_command_x_vel: 0.5993
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4991
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 2.30s
                        Total time: 200.62s
                               ETA: 21848.2s

################################################################################
                     [1m Learning iteration 91/10000 [0m                      

                       Computation: 42634 steps/s (collection: 1.941s, learning 0.365s)
               Value function loss: 0.0059
           Forward prediction loss: 0.0000
                          CMT loss: 0.0033
                    Surrogate loss: 0.0002
                    Policy entropy: 11.8513
             Mean action noise std: 0.65
                       Mean reward: 10.37
               Mean episode length: 985.65
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1208
       Mean episode rew_ang_vel_xy: -0.1132
        Mean episode rew_collision: -0.0747
          Mean episode rew_dof_acc: -0.0982
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0206
          Mean episode rew_torques: -0.0515
 Mean episode rew_tracking_ang_vel: 0.3138
 Mean episode rew_tracking_lin_vel: 0.6911
            Mean episode rew_total: 0.5260
        Mean episode terrain_level: 2.2461
    Mean episode min_command_x_vel: -0.5998
    Mean episode max_command_x_vel: 0.5993
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4991
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 2.31s
                        Total time: 202.93s
                               ETA: 21856.8s

################################################################################
                     [1m Learning iteration 92/10000 [0m                      

                       Computation: 42801 steps/s (collection: 1.931s, learning 0.366s)
               Value function loss: 0.0068
           Forward prediction loss: 0.0000
                          CMT loss: 0.0034
                    Surrogate loss: -0.0007
                    Policy entropy: 11.8384
             Mean action noise std: 0.65
                       Mean reward: 10.89
               Mean episode length: 985.90
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1181
       Mean episode rew_ang_vel_xy: -0.1091
        Mean episode rew_collision: -0.0467
          Mean episode rew_dof_acc: -0.0934
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0200
          Mean episode rew_torques: -0.0501
 Mean episode rew_tracking_ang_vel: 0.3105
 Mean episode rew_tracking_lin_vel: 0.6655
            Mean episode rew_total: 0.5386
        Mean episode terrain_level: 2.2325
    Mean episode min_command_x_vel: -0.5998
    Mean episode max_command_x_vel: 0.5996
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4991
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 2.30s
                        Total time: 205.23s
                               ETA: 21864.3s

################################################################################
                     [1m Learning iteration 93/10000 [0m                      

                       Computation: 43296 steps/s (collection: 1.903s, learning 0.367s)
               Value function loss: 0.0052
           Forward prediction loss: 0.0000
                          CMT loss: 0.0033
                    Surrogate loss: 0.0022
                    Policy entropy: 11.8204
             Mean action noise std: 0.65
                       Mean reward: 10.93
               Mean episode length: 982.52
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1199
       Mean episode rew_ang_vel_xy: -0.1124
        Mean episode rew_collision: -0.0792
          Mean episode rew_dof_acc: -0.1009
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0202
          Mean episode rew_torques: -0.0513
 Mean episode rew_tracking_ang_vel: 0.3076
 Mean episode rew_tracking_lin_vel: 0.6778
            Mean episode rew_total: 0.5016
        Mean episode terrain_level: 2.2225
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4991
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 2.27s
                        Total time: 207.50s
                               ETA: 21868.8s

################################################################################
                     [1m Learning iteration 94/10000 [0m                      

                       Computation: 43554 steps/s (collection: 1.891s, learning 0.366s)
               Value function loss: 0.0063
           Forward prediction loss: 0.0000
                          CMT loss: 0.0032
                    Surrogate loss: -0.0002
                    Policy entropy: 11.8064
             Mean action noise std: 0.65
                       Mean reward: 11.24
               Mean episode length: 955.89
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1128
       Mean episode rew_ang_vel_xy: -0.1035
        Mean episode rew_collision: -0.0389
          Mean episode rew_dof_acc: -0.0879
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0194
          Mean episode rew_torques: -0.0487
 Mean episode rew_tracking_ang_vel: 0.2951
 Mean episode rew_tracking_lin_vel: 0.6620
            Mean episode rew_total: 0.5459
        Mean episode terrain_level: 2.2149
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4991
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 2.26s
                        Total time: 209.75s
                               ETA: 21871.8s

################################################################################
                     [1m Learning iteration 95/10000 [0m                      

                       Computation: 44288 steps/s (collection: 1.855s, learning 0.364s)
               Value function loss: 0.0056
           Forward prediction loss: 0.0000
                          CMT loss: 0.0032
                    Surrogate loss: -0.0004
                    Policy entropy: 11.7838
             Mean action noise std: 0.65
                       Mean reward: 10.83
               Mean episode length: 922.49
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1016
       Mean episode rew_ang_vel_xy: -0.0959
        Mean episode rew_collision: -0.0601
          Mean episode rew_dof_acc: -0.0844
   Mean episode rew_dof_pos_limits: -0.0001
        Mean episode rew_lin_vel_z: -0.0186
          Mean episode rew_torques: -0.0442
 Mean episode rew_tracking_ang_vel: 0.2719
 Mean episode rew_tracking_lin_vel: 0.6094
            Mean episode rew_total: 0.4765
        Mean episode terrain_level: 2.2059
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4991
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 2.22s
                        Total time: 211.97s
                               ETA: 21870.8s

################################################################################
                     [1m Learning iteration 96/10000 [0m                      

                       Computation: 42321 steps/s (collection: 1.958s, learning 0.365s)
               Value function loss: 0.0059
           Forward prediction loss: 0.0000
                          CMT loss: 0.0032
                    Surrogate loss: -0.0001
                    Policy entropy: 11.7547
             Mean action noise std: 0.65
                       Mean reward: 11.13
               Mean episode length: 964.00
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1156
       Mean episode rew_ang_vel_xy: -0.1052
        Mean episode rew_collision: -0.0643
          Mean episode rew_dof_acc: -0.0895
   Mean episode rew_dof_pos_limits: -0.0000
        Mean episode rew_lin_vel_z: -0.0195
          Mean episode rew_torques: -0.0498
 Mean episode rew_tracking_ang_vel: 0.3165
 Mean episode rew_tracking_lin_vel: 0.7079
            Mean episode rew_total: 0.5805
        Mean episode terrain_level: 2.1957
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4991
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 2.32s
                        Total time: 214.30s
                               ETA: 21880.3s

################################################################################
                     [1m Learning iteration 97/10000 [0m                      

                       Computation: 44688 steps/s (collection: 1.836s, learning 0.364s)
               Value function loss: 0.0061
           Forward prediction loss: 0.0000
                          CMT loss: 0.0033
                    Surrogate loss: 0.0005
                    Policy entropy: 11.7298
             Mean action noise std: 0.64
                       Mean reward: 11.01
               Mean episode length: 942.23
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1117
       Mean episode rew_ang_vel_xy: -0.1036
        Mean episode rew_collision: -0.0655
          Mean episode rew_dof_acc: -0.0889
   Mean episode rew_dof_pos_limits: -0.0001
        Mean episode rew_lin_vel_z: -0.0192
          Mean episode rew_torques: -0.0490
 Mean episode rew_tracking_ang_vel: 0.3040
 Mean episode rew_tracking_lin_vel: 0.6847
            Mean episode rew_total: 0.5507
        Mean episode terrain_level: 2.1822
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4991
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 2.20s
                        Total time: 216.50s
                               ETA: 21877.1s

################################################################################
                     [1m Learning iteration 98/10000 [0m                      

                       Computation: 44115 steps/s (collection: 1.863s, learning 0.366s)
               Value function loss: 0.0060
           Forward prediction loss: 0.0000
                          CMT loss: 0.0033
                    Surrogate loss: -0.0005
                    Policy entropy: 11.7107
             Mean action noise std: 0.64
                       Mean reward: 11.05
               Mean episode length: 965.52
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1134
       Mean episode rew_ang_vel_xy: -0.1043
        Mean episode rew_collision: -0.0696
          Mean episode rew_dof_acc: -0.0932
   Mean episode rew_dof_pos_limits: -0.0001
        Mean episode rew_lin_vel_z: -0.0195
          Mean episode rew_torques: -0.0496
 Mean episode rew_tracking_ang_vel: 0.3094
 Mean episode rew_tracking_lin_vel: 0.6551
            Mean episode rew_total: 0.5147
        Mean episode terrain_level: 2.1695
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4991
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 2.23s
                        Total time: 218.72s
                               ETA: 21876.9s

################################################################################
                     [1m Learning iteration 99/10000 [0m                      

                       Computation: 43019 steps/s (collection: 1.919s, learning 0.366s)
               Value function loss: 0.0062
           Forward prediction loss: 0.0000
                          CMT loss: 0.0034
                    Surrogate loss: -0.0005
                    Policy entropy: 11.6876
             Mean action noise std: 0.64
                       Mean reward: 10.15
               Mean episode length: 907.81
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1089
       Mean episode rew_ang_vel_xy: -0.1000
        Mean episode rew_collision: -0.0543
          Mean episode rew_dof_acc: -0.0893
   Mean episode rew_dof_pos_limits: -0.0001
        Mean episode rew_lin_vel_z: -0.0195
          Mean episode rew_torques: -0.0481
 Mean episode rew_tracking_ang_vel: 0.2952
 Mean episode rew_tracking_lin_vel: 0.6339
            Mean episode rew_total: 0.5090
        Mean episode terrain_level: 2.1575
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4991
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 2.29s
                        Total time: 221.01s
                               ETA: 21882.2s

################################################################################
                     [1m Learning iteration 100/10000 [0m                     

                       Computation: 42419 steps/s (collection: 1.952s, learning 0.365s)
               Value function loss: 0.0067
           Forward prediction loss: 0.0000
                          CMT loss: 0.0034
                    Surrogate loss: 0.0001
                    Policy entropy: 11.6669
             Mean action noise std: 0.64
                       Mean reward: 9.95
               Mean episode length: 911.55
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1048
       Mean episode rew_ang_vel_xy: -0.0955
        Mean episode rew_collision: -0.0482
          Mean episode rew_dof_acc: -0.0855
   Mean episode rew_dof_pos_limits: -0.0002
        Mean episode rew_lin_vel_z: -0.0192
          Mean episode rew_torques: -0.0464
 Mean episode rew_tracking_ang_vel: 0.2876
 Mean episode rew_tracking_lin_vel: 0.5998
            Mean episode rew_total: 0.4877
        Mean episode terrain_level: 2.1456
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4991
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 2.32s
                        Total time: 223.33s
                               ETA: 21890.5s

################################################################################
                     [1m Learning iteration 101/10000 [0m                     

                       Computation: 44617 steps/s (collection: 1.839s, learning 0.364s)
               Value function loss: 0.0055
           Forward prediction loss: 0.0000
                          CMT loss: 0.0032
                    Surrogate loss: 0.0002
                    Policy entropy: 11.6492
             Mean action noise std: 0.64
                       Mean reward: 10.71
               Mean episode length: 957.33
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1119
       Mean episode rew_ang_vel_xy: -0.1024
        Mean episode rew_collision: -0.0563
          Mean episode rew_dof_acc: -0.0927
   Mean episode rew_dof_pos_limits: -0.0001
        Mean episode rew_lin_vel_z: -0.0198
          Mean episode rew_torques: -0.0500
 Mean episode rew_tracking_ang_vel: 0.3101
 Mean episode rew_tracking_lin_vel: 0.6590
            Mean episode rew_total: 0.5359
        Mean episode terrain_level: 2.1336
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4991
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 2.20s
                        Total time: 225.53s
                               ETA: 21887.5s

################################################################################
                     [1m Learning iteration 102/10000 [0m                     

                       Computation: 44347 steps/s (collection: 1.848s, learning 0.369s)
               Value function loss: 0.0055
           Forward prediction loss: 0.0000
                          CMT loss: 0.0032
                    Surrogate loss: -0.0002
                    Policy entropy: 11.6205
             Mean action noise std: 0.64
                       Mean reward: 10.70
               Mean episode length: 941.93
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1101
       Mean episode rew_ang_vel_xy: -0.1005
        Mean episode rew_collision: -0.0552
          Mean episode rew_dof_acc: -0.0890
   Mean episode rew_dof_pos_limits: -0.0001
        Mean episode rew_lin_vel_z: -0.0192
          Mean episode rew_torques: -0.0490
 Mean episode rew_tracking_ang_vel: 0.3055
 Mean episode rew_tracking_lin_vel: 0.6924
            Mean episode rew_total: 0.5749
        Mean episode terrain_level: 2.1225
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4991
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 2.22s
                        Total time: 227.75s
                               ETA: 21885.8s

################################################################################
                     [1m Learning iteration 103/10000 [0m                     

                       Computation: 42833 steps/s (collection: 1.928s, learning 0.367s)
               Value function loss: 0.0055
           Forward prediction loss: 0.0000
                          CMT loss: 0.0033
                    Surrogate loss: -0.0010
                    Policy entropy: 11.5930
             Mean action noise std: 0.64
                       Mean reward: 10.64
               Mean episode length: 935.22
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1052
       Mean episode rew_ang_vel_xy: -0.0952
        Mean episode rew_collision: -0.0502
          Mean episode rew_dof_acc: -0.0860
   Mean episode rew_dof_pos_limits: -0.0001
        Mean episode rew_lin_vel_z: -0.0190
          Mean episode rew_torques: -0.0457
 Mean episode rew_tracking_ang_vel: 0.2969
 Mean episode rew_tracking_lin_vel: 0.6577
            Mean episode rew_total: 0.5532
        Mean episode terrain_level: 2.1125
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4991
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 2.30s
                        Total time: 230.04s
                               ETA: 21891.6s

################################################################################
                     [1m Learning iteration 104/10000 [0m                     

                       Computation: 36749 steps/s (collection: 2.309s, learning 0.366s)
               Value function loss: 0.0173
           Forward prediction loss: 0.0000
                          CMT loss: 0.0033
                    Surrogate loss: -0.0004
                    Policy entropy: 11.5729
             Mean action noise std: 0.64
                       Mean reward: 8.95
               Mean episode length: 860.56
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0925
       Mean episode rew_ang_vel_xy: -0.0856
        Mean episode rew_collision: -0.0625
          Mean episode rew_dof_acc: -0.0756
   Mean episode rew_dof_pos_limits: -0.0002
        Mean episode rew_lin_vel_z: -0.0177
          Mean episode rew_torques: -0.0418
 Mean episode rew_tracking_ang_vel: 0.2500
 Mean episode rew_tracking_lin_vel: 0.5232
            Mean episode rew_total: 0.3972
        Mean episode terrain_level: 2.1023
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4992
  Mean episode min_command_yaw_vel: -0.4997
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 2.67s
                        Total time: 232.72s
                               ETA: 21933.0s

################################################################################
                     [1m Learning iteration 105/10000 [0m                     

                       Computation: 43697 steps/s (collection: 1.886s, learning 0.364s)
               Value function loss: 0.0064
           Forward prediction loss: 0.0000
                          CMT loss: 0.0033
                    Surrogate loss: 0.0001
                    Policy entropy: 11.5650
             Mean action noise std: 0.64
                       Mean reward: 8.31
               Mean episode length: 853.25
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0945
       Mean episode rew_ang_vel_xy: -0.0847
        Mean episode rew_collision: -0.0630
          Mean episode rew_dof_acc: -0.0777
   Mean episode rew_dof_pos_limits: -0.0006
        Mean episode rew_lin_vel_z: -0.0182
          Mean episode rew_torques: -0.0423
 Mean episode rew_tracking_ang_vel: 0.2592
 Mean episode rew_tracking_lin_vel: 0.5204
            Mean episode rew_total: 0.3986
        Mean episode terrain_level: 2.0900
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.4993
    Mean episode max_command_y_vel: 0.4991
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 2.25s
                        Total time: 234.97s
                               ETA: 21933.9s

################################################################################
                     [1m Learning iteration 106/10000 [0m                     

                       Computation: 42623 steps/s (collection: 1.942s, learning 0.365s)
               Value function loss: 0.0061
           Forward prediction loss: 0.0000
                          CMT loss: 0.0034
                    Surrogate loss: 0.0003
                    Policy entropy: 11.5533
             Mean action noise std: 0.63
                       Mean reward: 9.25
               Mean episode length: 919.06
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1057
       Mean episode rew_ang_vel_xy: -0.0971
        Mean episode rew_collision: -0.0615
          Mean episode rew_dof_acc: -0.0894
   Mean episode rew_dof_pos_limits: -0.0002
        Mean episode rew_lin_vel_z: -0.0197
          Mean episode rew_torques: -0.0470
 Mean episode rew_tracking_ang_vel: 0.2886
 Mean episode rew_tracking_lin_vel: 0.6096
            Mean episode rew_total: 0.4776
        Mean episode terrain_level: 2.0782
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.4993
    Mean episode max_command_y_vel: 0.4991
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 2.31s
                        Total time: 237.27s
                               ETA: 21940.0s

################################################################################
                     [1m Learning iteration 107/10000 [0m                     

                       Computation: 41974 steps/s (collection: 1.978s, learning 0.364s)
               Value function loss: 0.0088
           Forward prediction loss: 0.0000
                          CMT loss: 0.0034
                    Surrogate loss: 0.0002
                    Policy entropy: 11.5306
             Mean action noise std: 0.63
                       Mean reward: 7.96
               Mean episode length: 891.61
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1002
       Mean episode rew_ang_vel_xy: -0.0909
        Mean episode rew_collision: -0.0754
          Mean episode rew_dof_acc: -0.0811
   Mean episode rew_dof_pos_limits: -0.0005
        Mean episode rew_lin_vel_z: -0.0188
          Mean episode rew_torques: -0.0447
 Mean episode rew_tracking_ang_vel: 0.2843
 Mean episode rew_tracking_lin_vel: 0.5469
            Mean episode rew_total: 0.4196
        Mean episode terrain_level: 2.0638
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.4992
    Mean episode max_command_y_vel: 0.4991
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 2.34s
                        Total time: 239.61s
                               ETA: 21949.2s

################################################################################
                     [1m Learning iteration 108/10000 [0m                     

                       Computation: 42635 steps/s (collection: 1.943s, learning 0.363s)
               Value function loss: 0.0078
           Forward prediction loss: 0.0000
                          CMT loss: 0.0034
                    Surrogate loss: -0.0009
                    Policy entropy: 11.5061
             Mean action noise std: 0.63
                       Mean reward: 8.57
               Mean episode length: 890.28
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1000
       Mean episode rew_ang_vel_xy: -0.0904
        Mean episode rew_collision: -0.0693
          Mean episode rew_dof_acc: -0.0828
   Mean episode rew_dof_pos_limits: -0.0003
        Mean episode rew_lin_vel_z: -0.0187
          Mean episode rew_torques: -0.0446
 Mean episode rew_tracking_ang_vel: 0.2740
 Mean episode rew_tracking_lin_vel: 0.5576
            Mean episode rew_total: 0.4254
        Mean episode terrain_level: 2.0489
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4990
    Mean episode max_command_y_vel: 0.4991
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4993
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 2.31s
                        Total time: 241.92s
                               ETA: 21954.9s

################################################################################
                     [1m Learning iteration 109/10000 [0m                     

                       Computation: 41082 steps/s (collection: 2.027s, learning 0.366s)
               Value function loss: 0.0075
           Forward prediction loss: 0.0000
                          CMT loss: 0.0034
                    Surrogate loss: -0.0018
                    Policy entropy: 11.4804
             Mean action noise std: 0.63
                       Mean reward: 9.04
               Mean episode length: 912.29
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1033
       Mean episode rew_ang_vel_xy: -0.0934
        Mean episode rew_collision: -0.0621
          Mean episode rew_dof_acc: -0.0864
   Mean episode rew_dof_pos_limits: -0.0005
        Mean episode rew_lin_vel_z: -0.0192
          Mean episode rew_torques: -0.0464
 Mean episode rew_tracking_ang_vel: 0.2881
 Mean episode rew_tracking_lin_vel: 0.5808
            Mean episode rew_total: 0.4576
        Mean episode terrain_level: 2.0356
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4990
    Mean episode max_command_y_vel: 0.4991
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4993
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 2.39s
                        Total time: 244.31s
                               ETA: 21968.2s

################################################################################
                     [1m Learning iteration 110/10000 [0m                     

                       Computation: 44467 steps/s (collection: 1.853s, learning 0.357s)
               Value function loss: 0.0074
           Forward prediction loss: 0.0000
                          CMT loss: 0.0033
                    Surrogate loss: -0.0007
                    Policy entropy: 11.4601
             Mean action noise std: 0.63
                       Mean reward: 8.68
               Mean episode length: 875.74
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0980
       Mean episode rew_ang_vel_xy: -0.0899
        Mean episode rew_collision: -0.0610
          Mean episode rew_dof_acc: -0.0812
   Mean episode rew_dof_pos_limits: -0.0005
        Mean episode rew_lin_vel_z: -0.0193
          Mean episode rew_torques: -0.0444
 Mean episode rew_tracking_ang_vel: 0.2764
 Mean episode rew_tracking_lin_vel: 0.5624
            Mean episode rew_total: 0.4445
        Mean episode terrain_level: 2.0194
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.4991
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4993
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 2.21s
                        Total time: 246.52s
                               ETA: 21965.1s

################################################################################
                     [1m Learning iteration 111/10000 [0m                     

                       Computation: 44320 steps/s (collection: 1.854s, learning 0.364s)
               Value function loss: 0.0070
           Forward prediction loss: 0.0000
                          CMT loss: 0.0032
                    Surrogate loss: -0.0002
                    Policy entropy: 11.4424
             Mean action noise std: 0.63
                       Mean reward: 8.01
               Mean episode length: 878.41
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0988
       Mean episode rew_ang_vel_xy: -0.0893
        Mean episode rew_collision: -0.0889
          Mean episode rew_dof_acc: -0.0826
   Mean episode rew_dof_pos_limits: -0.0005
        Mean episode rew_lin_vel_z: -0.0193
          Mean episode rew_torques: -0.0444
 Mean episode rew_tracking_ang_vel: 0.2843
 Mean episode rew_tracking_lin_vel: 0.5574
            Mean episode rew_total: 0.4178
        Mean episode terrain_level: 2.0041
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.4991
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4993
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 2.22s
                        Total time: 248.74s
                               ETA: 21962.6s

################################################################################
                     [1m Learning iteration 112/10000 [0m                     

                       Computation: 42042 steps/s (collection: 1.974s, learning 0.364s)
               Value function loss: 0.0072
           Forward prediction loss: 0.0000
                          CMT loss: 0.0031
                    Surrogate loss: -0.0010
                    Policy entropy: 11.4198
             Mean action noise std: 0.63
                       Mean reward: 8.62
               Mean episode length: 853.44
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0970
       Mean episode rew_ang_vel_xy: -0.0880
        Mean episode rew_collision: -0.1144
          Mean episode rew_dof_acc: -0.0854
   Mean episode rew_dof_pos_limits: -0.0004
        Mean episode rew_lin_vel_z: -0.0191
          Mean episode rew_torques: -0.0444
 Mean episode rew_tracking_ang_vel: 0.2743
 Mean episode rew_tracking_lin_vel: 0.5594
            Mean episode rew_total: 0.3850
        Mean episode terrain_level: 1.9902
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.4995
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4993
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 2.34s
                        Total time: 251.08s
                               ETA: 21970.6s

################################################################################
                     [1m Learning iteration 113/10000 [0m                     

                       Computation: 42792 steps/s (collection: 1.933s, learning 0.364s)
               Value function loss: 0.0065
           Forward prediction loss: 0.0000
                          CMT loss: 0.0031
                    Surrogate loss: -0.0020
                    Policy entropy: 11.3975
             Mean action noise std: 0.63
                       Mean reward: 9.36
               Mean episode length: 905.41
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.1014
       Mean episode rew_ang_vel_xy: -0.0905
        Mean episode rew_collision: -0.0644
          Mean episode rew_dof_acc: -0.0835
   Mean episode rew_dof_pos_limits: -0.0006
        Mean episode rew_lin_vel_z: -0.0190
          Mean episode rew_torques: -0.0453
 Mean episode rew_tracking_ang_vel: 0.2903
 Mean episode rew_tracking_lin_vel: 0.6036
            Mean episode rew_total: 0.4892
        Mean episode terrain_level: 1.9773
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4993
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 2.30s
                        Total time: 253.38s
                               ETA: 21975.0s

################################################################################
                     [1m Learning iteration 114/10000 [0m                     

                       Computation: 44237 steps/s (collection: 1.860s, learning 0.363s)
               Value function loss: 0.0061
           Forward prediction loss: 0.0000
                          CMT loss: 0.0030
                    Surrogate loss: 0.0007
                    Policy entropy: 11.3719
             Mean action noise std: 0.63
                       Mean reward: 7.79
               Mean episode length: 832.89
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0916
       Mean episode rew_ang_vel_xy: -0.0831
        Mean episode rew_collision: -0.0706
          Mean episode rew_dof_acc: -0.0792
   Mean episode rew_dof_pos_limits: -0.0005
        Mean episode rew_lin_vel_z: -0.0184
          Mean episode rew_torques: -0.0420
 Mean episode rew_tracking_ang_vel: 0.2571
 Mean episode rew_tracking_lin_vel: 0.5293
            Mean episode rew_total: 0.4010
        Mean episode terrain_level: 1.9684
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4993
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 2.22s
                        Total time: 255.60s
                               ETA: 21972.7s

################################################################################
                     [1m Learning iteration 115/10000 [0m                     

                       Computation: 43353 steps/s (collection: 1.901s, learning 0.366s)
               Value function loss: 0.0071
           Forward prediction loss: 0.0000
                          CMT loss: 0.0030
                    Surrogate loss: -0.0012
                    Policy entropy: 11.3568
             Mean action noise std: 0.62
                       Mean reward: 8.24
               Mean episode length: 830.78
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0919
       Mean episode rew_ang_vel_xy: -0.0829
        Mean episode rew_collision: -0.0611
          Mean episode rew_dof_acc: -0.0796
   Mean episode rew_dof_pos_limits: -0.0007
        Mean episode rew_lin_vel_z: -0.0183
          Mean episode rew_torques: -0.0421
 Mean episode rew_tracking_ang_vel: 0.2606
 Mean episode rew_tracking_lin_vel: 0.5444
            Mean episode rew_total: 0.4284
        Mean episode terrain_level: 1.9592
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4993
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 2.27s
                        Total time: 257.87s
                               ETA: 21974.3s

################################################################################
                     [1m Learning iteration 116/10000 [0m                     

                       Computation: 42794 steps/s (collection: 1.930s, learning 0.367s)
               Value function loss: 0.0065
           Forward prediction loss: 0.0000
                          CMT loss: 0.0030
                    Surrogate loss: 0.0000
                    Policy entropy: 11.3351
             Mean action noise std: 0.62
                       Mean reward: 8.81
               Mean episode length: 860.17
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0996
       Mean episode rew_ang_vel_xy: -0.0888
        Mean episode rew_collision: -0.0559
          Mean episode rew_dof_acc: -0.0888
   Mean episode rew_dof_pos_limits: -0.0004
        Mean episode rew_lin_vel_z: -0.0192
          Mean episode rew_torques: -0.0458
 Mean episode rew_tracking_ang_vel: 0.2832
 Mean episode rew_tracking_lin_vel: 0.5568
            Mean episode rew_total: 0.4413
        Mean episode terrain_level: 1.9491
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4992
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4993
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 2.30s
                        Total time: 260.16s
                               ETA: 21978.3s

################################################################################
                     [1m Learning iteration 117/10000 [0m                     

                       Computation: 43074 steps/s (collection: 1.916s, learning 0.366s)
               Value function loss: 0.0069
           Forward prediction loss: 0.0000
                          CMT loss: 0.0029
                    Surrogate loss: -0.0014
                    Policy entropy: 11.3024
             Mean action noise std: 0.62
                       Mean reward: 8.62
               Mean episode length: 860.59
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0939
       Mean episode rew_ang_vel_xy: -0.0841
        Mean episode rew_collision: -0.0724
          Mean episode rew_dof_acc: -0.0808
   Mean episode rew_dof_pos_limits: -0.0007
        Mean episode rew_lin_vel_z: -0.0183
          Mean episode rew_torques: -0.0436
 Mean episode rew_tracking_ang_vel: 0.2724
 Mean episode rew_tracking_lin_vel: 0.5512
            Mean episode rew_total: 0.4299
        Mean episode terrain_level: 1.9414
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4990
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4993
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 2.28s
                        Total time: 262.45s
                               ETA: 21981.0s

################################################################################
                     [1m Learning iteration 118/10000 [0m                     

                       Computation: 44426 steps/s (collection: 1.848s, learning 0.365s)
               Value function loss: 0.0068
           Forward prediction loss: 0.0000
                          CMT loss: 0.0028
                    Surrogate loss: 0.0004
                    Policy entropy: 11.2667
             Mean action noise std: 0.62
                       Mean reward: 8.49
               Mean episode length: 811.88
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0859
       Mean episode rew_ang_vel_xy: -0.0764
        Mean episode rew_collision: -0.0485
          Mean episode rew_dof_acc: -0.0710
   Mean episode rew_dof_pos_limits: -0.0004
        Mean episode rew_lin_vel_z: -0.0174
          Mean episode rew_torques: -0.0393
 Mean episode rew_tracking_ang_vel: 0.2554
 Mean episode rew_tracking_lin_vel: 0.4909
            Mean episode rew_total: 0.4075
        Mean episode terrain_level: 1.9323
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4990
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4993
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 2.21s
                        Total time: 264.66s
                               ETA: 21977.8s

################################################################################
                     [1m Learning iteration 119/10000 [0m                     

                       Computation: 43323 steps/s (collection: 1.902s, learning 0.367s)
               Value function loss: 0.0075
           Forward prediction loss: 0.0000
                          CMT loss: 0.0029
                    Surrogate loss: 0.0005
                    Policy entropy: 11.2493
             Mean action noise std: 0.62
                       Mean reward: 8.25
               Mean episode length: 806.00
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0893
       Mean episode rew_ang_vel_xy: -0.0786
        Mean episode rew_collision: -0.0628
          Mean episode rew_dof_acc: -0.0712
   Mean episode rew_dof_pos_limits: -0.0008
        Mean episode rew_lin_vel_z: -0.0174
          Mean episode rew_torques: -0.0410
 Mean episode rew_tracking_ang_vel: 0.2654
 Mean episode rew_tracking_lin_vel: 0.5605
            Mean episode rew_total: 0.4649
        Mean episode terrain_level: 1.9244
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4990
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4993
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 2.27s
                        Total time: 266.93s
                               ETA: 21979.3s

################################################################################
                     [1m Learning iteration 120/10000 [0m                     

                       Computation: 44362 steps/s (collection: 1.846s, learning 0.370s)
               Value function loss: 0.0071
           Forward prediction loss: 0.0000
                          CMT loss: 0.0027
                    Surrogate loss: -0.0004
                    Policy entropy: 11.2253
             Mean action noise std: 0.62
                       Mean reward: 8.51
               Mean episode length: 814.99
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0764
       Mean episode rew_ang_vel_xy: -0.0688
        Mean episode rew_collision: -0.0381
          Mean episode rew_dof_acc: -0.0635
   Mean episode rew_dof_pos_limits: -0.0006
        Mean episode rew_lin_vel_z: -0.0165
          Mean episode rew_torques: -0.0360
 Mean episode rew_tracking_ang_vel: 0.2212
 Mean episode rew_tracking_lin_vel: 0.4604
            Mean episode rew_total: 0.3815
        Mean episode terrain_level: 1.9186
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4990
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4993
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 2.22s
                        Total time: 269.14s
                               ETA: 21976.4s

################################################################################
                     [1m Learning iteration 121/10000 [0m                     

                       Computation: 44790 steps/s (collection: 1.828s, learning 0.366s)
               Value function loss: 0.0082
           Forward prediction loss: 0.0000
                          CMT loss: 0.0027
                    Surrogate loss: -0.0014
                    Policy entropy: 11.2026
             Mean action noise std: 0.62
                       Mean reward: 7.43
               Mean episode length: 781.92
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0841
       Mean episode rew_ang_vel_xy: -0.0759
        Mean episode rew_collision: -0.0721
          Mean episode rew_dof_acc: -0.0741
   Mean episode rew_dof_pos_limits: -0.0006
        Mean episode rew_lin_vel_z: -0.0182
          Mean episode rew_torques: -0.0389
 Mean episode rew_tracking_ang_vel: 0.2461
 Mean episode rew_tracking_lin_vel: 0.4817
            Mean episode rew_total: 0.3639
        Mean episode terrain_level: 1.9101
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.4990
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4993
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 2.19s
                        Total time: 271.34s
                               ETA: 21971.8s

################################################################################
                     [1m Learning iteration 122/10000 [0m                     

                       Computation: 44162 steps/s (collection: 1.863s, learning 0.363s)
               Value function loss: 0.0077
           Forward prediction loss: 0.0000
                          CMT loss: 0.0027
                    Surrogate loss: 0.0000
                    Policy entropy: 11.1807
             Mean action noise std: 0.62
                       Mean reward: 8.11
               Mean episode length: 800.56
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0849
       Mean episode rew_ang_vel_xy: -0.0750
        Mean episode rew_collision: -0.0581
          Mean episode rew_dof_acc: -0.0737
   Mean episode rew_dof_pos_limits: -0.0007
        Mean episode rew_lin_vel_z: -0.0172
          Mean episode rew_torques: -0.0387
 Mean episode rew_tracking_ang_vel: 0.2494
 Mean episode rew_tracking_lin_vel: 0.4892
            Mean episode rew_total: 0.3903
        Mean episode terrain_level: 1.9032
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.4990
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4993
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 2.23s
                        Total time: 273.56s
                               ETA: 21969.7s

################################################################################
                     [1m Learning iteration 123/10000 [0m                     

                       Computation: 43951 steps/s (collection: 1.872s, learning 0.365s)
               Value function loss: 0.0082
           Forward prediction loss: 0.0000
                          CMT loss: 0.0028
                    Surrogate loss: 0.0009
                    Policy entropy: 11.1572
             Mean action noise std: 0.61
                       Mean reward: 6.91
               Mean episode length: 743.39
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0813
       Mean episode rew_ang_vel_xy: -0.0715
        Mean episode rew_collision: -0.0648
          Mean episode rew_dof_acc: -0.0680
   Mean episode rew_dof_pos_limits: -0.0007
        Mean episode rew_lin_vel_z: -0.0166
          Mean episode rew_torques: -0.0375
 Mean episode rew_tracking_ang_vel: 0.2422
 Mean episode rew_tracking_lin_vel: 0.4677
            Mean episode rew_total: 0.3696
        Mean episode terrain_level: 1.8954
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.4990
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4994
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 2.24s
                        Total time: 275.80s
                               ETA: 21968.5s

################################################################################
                     [1m Learning iteration 124/10000 [0m                     

                       Computation: 44619 steps/s (collection: 1.834s, learning 0.369s)
               Value function loss: 0.0076
           Forward prediction loss: 0.0000
                          CMT loss: 0.0029
                    Surrogate loss: -0.0008
                    Policy entropy: 11.1371
             Mean action noise std: 0.61
                       Mean reward: 7.12
               Mean episode length: 758.25
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0715
       Mean episode rew_ang_vel_xy: -0.0638
        Mean episode rew_collision: -0.0647
          Mean episode rew_dof_acc: -0.0601
   Mean episode rew_dof_pos_limits: -0.0011
        Mean episode rew_lin_vel_z: -0.0154
          Mean episode rew_torques: -0.0340
 Mean episode rew_tracking_ang_vel: 0.2097
 Mean episode rew_tracking_lin_vel: 0.4101
            Mean episode rew_total: 0.3092
        Mean episode terrain_level: 1.8866
    Mean episode min_command_x_vel: -0.5997
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.4990
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.1733
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 2.20s
                        Total time: 278.00s
                               ETA: 21964.6s

################################################################################
                     [1m Learning iteration 125/10000 [0m                     

                       Computation: 30513 steps/s (collection: 2.853s, learning 0.368s)
               Value function loss: 0.0167
           Forward prediction loss: 0.0000
                          CMT loss: 0.0055
                    Surrogate loss: 0.0067
                    Policy entropy: 11.1259
             Mean action noise std: 0.61
                       Mean reward: 8.64
               Mean episode length: 849.51
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0782
       Mean episode rew_ang_vel_xy: -0.0692
        Mean episode rew_collision: -0.0385
          Mean episode rew_dof_acc: -0.0688
   Mean episode rew_dof_pos_limits: -0.0008
        Mean episode rew_lin_vel_z: -0.0173
          Mean episode rew_torques: -0.0366
 Mean episode rew_tracking_ang_vel: 0.2285
 Mean episode rew_tracking_lin_vel: 0.4399
            Mean episode rew_total: 0.3589
        Mean episode terrain_level: 1.7732
    Mean episode min_command_x_vel: -0.9640
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4989
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4996
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.3820
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 3.22s
                        Total time: 281.23s
                               ETA: 22040.6s

################################################################################
                     [1m Learning iteration 126/10000 [0m                     

                       Computation: 41891 steps/s (collection: 1.983s, learning 0.364s)
               Value function loss: 0.0110
           Forward prediction loss: 0.0000
                          CMT loss: 0.0034
                    Surrogate loss: -0.0005
                    Policy entropy: 11.1248
             Mean action noise std: 0.61
                       Mean reward: 7.38
               Mean episode length: 752.08
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0785
       Mean episode rew_ang_vel_xy: -0.0696
        Mean episode rew_collision: -0.0417
          Mean episode rew_dof_acc: -0.0682
   Mean episode rew_dof_pos_limits: -0.0009
        Mean episode rew_lin_vel_z: -0.0167
          Mean episode rew_torques: -0.0368
 Mean episode rew_tracking_ang_vel: 0.2290
 Mean episode rew_tracking_lin_vel: 0.4426
            Mean episode rew_total: 0.3591
        Mean episode terrain_level: 1.7444
    Mean episode min_command_x_vel: -0.9972
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4989
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4996
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 2.35s
                        Total time: 283.57s
                               ETA: 22047.3s

################################################################################
                     [1m Learning iteration 127/10000 [0m                     

                       Computation: 41512 steps/s (collection: 2.005s, learning 0.363s)
               Value function loss: 0.0093
           Forward prediction loss: 0.0000
                          CMT loss: 0.0033
                    Surrogate loss: 0.0001
                    Policy entropy: 11.1161
             Mean action noise std: 0.61
                       Mean reward: 7.85
               Mean episode length: 769.99
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0743
       Mean episode rew_ang_vel_xy: -0.0688
        Mean episode rew_collision: -0.0789
          Mean episode rew_dof_acc: -0.0661
   Mean episode rew_dof_pos_limits: -0.0006
        Mean episode rew_lin_vel_z: -0.0172
          Mean episode rew_torques: -0.0346
 Mean episode rew_tracking_ang_vel: 0.2187
 Mean episode rew_tracking_lin_vel: 0.4251
            Mean episode rew_total: 0.3033
        Mean episode terrain_level: 1.7312
    Mean episode min_command_x_vel: -0.9972
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4989
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4997
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 2.37s
                        Total time: 285.94s
                               ETA: 22055.4s

################################################################################
                     [1m Learning iteration 128/10000 [0m                     

                       Computation: 39127 steps/s (collection: 2.148s, learning 0.364s)
               Value function loss: 0.0110
           Forward prediction loss: 0.0000
                          CMT loss: 0.0035
                    Surrogate loss: 0.0001
                    Policy entropy: 11.1100
             Mean action noise std: 0.61
                       Mean reward: 7.38
               Mean episode length: 761.83
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0803
       Mean episode rew_ang_vel_xy: -0.0714
        Mean episode rew_collision: -0.0592
          Mean episode rew_dof_acc: -0.0692
   Mean episode rew_dof_pos_limits: -0.0009
        Mean episode rew_lin_vel_z: -0.0172
          Mean episode rew_torques: -0.0370
 Mean episode rew_tracking_ang_vel: 0.2353
 Mean episode rew_tracking_lin_vel: 0.4667
            Mean episode rew_total: 0.3667
        Mean episode terrain_level: 1.7109
    Mean episode min_command_x_vel: -0.9972
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4989
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 2.51s
                        Total time: 288.45s
                               ETA: 22074.5s

################################################################################
                     [1m Learning iteration 129/10000 [0m                     

                       Computation: 38488 steps/s (collection: 2.192s, learning 0.362s)
               Value function loss: 0.0085
           Forward prediction loss: 0.0000
                          CMT loss: 0.0035
                    Surrogate loss: -0.0008
                    Policy entropy: 11.1063
             Mean action noise std: 0.61
                       Mean reward: 7.47
               Mean episode length: 846.31
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0889
       Mean episode rew_ang_vel_xy: -0.0781
        Mean episode rew_collision: -0.0696
          Mean episode rew_dof_acc: -0.0763
   Mean episode rew_dof_pos_limits: -0.0008
        Mean episode rew_lin_vel_z: -0.0179
          Mean episode rew_torques: -0.0415
 Mean episode rew_tracking_ang_vel: 0.2650
 Mean episode rew_tracking_lin_vel: 0.5003
            Mean episode rew_total: 0.3922
        Mean episode terrain_level: 1.6896
    Mean episode min_command_x_vel: -0.9972
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4989
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 2.55s
                        Total time: 291.01s
                               ETA: 22096.4s

################################################################################
                     [1m Learning iteration 130/10000 [0m                     

                       Computation: 39923 steps/s (collection: 2.099s, learning 0.363s)
               Value function loss: 0.0100
           Forward prediction loss: 0.0000
                          CMT loss: 0.0036
                    Surrogate loss: -0.0011
                    Policy entropy: 11.0953
             Mean action noise std: 0.61
                       Mean reward: 7.69
               Mean episode length: 793.72
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0798
       Mean episode rew_ang_vel_xy: -0.0707
        Mean episode rew_collision: -0.0449
          Mean episode rew_dof_acc: -0.0709
   Mean episode rew_dof_pos_limits: -0.0008
        Mean episode rew_lin_vel_z: -0.0171
          Mean episode rew_torques: -0.0372
 Mean episode rew_tracking_ang_vel: 0.2401
 Mean episode rew_tracking_lin_vel: 0.4555
            Mean episode rew_total: 0.3742
        Mean episode terrain_level: 1.6707
    Mean episode min_command_x_vel: -0.9972
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4989
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 2.46s
                        Total time: 293.47s
                               ETA: 22111.1s

################################################################################
                     [1m Learning iteration 131/10000 [0m                     

                       Computation: 40152 steps/s (collection: 2.085s, learning 0.363s)
               Value function loss: 0.0099
           Forward prediction loss: 0.0000
                          CMT loss: 0.0037
                    Surrogate loss: -0.0011
                    Policy entropy: 11.0900
             Mean action noise std: 0.61
                       Mean reward: 8.80
               Mean episode length: 812.31
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0856
       Mean episode rew_ang_vel_xy: -0.0758
        Mean episode rew_collision: -0.0523
          Mean episode rew_dof_acc: -0.0747
   Mean episode rew_dof_pos_limits: -0.0009
        Mean episode rew_lin_vel_z: -0.0180
          Mean episode rew_torques: -0.0402
 Mean episode rew_tracking_ang_vel: 0.2582
 Mean episode rew_tracking_lin_vel: 0.5181
            Mean episode rew_total: 0.4287
        Mean episode terrain_level: 1.6542
    Mean episode min_command_x_vel: -0.9972
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4994
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 2.45s
                        Total time: 295.92s
                               ETA: 22124.4s

################################################################################
                     [1m Learning iteration 132/10000 [0m                     

                       Computation: 40653 steps/s (collection: 2.052s, learning 0.366s)
               Value function loss: 0.0089
           Forward prediction loss: 0.0000
                          CMT loss: 0.0037
                    Surrogate loss: -0.0001
                    Policy entropy: 11.0801
             Mean action noise std: 0.61
                       Mean reward: 7.36
               Mean episode length: 754.45
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0762
       Mean episode rew_ang_vel_xy: -0.0670
        Mean episode rew_collision: -0.0441
          Mean episode rew_dof_acc: -0.0673
   Mean episode rew_dof_pos_limits: -0.0009
        Mean episode rew_lin_vel_z: -0.0173
          Mean episode rew_torques: -0.0355
 Mean episode rew_tracking_ang_vel: 0.2302
 Mean episode rew_tracking_lin_vel: 0.4387
            Mean episode rew_total: 0.3606
        Mean episode terrain_level: 1.6417
    Mean episode min_command_x_vel: -0.9972
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 2.42s
                        Total time: 298.34s
                               ETA: 22135.2s

################################################################################
                     [1m Learning iteration 133/10000 [0m                     

                       Computation: 41343 steps/s (collection: 2.012s, learning 0.366s)
               Value function loss: 0.0083
           Forward prediction loss: 0.0000
                          CMT loss: 0.0037
                    Surrogate loss: -0.0001
                    Policy entropy: 11.0735
             Mean action noise std: 0.61
                       Mean reward: 7.25
               Mean episode length: 800.53
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0868
       Mean episode rew_ang_vel_xy: -0.0764
        Mean episode rew_collision: -0.0585
          Mean episode rew_dof_acc: -0.0787
   Mean episode rew_dof_pos_limits: -0.0009
        Mean episode rew_lin_vel_z: -0.0181
          Mean episode rew_torques: -0.0406
 Mean episode rew_tracking_ang_vel: 0.2582
 Mean episode rew_tracking_lin_vel: 0.5022
            Mean episode rew_total: 0.4006
        Mean episode terrain_level: 1.6287
    Mean episode min_command_x_vel: -0.9973
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 2.38s
                        Total time: 300.71s
                               ETA: 22142.9s

################################################################################
                     [1m Learning iteration 134/10000 [0m                     

                       Computation: 40734 steps/s (collection: 2.049s, learning 0.365s)
               Value function loss: 0.0086
           Forward prediction loss: 0.0000
                          CMT loss: 0.0038
                    Surrogate loss: -0.0000
                    Policy entropy: 11.0669
             Mean action noise std: 0.61
                       Mean reward: 7.85
               Mean episode length: 830.46
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0826
       Mean episode rew_ang_vel_xy: -0.0741
        Mean episode rew_collision: -0.0640
          Mean episode rew_dof_acc: -0.0754
   Mean episode rew_dof_pos_limits: -0.0008
        Mean episode rew_lin_vel_z: -0.0179
          Mean episode rew_torques: -0.0388
 Mean episode rew_tracking_ang_vel: 0.2465
 Mean episode rew_tracking_lin_vel: 0.4882
            Mean episode rew_total: 0.3811
        Mean episode terrain_level: 1.6137
    Mean episode min_command_x_vel: -0.9973
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 2.41s
                        Total time: 303.13s
                               ETA: 22153.0s

################################################################################
                     [1m Learning iteration 135/10000 [0m                     

                       Computation: 41545 steps/s (collection: 2.001s, learning 0.365s)
               Value function loss: 0.0072
           Forward prediction loss: 0.0000
                          CMT loss: 0.0036
                    Surrogate loss: 0.0001
                    Policy entropy: 11.0564
             Mean action noise std: 0.61
                       Mean reward: 6.24
               Mean episode length: 705.07
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0741
       Mean episode rew_ang_vel_xy: -0.0655
        Mean episode rew_collision: -0.0427
          Mean episode rew_dof_acc: -0.0682
   Mean episode rew_dof_pos_limits: -0.0013
        Mean episode rew_lin_vel_z: -0.0172
          Mean episode rew_torques: -0.0346
 Mean episode rew_tracking_ang_vel: 0.2251
 Mean episode rew_tracking_lin_vel: 0.3754
            Mean episode rew_total: 0.2969
        Mean episode terrain_level: 1.6005
    Mean episode min_command_x_vel: -0.9973
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 2.37s
                        Total time: 305.49s
                               ETA: 22159.5s

################################################################################
                     [1m Learning iteration 136/10000 [0m                     

                       Computation: 42452 steps/s (collection: 1.948s, learning 0.368s)
               Value function loss: 0.0072
           Forward prediction loss: 0.0000
                          CMT loss: 0.0037
                    Surrogate loss: 0.0003
                    Policy entropy: 11.0446
             Mean action noise std: 0.61
                       Mean reward: 7.35
               Mean episode length: 768.60
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0830
       Mean episode rew_ang_vel_xy: -0.0712
        Mean episode rew_collision: -0.0244
          Mean episode rew_dof_acc: -0.0701
   Mean episode rew_dof_pos_limits: -0.0011
        Mean episode rew_lin_vel_z: -0.0178
          Mean episode rew_torques: -0.0392
 Mean episode rew_tracking_ang_vel: 0.2461
 Mean episode rew_tracking_lin_vel: 0.4685
            Mean episode rew_total: 0.4079
        Mean episode terrain_level: 1.5896
    Mean episode min_command_x_vel: -0.9973
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 2.32s
                        Total time: 307.81s
                               ETA: 22162.3s

################################################################################
                     [1m Learning iteration 137/10000 [0m                     

                       Computation: 42267 steps/s (collection: 1.960s, learning 0.366s)
               Value function loss: 0.0079
           Forward prediction loss: 0.0000
                          CMT loss: 0.0038
                    Surrogate loss: 0.0003
                    Policy entropy: 11.0309
             Mean action noise std: 0.61
                       Mean reward: 6.32
               Mean episode length: 679.85
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0700
       Mean episode rew_ang_vel_xy: -0.0624
        Mean episode rew_collision: -0.0394
          Mean episode rew_dof_acc: -0.0653
   Mean episode rew_dof_pos_limits: -0.0011
        Mean episode rew_lin_vel_z: -0.0171
          Mean episode rew_torques: -0.0331
 Mean episode rew_tracking_ang_vel: 0.2129
 Mean episode rew_tracking_lin_vel: 0.4039
            Mean episode rew_total: 0.3285
        Mean episode terrain_level: 1.5800
    Mean episode min_command_x_vel: -0.9973
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 2.33s
                        Total time: 310.13s
                               ETA: 22165.7s

################################################################################
                     [1m Learning iteration 138/10000 [0m                     

                       Computation: 40371 steps/s (collection: 2.064s, learning 0.371s)
               Value function loss: 0.0083
           Forward prediction loss: 0.0000
                          CMT loss: 0.0040
                    Surrogate loss: 0.0002
                    Policy entropy: 11.0228
             Mean action noise std: 0.61
                       Mean reward: 7.78
               Mean episode length: 770.85
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0792
       Mean episode rew_ang_vel_xy: -0.0688
        Mean episode rew_collision: -0.0471
          Mean episode rew_dof_acc: -0.0689
   Mean episode rew_dof_pos_limits: -0.0011
        Mean episode rew_lin_vel_z: -0.0168
          Mean episode rew_torques: -0.0375
 Mean episode rew_tracking_ang_vel: 0.2404
 Mean episode rew_tracking_lin_vel: 0.4497
            Mean episode rew_total: 0.3708
        Mean episode terrain_level: 1.5663
    Mean episode min_command_x_vel: -0.9973
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 2.43s
                        Total time: 312.57s
                               ETA: 22176.7s

################################################################################
                     [1m Learning iteration 139/10000 [0m                     

                       Computation: 42073 steps/s (collection: 1.973s, learning 0.364s)
               Value function loss: 0.0076
           Forward prediction loss: 0.0000
                          CMT loss: 0.0040
                    Surrogate loss: -0.0002
                    Policy entropy: 11.0095
             Mean action noise std: 0.61
                       Mean reward: 6.61
               Mean episode length: 707.66
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0720
       Mean episode rew_ang_vel_xy: -0.0628
        Mean episode rew_collision: -0.0372
          Mean episode rew_dof_acc: -0.0657
   Mean episode rew_dof_pos_limits: -0.0011
        Mean episode rew_lin_vel_z: -0.0163
          Mean episode rew_torques: -0.0341
 Mean episode rew_tracking_ang_vel: 0.2209
 Mean episode rew_tracking_lin_vel: 0.3943
            Mean episode rew_total: 0.3260
        Mean episode terrain_level: 1.5533
    Mean episode min_command_x_vel: -0.9973
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 2.34s
                        Total time: 314.91s
                               ETA: 22180.7s

################################################################################
                     [1m Learning iteration 140/10000 [0m                     

                       Computation: 42181 steps/s (collection: 1.966s, learning 0.364s)
               Value function loss: 0.0082
           Forward prediction loss: 0.0000
                          CMT loss: 0.0039
                    Surrogate loss: -0.0001
                    Policy entropy: 10.9991
             Mean action noise std: 0.61
                       Mean reward: 6.96
               Mean episode length: 727.58
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0754
       Mean episode rew_ang_vel_xy: -0.0668
        Mean episode rew_collision: -0.0450
          Mean episode rew_dof_acc: -0.0685
   Mean episode rew_dof_pos_limits: -0.0011
        Mean episode rew_lin_vel_z: -0.0169
          Mean episode rew_torques: -0.0356
 Mean episode rew_tracking_ang_vel: 0.2358
 Mean episode rew_tracking_lin_vel: 0.4305
            Mean episode rew_total: 0.3571
        Mean episode terrain_level: 1.5404
    Mean episode min_command_x_vel: -0.9973
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 2.33s
                        Total time: 317.24s
                               ETA: 22184.1s

################################################################################
                     [1m Learning iteration 141/10000 [0m                     

                       Computation: 40531 steps/s (collection: 2.059s, learning 0.367s)
               Value function loss: 0.0081
           Forward prediction loss: 0.0000
                          CMT loss: 0.0041
                    Surrogate loss: 0.0004
                    Policy entropy: 10.9867
             Mean action noise std: 0.61
                       Mean reward: 6.93
               Mean episode length: 786.95
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0835
       Mean episode rew_ang_vel_xy: -0.0735
        Mean episode rew_collision: -0.0701
          Mean episode rew_dof_acc: -0.0788
   Mean episode rew_dof_pos_limits: -0.0014
        Mean episode rew_lin_vel_z: -0.0186
          Mean episode rew_torques: -0.0398
 Mean episode rew_tracking_ang_vel: 0.2517
 Mean episode rew_tracking_lin_vel: 0.4702
            Mean episode rew_total: 0.3561
        Mean episode terrain_level: 1.5251
    Mean episode min_command_x_vel: -0.9973
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 2.43s
                        Total time: 319.66s
                               ETA: 22194.0s

################################################################################
                     [1m Learning iteration 142/10000 [0m                     

                       Computation: 40854 steps/s (collection: 2.041s, learning 0.365s)
               Value function loss: 0.0079
           Forward prediction loss: 0.0000
                          CMT loss: 0.0042
                    Surrogate loss: 0.0009
                    Policy entropy: 10.9810
             Mean action noise std: 0.61
                       Mean reward: 7.67
               Mean episode length: 801.72
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0822
       Mean episode rew_ang_vel_xy: -0.0714
        Mean episode rew_collision: -0.0654
          Mean episode rew_dof_acc: -0.0748
   Mean episode rew_dof_pos_limits: -0.0014
        Mean episode rew_lin_vel_z: -0.0174
          Mean episode rew_torques: -0.0395
 Mean episode rew_tracking_ang_vel: 0.2525
 Mean episode rew_tracking_lin_vel: 0.4751
            Mean episode rew_total: 0.3755
        Mean episode terrain_level: 1.5114
    Mean episode min_command_x_vel: -0.9973
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 2.41s
                        Total time: 322.07s
                               ETA: 22202.5s

################################################################################
                     [1m Learning iteration 143/10000 [0m                     

                       Computation: 41957 steps/s (collection: 1.973s, learning 0.370s)
               Value function loss: 0.0077
           Forward prediction loss: 0.0000
                          CMT loss: 0.0041
                    Surrogate loss: -0.0005
                    Policy entropy: 10.9731
             Mean action noise std: 0.61
                       Mean reward: 7.00
               Mean episode length: 747.30
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0798
       Mean episode rew_ang_vel_xy: -0.0692
        Mean episode rew_collision: -0.0498
          Mean episode rew_dof_acc: -0.0727
   Mean episode rew_dof_pos_limits: -0.0011
        Mean episode rew_lin_vel_z: -0.0176
          Mean episode rew_torques: -0.0377
 Mean episode rew_tracking_ang_vel: 0.2462
 Mean episode rew_tracking_lin_vel: 0.4691
            Mean episode rew_total: 0.3874
        Mean episode terrain_level: 1.4998
    Mean episode min_command_x_vel: -0.9978
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 2.34s
                        Total time: 324.41s
                               ETA: 22206.4s

################################################################################
                     [1m Learning iteration 144/10000 [0m                     

                       Computation: 40713 steps/s (collection: 2.051s, learning 0.364s)
               Value function loss: 0.0068
           Forward prediction loss: 0.0000
                          CMT loss: 0.0042
                    Surrogate loss: 0.0004
                    Policy entropy: 10.9602
             Mean action noise std: 0.60
                       Mean reward: 7.61
               Mean episode length: 770.40
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0752
       Mean episode rew_ang_vel_xy: -0.0664
        Mean episode rew_collision: -0.0281
          Mean episode rew_dof_acc: -0.0706
   Mean episode rew_dof_pos_limits: -0.0013
        Mean episode rew_lin_vel_z: -0.0169
          Mean episode rew_torques: -0.0358
 Mean episode rew_tracking_ang_vel: 0.2282
 Mean episode rew_tracking_lin_vel: 0.4277
            Mean episode rew_total: 0.3617
        Mean episode terrain_level: 1.4890
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4996
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 2.41s
                        Total time: 326.83s
                               ETA: 22215.1s

################################################################################
                     [1m Learning iteration 145/10000 [0m                     

                       Computation: 41189 steps/s (collection: 2.018s, learning 0.369s)
               Value function loss: 0.0071
           Forward prediction loss: 0.0000
                          CMT loss: 0.0042
                    Surrogate loss: 0.0005
                    Policy entropy: 10.9443
             Mean action noise std: 0.60
                       Mean reward: 8.15
               Mean episode length: 770.55
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0791
       Mean episode rew_ang_vel_xy: -0.0692
        Mean episode rew_collision: -0.0506
          Mean episode rew_dof_acc: -0.0711
   Mean episode rew_dof_pos_limits: -0.0012
        Mean episode rew_lin_vel_z: -0.0170
          Mean episode rew_torques: -0.0373
 Mean episode rew_tracking_ang_vel: 0.2496
 Mean episode rew_tracking_lin_vel: 0.4813
            Mean episode rew_total: 0.4054
        Mean episode terrain_level: 1.4801
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4992
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 2.39s
                        Total time: 329.21s
                               ETA: 22221.9s

################################################################################
                     [1m Learning iteration 146/10000 [0m                     

                       Computation: 35946 steps/s (collection: 2.368s, learning 0.367s)
               Value function loss: 0.0120
           Forward prediction loss: 0.0000
                          CMT loss: 0.0042
                    Surrogate loss: 0.0007
                    Policy entropy: 10.9310
             Mean action noise std: 0.60
                       Mean reward: 7.26
               Mean episode length: 724.51
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0730
       Mean episode rew_ang_vel_xy: -0.0638
        Mean episode rew_collision: -0.0235
          Mean episode rew_dof_acc: -0.0674
   Mean episode rew_dof_pos_limits: -0.0014
        Mean episode rew_lin_vel_z: -0.0173
          Mean episode rew_torques: -0.0350
 Mean episode rew_tracking_ang_vel: 0.2181
 Mean episode rew_tracking_lin_vel: 0.4198
            Mean episode rew_total: 0.3564
        Mean episode terrain_level: 1.4700
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4992
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 2.73s
                        Total time: 331.95s
                               ETA: 22251.8s

################################################################################
                     [1m Learning iteration 147/10000 [0m                     

                       Computation: 40596 steps/s (collection: 2.056s, learning 0.365s)
               Value function loss: 0.0083
           Forward prediction loss: 0.0000
                          CMT loss: 0.0042
                    Surrogate loss: 0.0004
                    Policy entropy: 10.9212
             Mean action noise std: 0.60
                       Mean reward: 7.04
               Mean episode length: 734.70
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0759
       Mean episode rew_ang_vel_xy: -0.0657
        Mean episode rew_collision: -0.0234
          Mean episode rew_dof_acc: -0.0685
   Mean episode rew_dof_pos_limits: -0.0014
        Mean episode rew_lin_vel_z: -0.0171
          Mean episode rew_torques: -0.0362
 Mean episode rew_tracking_ang_vel: 0.2340
 Mean episode rew_tracking_lin_vel: 0.4320
            Mean episode rew_total: 0.3779
        Mean episode terrain_level: 1.4575
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4992
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 2.42s
                        Total time: 334.37s
                               ETA: 22260.4s

################################################################################
                     [1m Learning iteration 148/10000 [0m                     

                       Computation: 41170 steps/s (collection: 2.023s, learning 0.365s)
               Value function loss: 0.0107
           Forward prediction loss: 0.0000
                          CMT loss: 0.0042
                    Surrogate loss: 0.0001
                    Policy entropy: 10.9165
             Mean action noise std: 0.60
                       Mean reward: 7.71
               Mean episode length: 818.42
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0816
       Mean episode rew_ang_vel_xy: -0.0722
        Mean episode rew_collision: -0.0365
          Mean episode rew_dof_acc: -0.0756
   Mean episode rew_dof_pos_limits: -0.0016
        Mean episode rew_lin_vel_z: -0.0180
          Mean episode rew_torques: -0.0390
 Mean episode rew_tracking_ang_vel: 0.2517
 Mean episode rew_tracking_lin_vel: 0.4839
            Mean episode rew_total: 0.4110
        Mean episode terrain_level: 1.4441
    Mean episode min_command_x_vel: -0.9978
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4998
  Mean episode max_command_yaw_vel: 0.4992
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 2.39s
                        Total time: 336.76s
                               ETA: 22266.6s

################################################################################
                     [1m Learning iteration 149/10000 [0m                     

                       Computation: 40416 steps/s (collection: 2.067s, learning 0.365s)
               Value function loss: 0.0099
           Forward prediction loss: 0.0000
                          CMT loss: 0.0042
                    Surrogate loss: 0.0000
                    Policy entropy: 10.9104
             Mean action noise std: 0.60
                       Mean reward: 7.70
               Mean episode length: 818.74
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0842
       Mean episode rew_ang_vel_xy: -0.0732
        Mean episode rew_collision: -0.0678
          Mean episode rew_dof_acc: -0.0759
   Mean episode rew_dof_pos_limits: -0.0014
        Mean episode rew_lin_vel_z: -0.0184
          Mean episode rew_torques: -0.0402
 Mean episode rew_tracking_ang_vel: 0.2576
 Mean episode rew_tracking_lin_vel: 0.4926
            Mean episode rew_total: 0.3890
        Mean episode terrain_level: 1.4289
    Mean episode min_command_x_vel: -0.9973
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4997
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 2.43s
                        Total time: 339.19s
                               ETA: 22275.7s

################################################################################
                     [1m Learning iteration 150/10000 [0m                     

                       Computation: 39576 steps/s (collection: 2.121s, learning 0.363s)
               Value function loss: 0.0098
           Forward prediction loss: 0.0000
                          CMT loss: 0.0043
                    Surrogate loss: 0.0020
                    Policy entropy: 10.9075
             Mean action noise std: 0.60
                       Mean reward: 7.83
               Mean episode length: 843.09
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0815
       Mean episode rew_ang_vel_xy: -0.0710
        Mean episode rew_collision: -0.0584
          Mean episode rew_dof_acc: -0.0770
   Mean episode rew_dof_pos_limits: -0.0015
        Mean episode rew_lin_vel_z: -0.0179
          Mean episode rew_torques: -0.0390
 Mean episode rew_tracking_ang_vel: 0.2506
 Mean episode rew_tracking_lin_vel: 0.4658
            Mean episode rew_total: 0.3700
        Mean episode terrain_level: 1.4141
    Mean episode min_command_x_vel: -0.9973
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4997
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 2.48s
                        Total time: 341.67s
                               ETA: 22287.9s

################################################################################
                     [1m Learning iteration 151/10000 [0m                     

                       Computation: 40504 steps/s (collection: 2.065s, learning 0.362s)
               Value function loss: 0.0102
           Forward prediction loss: 0.0000
                          CMT loss: 0.0044
                    Surrogate loss: 0.0011
                    Policy entropy: 10.9055
             Mean action noise std: 0.60
                       Mean reward: 6.63
               Mean episode length: 781.59
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0798
       Mean episode rew_ang_vel_xy: -0.0705
        Mean episode rew_collision: -0.0641
          Mean episode rew_dof_acc: -0.0742
   Mean episode rew_dof_pos_limits: -0.0014
        Mean episode rew_lin_vel_z: -0.0184
          Mean episode rew_torques: -0.0378
 Mean episode rew_tracking_ang_vel: 0.2462
 Mean episode rew_tracking_lin_vel: 0.4471
            Mean episode rew_total: 0.3471
        Mean episode terrain_level: 1.4003
    Mean episode min_command_x_vel: -0.9973
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4995
  Mean episode min_command_yaw_vel: -0.4997
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 2.43s
                        Total time: 344.10s
                               ETA: 22296.3s

################################################################################
                     [1m Learning iteration 152/10000 [0m                     

                       Computation: 41108 steps/s (collection: 2.027s, learning 0.365s)
               Value function loss: 0.0103
           Forward prediction loss: 0.0000
                          CMT loss: 0.0043
                    Surrogate loss: 0.0006
                    Policy entropy: 10.9015
             Mean action noise std: 0.60
                       Mean reward: 8.11
               Mean episode length: 792.00
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0753
       Mean episode rew_ang_vel_xy: -0.0661
        Mean episode rew_collision: -0.0261
          Mean episode rew_dof_acc: -0.0721
   Mean episode rew_dof_pos_limits: -0.0014
        Mean episode rew_lin_vel_z: -0.0180
          Mean episode rew_torques: -0.0362
 Mean episode rew_tracking_ang_vel: 0.2296
 Mean episode rew_tracking_lin_vel: 0.4385
            Mean episode rew_total: 0.3728
        Mean episode terrain_level: 1.3867
    Mean episode min_command_x_vel: -0.9973
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4993
    Mean episode max_command_y_vel: 0.4994
  Mean episode min_command_yaw_vel: -0.4996
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 2.39s
                        Total time: 346.49s
                               ETA: 22302.2s

################################################################################
                     [1m Learning iteration 153/10000 [0m                     

                       Computation: 39558 steps/s (collection: 2.119s, learning 0.366s)
               Value function loss: 0.0092
           Forward prediction loss: 0.0000
                          CMT loss: 0.0045
                    Surrogate loss: 0.0002
                    Policy entropy: 10.8974
             Mean action noise std: 0.60
                       Mean reward: 7.01
               Mean episode length: 744.16
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0748
       Mean episode rew_ang_vel_xy: -0.0644
        Mean episode rew_collision: -0.0262
          Mean episode rew_dof_acc: -0.0701
   Mean episode rew_dof_pos_limits: -0.0018
        Mean episode rew_lin_vel_z: -0.0178
          Mean episode rew_torques: -0.0358
 Mean episode rew_tracking_ang_vel: 0.2292
 Mean episode rew_tracking_lin_vel: 0.4184
            Mean episode rew_total: 0.3569
        Mean episode terrain_level: 1.3713
    Mean episode min_command_x_vel: -0.9987
    Mean episode max_command_x_vel: 0.5992
    Mean episode min_command_y_vel: -0.4993
    Mean episode max_command_y_vel: 0.4994
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 2.49s
                        Total time: 348.98s
                               ETA: 22314.1s

################################################################################
                     [1m Learning iteration 154/10000 [0m                     

                       Computation: 41685 steps/s (collection: 1.993s, learning 0.365s)
               Value function loss: 0.0099
           Forward prediction loss: 0.0000
                          CMT loss: 0.0044
                    Surrogate loss: 0.0003
                    Policy entropy: 10.8974
             Mean action noise std: 0.60
                       Mean reward: 8.53
               Mean episode length: 771.96
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0754
       Mean episode rew_ang_vel_xy: -0.0657
        Mean episode rew_collision: -0.0458
          Mean episode rew_dof_acc: -0.0719
   Mean episode rew_dof_pos_limits: -0.0014
        Mean episode rew_lin_vel_z: -0.0175
          Mean episode rew_torques: -0.0358
 Mean episode rew_tracking_ang_vel: 0.2360
 Mean episode rew_tracking_lin_vel: 0.4709
            Mean episode rew_total: 0.3933
        Mean episode terrain_level: 1.3572
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5992
    Mean episode min_command_y_vel: -0.4993
    Mean episode max_command_y_vel: 0.4994
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 2.36s
                        Total time: 351.33s
                               ETA: 22317.7s

################################################################################
                     [1m Learning iteration 155/10000 [0m                     

                       Computation: 42058 steps/s (collection: 1.970s, learning 0.367s)
               Value function loss: 0.0075
           Forward prediction loss: 0.0000
                          CMT loss: 0.0044
                    Surrogate loss: 0.0001
                    Policy entropy: 10.8949
             Mean action noise std: 0.60
                       Mean reward: 6.74
               Mean episode length: 684.07
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0677
       Mean episode rew_ang_vel_xy: -0.0591
        Mean episode rew_collision: -0.0260
          Mean episode rew_dof_acc: -0.0665
   Mean episode rew_dof_pos_limits: -0.0014
        Mean episode rew_lin_vel_z: -0.0171
          Mean episode rew_torques: -0.0323
 Mean episode rew_tracking_ang_vel: 0.2097
 Mean episode rew_tracking_lin_vel: 0.4010
            Mean episode rew_total: 0.3406
        Mean episode terrain_level: 1.3451
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5992
    Mean episode min_command_y_vel: -0.4993
    Mean episode max_command_y_vel: 0.4994
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 2.34s
                        Total time: 353.67s
                               ETA: 22319.8s

################################################################################
                     [1m Learning iteration 156/10000 [0m                     

                       Computation: 40759 steps/s (collection: 2.047s, learning 0.365s)
               Value function loss: 0.0074
           Forward prediction loss: 0.0000
                          CMT loss: 0.0043
                    Surrogate loss: 0.0015
                    Policy entropy: 10.8884
             Mean action noise std: 0.60
                       Mean reward: 6.80
               Mean episode length: 712.26
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0719
       Mean episode rew_ang_vel_xy: -0.0629
        Mean episode rew_collision: -0.0292
          Mean episode rew_dof_acc: -0.0681
   Mean episode rew_dof_pos_limits: -0.0018
        Mean episode rew_lin_vel_z: -0.0172
          Mean episode rew_torques: -0.0342
 Mean episode rew_tracking_ang_vel: 0.2273
 Mean episode rew_tracking_lin_vel: 0.4177
            Mean episode rew_total: 0.3598
        Mean episode terrain_level: 1.3345
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5992
    Mean episode min_command_y_vel: -0.4993
    Mean episode max_command_y_vel: 0.4994
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 2.41s
                        Total time: 356.08s
                               ETA: 22326.7s

################################################################################
                     [1m Learning iteration 157/10000 [0m                     

                       Computation: 40659 steps/s (collection: 2.054s, learning 0.363s)
               Value function loss: 0.0067
           Forward prediction loss: 0.0000
                          CMT loss: 0.0043
                    Surrogate loss: -0.0000
                    Policy entropy: 10.8774
             Mean action noise std: 0.60
                       Mean reward: 7.68
               Mean episode length: 774.40
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0754
       Mean episode rew_ang_vel_xy: -0.0655
        Mean episode rew_collision: -0.0279
          Mean episode rew_dof_acc: -0.0697
   Mean episode rew_dof_pos_limits: -0.0019
        Mean episode rew_lin_vel_z: -0.0171
          Mean episode rew_torques: -0.0367
 Mean episode rew_tracking_ang_vel: 0.2360
 Mean episode rew_tracking_lin_vel: 0.4560
            Mean episode rew_total: 0.3976
        Mean episode terrain_level: 1.3226
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5992
    Mean episode min_command_y_vel: -0.4993
    Mean episode max_command_y_vel: 0.4994
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 2.42s
                        Total time: 358.50s
                               ETA: 22333.7s

################################################################################
                     [1m Learning iteration 158/10000 [0m                     

                       Computation: 41601 steps/s (collection: 1.999s, learning 0.364s)
               Value function loss: 0.0069
           Forward prediction loss: 0.0000
                          CMT loss: 0.0042
                    Surrogate loss: 0.0001
                    Policy entropy: 10.8652
             Mean action noise std: 0.60
                       Mean reward: 7.49
               Mean episode length: 779.49
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0794
       Mean episode rew_ang_vel_xy: -0.0675
        Mean episode rew_collision: -0.0222
          Mean episode rew_dof_acc: -0.0747
   Mean episode rew_dof_pos_limits: -0.0017
        Mean episode rew_lin_vel_z: -0.0186
          Mean episode rew_torques: -0.0375
 Mean episode rew_tracking_ang_vel: 0.2400
 Mean episode rew_tracking_lin_vel: 0.4529
            Mean episode rew_total: 0.3915
        Mean episode terrain_level: 1.3137
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5992
    Mean episode min_command_y_vel: -0.4993
    Mean episode max_command_y_vel: 0.4994
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 2.36s
                        Total time: 360.86s
                               ETA: 22337.3s

################################################################################
                     [1m Learning iteration 159/10000 [0m                     

                       Computation: 40438 steps/s (collection: 2.063s, learning 0.367s)
               Value function loss: 0.0076
           Forward prediction loss: 0.0000
                          CMT loss: 0.0043
                    Surrogate loss: 0.0014
                    Policy entropy: 10.8500
             Mean action noise std: 0.60
                       Mean reward: 6.65
               Mean episode length: 713.48
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0713
       Mean episode rew_ang_vel_xy: -0.0614
        Mean episode rew_collision: -0.0332
          Mean episode rew_dof_acc: -0.0681
   Mean episode rew_dof_pos_limits: -0.0019
        Mean episode rew_lin_vel_z: -0.0174
          Mean episode rew_torques: -0.0343
 Mean episode rew_tracking_ang_vel: 0.2147
 Mean episode rew_tracking_lin_vel: 0.4020
            Mean episode rew_total: 0.3291
        Mean episode terrain_level: 1.3054
    Mean episode min_command_x_vel: -0.9984
    Mean episode max_command_x_vel: 0.5993
    Mean episode min_command_y_vel: -0.4994
    Mean episode max_command_y_vel: 0.4994
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 2.43s
                        Total time: 363.29s
                               ETA: 22344.9s

################################################################################
                     [1m Learning iteration 160/10000 [0m                     

                       Computation: 40958 steps/s (collection: 2.035s, learning 0.365s)
               Value function loss: 0.0081
           Forward prediction loss: 0.0000
                          CMT loss: 0.0043
                    Surrogate loss: -0.0004
                    Policy entropy: 10.8427
             Mean action noise std: 0.60
                       Mean reward: 7.32
               Mean episode length: 758.12
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0746
       Mean episode rew_ang_vel_xy: -0.0645
        Mean episode rew_collision: -0.0259
          Mean episode rew_dof_acc: -0.0725
   Mean episode rew_dof_pos_limits: -0.0021
        Mean episode rew_lin_vel_z: -0.0179
          Mean episode rew_torques: -0.0358
 Mean episode rew_tracking_ang_vel: 0.2314
 Mean episode rew_tracking_lin_vel: 0.4290
            Mean episode rew_total: 0.3672
        Mean episode terrain_level: 1.2989
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4995
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 2.40s
                        Total time: 365.70s
                               ETA: 22350.6s

################################################################################
                     [1m Learning iteration 161/10000 [0m                     

                       Computation: 42466 steps/s (collection: 1.952s, learning 0.363s)
               Value function loss: 0.0082
           Forward prediction loss: 0.0000
                          CMT loss: 0.0043
                    Surrogate loss: 0.0007
                    Policy entropy: 10.8330
             Mean action noise std: 0.60
                       Mean reward: 7.32
               Mean episode length: 741.59
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0729
       Mean episode rew_ang_vel_xy: -0.0620
        Mean episode rew_collision: -0.0204
          Mean episode rew_dof_acc: -0.0680
   Mean episode rew_dof_pos_limits: -0.0024
        Mean episode rew_lin_vel_z: -0.0177
          Mean episode rew_torques: -0.0351
 Mean episode rew_tracking_ang_vel: 0.2325
 Mean episode rew_tracking_lin_vel: 0.4337
            Mean episode rew_total: 0.3878
        Mean episode terrain_level: 1.2912
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4998
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 2.31s
                        Total time: 368.01s
                               ETA: 22350.9s

################################################################################
                     [1m Learning iteration 162/10000 [0m                     

                       Computation: 43159 steps/s (collection: 1.915s, learning 0.362s)
               Value function loss: 0.0081
           Forward prediction loss: 0.0000
                          CMT loss: 0.0043
                    Surrogate loss: 0.0005
                    Policy entropy: 10.8266
             Mean action noise std: 0.60
                       Mean reward: 5.46
               Mean episode length: 676.04
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0720
       Mean episode rew_ang_vel_xy: -0.0624
        Mean episode rew_collision: -0.0298
          Mean episode rew_dof_acc: -0.0740
   Mean episode rew_dof_pos_limits: -0.0022
        Mean episode rew_lin_vel_z: -0.0186
          Mean episode rew_torques: -0.0353
 Mean episode rew_tracking_ang_vel: 0.2162
 Mean episode rew_tracking_lin_vel: 0.3813
            Mean episode rew_total: 0.3032
        Mean episode terrain_level: 1.2836
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4998
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 2.28s
                        Total time: 370.29s
                               ETA: 22349.0s

################################################################################
                     [1m Learning iteration 163/10000 [0m                     

                       Computation: 41910 steps/s (collection: 1.982s, learning 0.363s)
               Value function loss: 0.0068
           Forward prediction loss: 0.0000
                          CMT loss: 0.0043
                    Surrogate loss: 0.0006
                    Policy entropy: 10.8214
             Mean action noise std: 0.60
                       Mean reward: 6.44
               Mean episode length: 747.82
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0754
       Mean episode rew_ang_vel_xy: -0.0653
        Mean episode rew_collision: -0.0600
          Mean episode rew_dof_acc: -0.0731
   Mean episode rew_dof_pos_limits: -0.0022
        Mean episode rew_lin_vel_z: -0.0183
          Mean episode rew_torques: -0.0355
 Mean episode rew_tracking_ang_vel: 0.2387
 Mean episode rew_tracking_lin_vel: 0.4402
            Mean episode rew_total: 0.3492
        Mean episode terrain_level: 1.2775
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4998
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 2.35s
                        Total time: 372.63s
                               ETA: 22351.2s

################################################################################
                     [1m Learning iteration 164/10000 [0m                     

                       Computation: 42677 steps/s (collection: 1.939s, learning 0.364s)
               Value function loss: 0.0077
           Forward prediction loss: 0.0000
                          CMT loss: 0.0043
                    Surrogate loss: 0.0008
                    Policy entropy: 10.8149
             Mean action noise std: 0.60
                       Mean reward: 6.54
               Mean episode length: 810.38
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0817
       Mean episode rew_ang_vel_xy: -0.0716
        Mean episode rew_collision: -0.0783
          Mean episode rew_dof_acc: -0.0814
   Mean episode rew_dof_pos_limits: -0.0024
        Mean episode rew_lin_vel_z: -0.0189
          Mean episode rew_torques: -0.0392
 Mean episode rew_tracking_ang_vel: 0.2597
 Mean episode rew_tracking_lin_vel: 0.4567
            Mean episode rew_total: 0.3430
        Mean episode terrain_level: 1.2700
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4998
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 2.30s
                        Total time: 374.94s
                               ETA: 22350.8s

################################################################################
                     [1m Learning iteration 165/10000 [0m                     

                       Computation: 42405 steps/s (collection: 1.952s, learning 0.366s)
               Value function loss: 0.0067
           Forward prediction loss: 0.0000
                          CMT loss: 0.0044
                    Surrogate loss: 0.0011
                    Policy entropy: 10.8057
             Mean action noise std: 0.60
                       Mean reward: 6.79
               Mean episode length: 748.84
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0734
       Mean episode rew_ang_vel_xy: -0.0633
        Mean episode rew_collision: -0.0260
          Mean episode rew_dof_acc: -0.0715
   Mean episode rew_dof_pos_limits: -0.0027
        Mean episode rew_lin_vel_z: -0.0182
          Mean episode rew_torques: -0.0359
 Mean episode rew_tracking_ang_vel: 0.2313
 Mean episode rew_tracking_lin_vel: 0.4012
            Mean episode rew_total: 0.3414
        Mean episode terrain_level: 1.2621
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4998
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.3911
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 2.32s
                        Total time: 377.25s
                               ETA: 22351.2s

################################################################################
                     [1m Learning iteration 166/10000 [0m                     

                       Computation: 34019 steps/s (collection: 2.526s, learning 0.363s)
               Value function loss: 0.0253
           Forward prediction loss: 0.0000
                          CMT loss: 0.0046
                    Surrogate loss: 0.0014
                    Policy entropy: 10.8025
             Mean action noise std: 0.60
                       Mean reward: 9.06
               Mean episode length: 994.28
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0778
       Mean episode rew_ang_vel_xy: -0.0673
        Mean episode rew_collision: -0.0384
          Mean episode rew_dof_acc: -0.0773
   Mean episode rew_dof_pos_limits: -0.0028
        Mean episode rew_lin_vel_z: -0.0188
          Mean episode rew_torques: -0.0374
 Mean episode rew_tracking_ang_vel: 0.2464
 Mean episode rew_tracking_lin_vel: 0.4253
            Mean episode rew_total: 0.3518
        Mean episode terrain_level: 1.2485
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4998
  Mean episode min_command_yaw_vel: -0.4994
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.4178
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 2.89s
                        Total time: 380.14s
                               ETA: 22385.3s

################################################################################
                     [1m Learning iteration 167/10000 [0m                     

                       Computation: 40557 steps/s (collection: 2.059s, learning 0.365s)
               Value function loss: 0.0081
           Forward prediction loss: 0.0000
                          CMT loss: 0.0060
                    Surrogate loss: 0.0065
                    Policy entropy: 10.8032
             Mean action noise std: 0.60
                       Mean reward: 7.12
               Mean episode length: 797.77
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0739
       Mean episode rew_ang_vel_xy: -0.0644
        Mean episode rew_collision: -0.0314
          Mean episode rew_dof_acc: -0.0743
   Mean episode rew_dof_pos_limits: -0.0022
        Mean episode rew_lin_vel_z: -0.0179
          Mean episode rew_torques: -0.0357
 Mean episode rew_tracking_ang_vel: 0.2298
 Mean episode rew_tracking_lin_vel: 0.3978
            Mean episode rew_total: 0.3277
        Mean episode terrain_level: 1.1835
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4998
  Mean episode min_command_yaw_vel: -0.4988
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 2.42s
                        Total time: 382.57s
                               ETA: 22391.6s

################################################################################
                     [1m Learning iteration 168/10000 [0m                     

                       Computation: 40609 steps/s (collection: 2.055s, learning 0.365s)
               Value function loss: 0.0071
           Forward prediction loss: 0.0000
                          CMT loss: 0.0044
                    Surrogate loss: 0.0016
                    Policy entropy: 10.8016
             Mean action noise std: 0.60
                       Mean reward: 7.78
               Mean episode length: 803.20
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0807
       Mean episode rew_ang_vel_xy: -0.0699
        Mean episode rew_collision: -0.0229
          Mean episode rew_dof_acc: -0.0789
   Mean episode rew_dof_pos_limits: -0.0030
        Mean episode rew_lin_vel_z: -0.0196
          Mean episode rew_torques: -0.0390
 Mean episode rew_tracking_ang_vel: 0.2490
 Mean episode rew_tracking_lin_vel: 0.4583
            Mean episode rew_total: 0.3932
        Mean episode terrain_level: 1.1778
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4998
  Mean episode min_command_yaw_vel: -0.4988
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 2.42s
                        Total time: 384.99s
                               ETA: 22397.7s

################################################################################
                     [1m Learning iteration 169/10000 [0m                     

                       Computation: 40685 steps/s (collection: 2.052s, learning 0.364s)
               Value function loss: 0.0095
           Forward prediction loss: 0.0000
                          CMT loss: 0.0045
                    Surrogate loss: 0.0006
                    Policy entropy: 10.7956
             Mean action noise std: 0.60
                       Mean reward: 7.95
               Mean episode length: 865.33
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0828
       Mean episode rew_ang_vel_xy: -0.0743
        Mean episode rew_collision: -0.0324
          Mean episode rew_dof_acc: -0.0857
   Mean episode rew_dof_pos_limits: -0.0025
        Mean episode rew_lin_vel_z: -0.0203
          Mean episode rew_torques: -0.0393
 Mean episode rew_tracking_ang_vel: 0.2567
 Mean episode rew_tracking_lin_vel: 0.4656
            Mean episode rew_total: 0.3849
        Mean episode terrain_level: 1.1689
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.4998
  Mean episode min_command_yaw_vel: -0.4988
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 2.42s
                        Total time: 387.41s
                               ETA: 22403.4s

################################################################################
                     [1m Learning iteration 170/10000 [0m                     

                       Computation: 38833 steps/s (collection: 2.166s, learning 0.366s)
               Value function loss: 0.0093
           Forward prediction loss: 0.0000
                          CMT loss: 0.0045
                    Surrogate loss: 0.0023
                    Policy entropy: 10.7930
             Mean action noise std: 0.60
                       Mean reward: 7.40
               Mean episode length: 841.72
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0854
       Mean episode rew_ang_vel_xy: -0.0732
        Mean episode rew_collision: -0.0321
          Mean episode rew_dof_acc: -0.0839
   Mean episode rew_dof_pos_limits: -0.0029
        Mean episode rew_lin_vel_z: -0.0196
          Mean episode rew_torques: -0.0409
 Mean episode rew_tracking_ang_vel: 0.2673
 Mean episode rew_tracking_lin_vel: 0.4799
            Mean episode rew_total: 0.4092
        Mean episode terrain_level: 1.1580
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.4998
  Mean episode min_command_yaw_vel: -0.4988
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 2.53s
                        Total time: 389.94s
                               ETA: 22415.7s

################################################################################
                     [1m Learning iteration 171/10000 [0m                     

                       Computation: 39051 steps/s (collection: 2.152s, learning 0.365s)
               Value function loss: 0.0096
           Forward prediction loss: 0.0000
                          CMT loss: 0.0044
                    Surrogate loss: 0.0010
                    Policy entropy: 10.7881
             Mean action noise std: 0.60
                       Mean reward: 8.07
               Mean episode length: 840.04
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0825
       Mean episode rew_ang_vel_xy: -0.0720
        Mean episode rew_collision: -0.0311
          Mean episode rew_dof_acc: -0.0813
   Mean episode rew_dof_pos_limits: -0.0029
        Mean episode rew_lin_vel_z: -0.0193
          Mean episode rew_torques: -0.0398
 Mean episode rew_tracking_ang_vel: 0.2599
 Mean episode rew_tracking_lin_vel: 0.4833
            Mean episode rew_total: 0.4143
        Mean episode terrain_level: 1.1450
    Mean episode min_command_x_vel: -0.9963
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.4998
  Mean episode min_command_yaw_vel: -0.4989
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 2.52s
                        Total time: 392.45s
                               ETA: 22426.9s

################################################################################
                     [1m Learning iteration 172/10000 [0m                     

                       Computation: 39735 steps/s (collection: 2.107s, learning 0.367s)
               Value function loss: 0.0090
           Forward prediction loss: 0.0000
                          CMT loss: 0.0045
                    Surrogate loss: 0.0018
                    Policy entropy: 10.7829
             Mean action noise std: 0.60
                       Mean reward: 7.86
               Mean episode length: 847.70
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0851
       Mean episode rew_ang_vel_xy: -0.0736
        Mean episode rew_collision: -0.0313
          Mean episode rew_dof_acc: -0.0882
   Mean episode rew_dof_pos_limits: -0.0027
        Mean episode rew_lin_vel_z: -0.0216
          Mean episode rew_torques: -0.0406
 Mean episode rew_tracking_ang_vel: 0.2559
 Mean episode rew_tracking_lin_vel: 0.4577
            Mean episode rew_total: 0.3705
        Mean episode terrain_level: 1.1332
    Mean episode min_command_x_vel: -0.9952
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.4998
  Mean episode min_command_yaw_vel: -0.4988
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 2.47s
                        Total time: 394.93s
                               ETA: 22435.6s

################################################################################
                     [1m Learning iteration 173/10000 [0m                     

                       Computation: 39185 steps/s (collection: 2.144s, learning 0.365s)
               Value function loss: 0.0088
           Forward prediction loss: 0.0000
                          CMT loss: 0.0044
                    Surrogate loss: -0.0004
                    Policy entropy: 10.7776
             Mean action noise std: 0.60
                       Mean reward: 7.47
               Mean episode length: 816.65
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0781
       Mean episode rew_ang_vel_xy: -0.0666
        Mean episode rew_collision: -0.0416
          Mean episode rew_dof_acc: -0.0753
   Mean episode rew_dof_pos_limits: -0.0031
        Mean episode rew_lin_vel_z: -0.0183
          Mean episode rew_torques: -0.0374
 Mean episode rew_tracking_ang_vel: 0.2485
 Mean episode rew_tracking_lin_vel: 0.4397
            Mean episode rew_total: 0.3678
        Mean episode terrain_level: 1.1221
    Mean episode min_command_x_vel: -0.9952
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.4992
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 2.51s
                        Total time: 397.44s
                               ETA: 22446.0s

################################################################################
                     [1m Learning iteration 174/10000 [0m                     

                       Computation: 38902 steps/s (collection: 2.162s, learning 0.365s)
               Value function loss: 0.0089
           Forward prediction loss: 0.0000
                          CMT loss: 0.0045
                    Surrogate loss: 0.0002
                    Policy entropy: 10.7668
             Mean action noise std: 0.60
                       Mean reward: 7.44
               Mean episode length: 748.75
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0744
       Mean episode rew_ang_vel_xy: -0.0643
        Mean episode rew_collision: -0.0251
          Mean episode rew_dof_acc: -0.0728
   Mean episode rew_dof_pos_limits: -0.0032
        Mean episode rew_lin_vel_z: -0.0184
          Mean episode rew_torques: -0.0356
 Mean episode rew_tracking_ang_vel: 0.2374
 Mean episode rew_tracking_lin_vel: 0.4213
            Mean episode rew_total: 0.3648
        Mean episode terrain_level: 1.1122
    Mean episode min_command_x_vel: -0.9952
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.4996
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 2.53s
                        Total time: 399.96s
                               ETA: 22457.4s

################################################################################
                     [1m Learning iteration 175/10000 [0m                     

                       Computation: 39922 steps/s (collection: 2.099s, learning 0.363s)
               Value function loss: 0.0086
           Forward prediction loss: 0.0000
                          CMT loss: 0.0045
                    Surrogate loss: 0.0008
                    Policy entropy: 10.7597
             Mean action noise std: 0.59
                       Mean reward: 7.65
               Mean episode length: 799.84
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0807
       Mean episode rew_ang_vel_xy: -0.0700
        Mean episode rew_collision: -0.0342
          Mean episode rew_dof_acc: -0.0833
   Mean episode rew_dof_pos_limits: -0.0030
        Mean episode rew_lin_vel_z: -0.0200
          Mean episode rew_torques: -0.0380
 Mean episode rew_tracking_ang_vel: 0.2535
 Mean episode rew_tracking_lin_vel: 0.4699
            Mean episode rew_total: 0.3942
        Mean episode terrain_level: 1.1008
    Mean episode min_command_x_vel: -0.9952
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.4996
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 2.46s
                        Total time: 402.43s
                               ETA: 22465.0s

################################################################################
                     [1m Learning iteration 176/10000 [0m                     

                       Computation: 40343 steps/s (collection: 2.071s, learning 0.365s)
               Value function loss: 0.0079
           Forward prediction loss: 0.0000
                          CMT loss: 0.0045
                    Surrogate loss: 0.0003
                    Policy entropy: 10.7544
             Mean action noise std: 0.59
                       Mean reward: 6.90
               Mean episode length: 748.28
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0747
       Mean episode rew_ang_vel_xy: -0.0654
        Mean episode rew_collision: -0.0355
          Mean episode rew_dof_acc: -0.0772
   Mean episode rew_dof_pos_limits: -0.0029
        Mean episode rew_lin_vel_z: -0.0196
          Mean episode rew_torques: -0.0357
 Mean episode rew_tracking_ang_vel: 0.2348
 Mean episode rew_tracking_lin_vel: 0.4288
            Mean episode rew_total: 0.3527
        Mean episode terrain_level: 1.0909
    Mean episode min_command_x_vel: -0.9963
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.4996
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 2.44s
                        Total time: 404.86s
                               ETA: 22471.0s

################################################################################
                     [1m Learning iteration 177/10000 [0m                     

                       Computation: 39529 steps/s (collection: 2.121s, learning 0.366s)
               Value function loss: 0.0078
           Forward prediction loss: 0.0000
                          CMT loss: 0.0044
                    Surrogate loss: 0.0000
                    Policy entropy: 10.7449
             Mean action noise std: 0.59
                       Mean reward: 7.83
               Mean episode length: 796.76
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0765
       Mean episode rew_ang_vel_xy: -0.0652
        Mean episode rew_collision: -0.0222
          Mean episode rew_dof_acc: -0.0755
   Mean episode rew_dof_pos_limits: -0.0034
        Mean episode rew_lin_vel_z: -0.0188
          Mean episode rew_torques: -0.0364
 Mean episode rew_tracking_ang_vel: 0.2450
 Mean episode rew_tracking_lin_vel: 0.4404
            Mean episode rew_total: 0.3874
        Mean episode terrain_level: 1.0826
    Mean episode min_command_x_vel: -0.9989
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.4996
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 2.49s
                        Total time: 407.35s
                               ETA: 22479.7s

################################################################################
                     [1m Learning iteration 178/10000 [0m                     

                       Computation: 40247 steps/s (collection: 2.076s, learning 0.366s)
               Value function loss: 0.0072
           Forward prediction loss: 0.0000
                          CMT loss: 0.0044
                    Surrogate loss: 0.0006
                    Policy entropy: 10.7364
             Mean action noise std: 0.59
                       Mean reward: 7.05
               Mean episode length: 780.57
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0779
       Mean episode rew_ang_vel_xy: -0.0665
        Mean episode rew_collision: -0.0251
          Mean episode rew_dof_acc: -0.0817
   Mean episode rew_dof_pos_limits: -0.0035
        Mean episode rew_lin_vel_z: -0.0206
          Mean episode rew_torques: -0.0368
 Mean episode rew_tracking_ang_vel: 0.2365
 Mean episode rew_tracking_lin_vel: 0.4137
            Mean episode rew_total: 0.3381
        Mean episode terrain_level: 1.0749
    Mean episode min_command_x_vel: -0.9989
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.4996
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 2.44s
                        Total time: 409.79s
                               ETA: 22485.9s

################################################################################
                     [1m Learning iteration 179/10000 [0m                     

                       Computation: 40272 steps/s (collection: 2.075s, learning 0.366s)
               Value function loss: 0.0070
           Forward prediction loss: 0.0000
                          CMT loss: 0.0044
                    Surrogate loss: 0.0011
                    Policy entropy: 10.7283
             Mean action noise std: 0.59
                       Mean reward: 6.97
               Mean episode length: 776.10
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0732
       Mean episode rew_ang_vel_xy: -0.0639
        Mean episode rew_collision: -0.0335
          Mean episode rew_dof_acc: -0.0734
   Mean episode rew_dof_pos_limits: -0.0035
        Mean episode rew_lin_vel_z: -0.0189
          Mean episode rew_torques: -0.0350
 Mean episode rew_tracking_ang_vel: 0.2327
 Mean episode rew_tracking_lin_vel: 0.4094
            Mean episode rew_total: 0.3407
        Mean episode terrain_level: 1.0681
    Mean episode min_command_x_vel: -0.9989
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.4996
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 2.44s
                        Total time: 412.23s
                               ETA: 22491.9s

################################################################################
                     [1m Learning iteration 180/10000 [0m                     

                       Computation: 40422 steps/s (collection: 2.068s, learning 0.364s)
               Value function loss: 0.0075
           Forward prediction loss: 0.0000
                          CMT loss: 0.0043
                    Surrogate loss: 0.0002
                    Policy entropy: 10.7210
             Mean action noise std: 0.59
                       Mean reward: 7.16
               Mean episode length: 822.79
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0832
       Mean episode rew_ang_vel_xy: -0.0708
        Mean episode rew_collision: -0.0387
          Mean episode rew_dof_acc: -0.0858
   Mean episode rew_dof_pos_limits: -0.0035
        Mean episode rew_lin_vel_z: -0.0205
          Mean episode rew_torques: -0.0396
 Mean episode rew_tracking_ang_vel: 0.2550
 Mean episode rew_tracking_lin_vel: 0.4461
            Mean episode rew_total: 0.3588
        Mean episode terrain_level: 1.0592
    Mean episode min_command_x_vel: -0.9989
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.4996
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 2.43s
                        Total time: 414.66s
                               ETA: 22497.3s

################################################################################
                     [1m Learning iteration 181/10000 [0m                     

                       Computation: 40446 steps/s (collection: 2.064s, learning 0.366s)
               Value function loss: 0.0079
           Forward prediction loss: 0.0000
                          CMT loss: 0.0043
                    Surrogate loss: 0.0013
                    Policy entropy: 10.7131
             Mean action noise std: 0.59
                       Mean reward: 7.44
               Mean episode length: 773.54
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0782
       Mean episode rew_ang_vel_xy: -0.0674
        Mean episode rew_collision: -0.0217
          Mean episode rew_dof_acc: -0.0849
   Mean episode rew_dof_pos_limits: -0.0033
        Mean episode rew_lin_vel_z: -0.0208
          Mean episode rew_torques: -0.0372
 Mean episode rew_tracking_ang_vel: 0.2396
 Mean episode rew_tracking_lin_vel: 0.4524
            Mean episode rew_total: 0.3786
        Mean episode terrain_level: 1.0503
    Mean episode min_command_x_vel: -0.9989
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 2.43s
                        Total time: 417.10s
                               ETA: 22502.5s

################################################################################
                     [1m Learning iteration 182/10000 [0m                     

                       Computation: 40091 steps/s (collection: 2.088s, learning 0.364s)
               Value function loss: 0.0072
           Forward prediction loss: 0.0000
                          CMT loss: 0.0044
                    Surrogate loss: 0.0014
                    Policy entropy: 10.7023
             Mean action noise std: 0.59
                       Mean reward: 8.33
               Mean episode length: 887.91
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0863
       Mean episode rew_ang_vel_xy: -0.0734
        Mean episode rew_collision: -0.0538
          Mean episode rew_dof_acc: -0.0836
   Mean episode rew_dof_pos_limits: -0.0044
        Mean episode rew_lin_vel_z: -0.0199
          Mean episode rew_torques: -0.0410
 Mean episode rew_tracking_ang_vel: 0.2742
 Mean episode rew_tracking_lin_vel: 0.4984
            Mean episode rew_total: 0.4102
        Mean episode terrain_level: 1.0422
    Mean episode min_command_x_vel: -0.9989
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 2.45s
                        Total time: 419.55s
                               ETA: 22508.8s

################################################################################
                     [1m Learning iteration 183/10000 [0m                     

                       Computation: 40388 steps/s (collection: 2.068s, learning 0.366s)
               Value function loss: 0.0075
           Forward prediction loss: 0.0000
                          CMT loss: 0.0043
                    Surrogate loss: 0.0005
                    Policy entropy: 10.6934
             Mean action noise std: 0.59
                       Mean reward: 8.00
               Mean episode length: 862.10
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0858
       Mean episode rew_ang_vel_xy: -0.0743
        Mean episode rew_collision: -0.0352
          Mean episode rew_dof_acc: -0.0897
   Mean episode rew_dof_pos_limits: -0.0041
        Mean episode rew_lin_vel_z: -0.0209
          Mean episode rew_torques: -0.0405
 Mean episode rew_tracking_ang_vel: 0.2647
 Mean episode rew_tracking_lin_vel: 0.4898
            Mean episode rew_total: 0.4040
        Mean episode terrain_level: 1.0319
    Mean episode min_command_x_vel: -0.9990
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 2.43s
                        Total time: 421.98s
                               ETA: 22514.1s

################################################################################
                     [1m Learning iteration 184/10000 [0m                     

                       Computation: 39912 steps/s (collection: 2.096s, learning 0.367s)
               Value function loss: 0.0071
           Forward prediction loss: 0.0000
                          CMT loss: 0.0045
                    Surrogate loss: 0.0017
                    Policy entropy: 10.6846
             Mean action noise std: 0.59
                       Mean reward: 8.23
               Mean episode length: 807.89
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0764
       Mean episode rew_ang_vel_xy: -0.0665
        Mean episode rew_collision: -0.0563
          Mean episode rew_dof_acc: -0.0764
   Mean episode rew_dof_pos_limits: -0.0037
        Mean episode rew_lin_vel_z: -0.0179
          Mean episode rew_torques: -0.0364
 Mean episode rew_tracking_ang_vel: 0.2450
 Mean episode rew_tracking_lin_vel: 0.4888
            Mean episode rew_total: 0.4001
        Mean episode terrain_level: 1.0245
    Mean episode min_command_x_vel: -0.9989
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 2.46s
                        Total time: 424.44s
                               ETA: 22520.8s

################################################################################
                     [1m Learning iteration 185/10000 [0m                     

                       Computation: 39925 steps/s (collection: 2.098s, learning 0.364s)
               Value function loss: 0.0082
           Forward prediction loss: 0.0000
                          CMT loss: 0.0044
                    Surrogate loss: 0.0004
                    Policy entropy: 10.6809
             Mean action noise std: 0.59
                       Mean reward: 6.84
               Mean episode length: 782.10
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0746
       Mean episode rew_ang_vel_xy: -0.0648
        Mean episode rew_collision: -0.0501
          Mean episode rew_dof_acc: -0.0764
   Mean episode rew_dof_pos_limits: -0.0039
        Mean episode rew_lin_vel_z: -0.0191
          Mean episode rew_torques: -0.0356
 Mean episode rew_tracking_ang_vel: 0.2350
 Mean episode rew_tracking_lin_vel: 0.4252
            Mean episode rew_total: 0.3356
        Mean episode terrain_level: 1.0174
    Mean episode min_command_x_vel: -0.9989
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.4996
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 2.46s
                        Total time: 426.91s
                               ETA: 22527.3s

################################################################################
                     [1m Learning iteration 186/10000 [0m                     

                       Computation: 41615 steps/s (collection: 1.999s, learning 0.363s)
               Value function loss: 0.0068
           Forward prediction loss: 0.0000
                          CMT loss: 0.0044
                    Surrogate loss: 0.0009
                    Policy entropy: 10.6759
             Mean action noise std: 0.59
                       Mean reward: 7.65
               Mean episode length: 778.08
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0773
       Mean episode rew_ang_vel_xy: -0.0656
        Mean episode rew_collision: -0.0256
          Mean episode rew_dof_acc: -0.0787
   Mean episode rew_dof_pos_limits: -0.0043
        Mean episode rew_lin_vel_z: -0.0201
          Mean episode rew_torques: -0.0363
 Mean episode rew_tracking_ang_vel: 0.2461
 Mean episode rew_tracking_lin_vel: 0.4573
            Mean episode rew_total: 0.3955
        Mean episode terrain_level: 1.0086
    Mean episode min_command_x_vel: -0.9989
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.4996
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 2.36s
                        Total time: 429.27s
                               ETA: 22528.6s

################################################################################
                     [1m Learning iteration 187/10000 [0m                     

                       Computation: 37560 steps/s (collection: 2.255s, learning 0.362s)
               Value function loss: 0.0192
           Forward prediction loss: 0.0000
                          CMT loss: 0.0044
                    Surrogate loss: 0.0005
                    Policy entropy: 10.6700
             Mean action noise std: 0.59
                       Mean reward: 7.57
               Mean episode length: 812.05
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0796
       Mean episode rew_ang_vel_xy: -0.0681
        Mean episode rew_collision: -0.0381
          Mean episode rew_dof_acc: -0.0814
   Mean episode rew_dof_pos_limits: -0.0042
        Mean episode rew_lin_vel_z: -0.0203
          Mean episode rew_torques: -0.0383
 Mean episode rew_tracking_ang_vel: 0.2496
 Mean episode rew_tracking_lin_vel: 0.4528
            Mean episode rew_total: 0.3724
        Mean episode terrain_level: 1.0015
    Mean episode min_command_x_vel: -0.9989
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.4996
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 2.62s
                        Total time: 431.89s
                               ETA: 22543.1s

################################################################################
                     [1m Learning iteration 188/10000 [0m                     

                       Computation: 40583 steps/s (collection: 2.052s, learning 0.370s)
               Value function loss: 0.0063
           Forward prediction loss: 0.0000
                          CMT loss: 0.0045
                    Surrogate loss: 0.0014
                    Policy entropy: 10.6689
             Mean action noise std: 0.59
                       Mean reward: 8.31
               Mean episode length: 833.69
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0808
       Mean episode rew_ang_vel_xy: -0.0689
        Mean episode rew_collision: -0.0265
          Mean episode rew_dof_acc: -0.0795
   Mean episode rew_dof_pos_limits: -0.0047
        Mean episode rew_lin_vel_z: -0.0195
          Mean episode rew_torques: -0.0379
 Mean episode rew_tracking_ang_vel: 0.2579
 Mean episode rew_tracking_lin_vel: 0.4724
            Mean episode rew_total: 0.4124
        Mean episode terrain_level: 0.9956
    Mean episode min_command_x_vel: -0.9989
    Mean episode max_command_x_vel: 0.5984
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.4996
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 2.42s
                        Total time: 434.31s
                               ETA: 22547.3s

################################################################################
                     [1m Learning iteration 189/10000 [0m                     

                       Computation: 41210 steps/s (collection: 2.021s, learning 0.365s)
               Value function loss: 0.0075
           Forward prediction loss: 0.0000
                          CMT loss: 0.0046
                    Surrogate loss: 0.0012
                    Policy entropy: 10.6629
             Mean action noise std: 0.59
                       Mean reward: 7.48
               Mean episode length: 847.68
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0829
       Mean episode rew_ang_vel_xy: -0.0699
        Mean episode rew_collision: -0.0486
          Mean episode rew_dof_acc: -0.0847
   Mean episode rew_dof_pos_limits: -0.0049
        Mean episode rew_lin_vel_z: -0.0207
          Mean episode rew_torques: -0.0389
 Mean episode rew_tracking_ang_vel: 0.2608
 Mean episode rew_tracking_lin_vel: 0.4705
            Mean episode rew_total: 0.3809
        Mean episode terrain_level: 0.9863
    Mean episode min_command_x_vel: -0.9989
    Mean episode max_command_x_vel: 0.5984
    Mean episode min_command_y_vel: -0.4991
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.4996
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 2.39s
                        Total time: 436.69s
                               ETA: 22549.5s

################################################################################
                     [1m Learning iteration 190/10000 [0m                     

                       Computation: 40738 steps/s (collection: 2.050s, learning 0.363s)
               Value function loss: 0.0082
           Forward prediction loss: 0.0000
                          CMT loss: 0.0045
                    Surrogate loss: 0.0000
                    Policy entropy: 10.6558
             Mean action noise std: 0.59
                       Mean reward: 8.35
               Mean episode length: 799.13
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0776
       Mean episode rew_ang_vel_xy: -0.0662
        Mean episode rew_collision: -0.0308
          Mean episode rew_dof_acc: -0.0805
   Mean episode rew_dof_pos_limits: -0.0044
        Mean episode rew_lin_vel_z: -0.0196
          Mean episode rew_torques: -0.0362
 Mean episode rew_tracking_ang_vel: 0.2527
 Mean episode rew_tracking_lin_vel: 0.4670
            Mean episode rew_total: 0.4044
        Mean episode terrain_level: 0.9790
    Mean episode min_command_x_vel: -0.9989
    Mean episode max_command_x_vel: 0.5986
    Mean episode min_command_y_vel: -0.4991
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.4996
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 2.41s
                        Total time: 439.11s
                               ETA: 22553.1s

################################################################################
                     [1m Learning iteration 191/10000 [0m                     

                       Computation: 40545 steps/s (collection: 2.061s, learning 0.364s)
               Value function loss: 0.0082
           Forward prediction loss: 0.0000
                          CMT loss: 0.0046
                    Surrogate loss: 0.0015
                    Policy entropy: 10.6527
             Mean action noise std: 0.59
                       Mean reward: 8.85
               Mean episode length: 892.86
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0864
       Mean episode rew_ang_vel_xy: -0.0726
        Mean episode rew_collision: -0.0415
          Mean episode rew_dof_acc: -0.0842
   Mean episode rew_dof_pos_limits: -0.0052
        Mean episode rew_lin_vel_z: -0.0205
          Mean episode rew_torques: -0.0408
 Mean episode rew_tracking_ang_vel: 0.2794
 Mean episode rew_tracking_lin_vel: 0.5123
            Mean episode rew_total: 0.4406
        Mean episode terrain_level: 0.9710
    Mean episode min_command_x_vel: -0.9989
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4991
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.4996
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 2.42s
                        Total time: 441.53s
                               ETA: 22557.2s

################################################################################
                     [1m Learning iteration 192/10000 [0m                     

                       Computation: 40626 steps/s (collection: 2.058s, learning 0.362s)
               Value function loss: 0.0085
           Forward prediction loss: 0.0000
                          CMT loss: 0.0046
                    Surrogate loss: 0.0007
                    Policy entropy: 10.6500
             Mean action noise std: 0.59
                       Mean reward: 7.72
               Mean episode length: 849.26
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0841
       Mean episode rew_ang_vel_xy: -0.0724
        Mean episode rew_collision: -0.0336
          Mean episode rew_dof_acc: -0.0842
   Mean episode rew_dof_pos_limits: -0.0051
        Mean episode rew_lin_vel_z: -0.0205
          Mean episode rew_torques: -0.0399
 Mean episode rew_tracking_ang_vel: 0.2670
 Mean episode rew_tracking_lin_vel: 0.4774
            Mean episode rew_total: 0.4048
        Mean episode terrain_level: 0.9639
    Mean episode min_command_x_vel: -0.9989
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4992
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.4996
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 2.42s
                        Total time: 443.95s
                               ETA: 22561.0s

################################################################################
                     [1m Learning iteration 193/10000 [0m                     

                       Computation: 42705 steps/s (collection: 1.939s, learning 0.363s)
               Value function loss: 0.0070
           Forward prediction loss: 0.0000
                          CMT loss: 0.0047
                    Surrogate loss: 0.0008
                    Policy entropy: 10.6478
             Mean action noise std: 0.59
                       Mean reward: 8.56
               Mean episode length: 883.53
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0855
       Mean episode rew_ang_vel_xy: -0.0722
        Mean episode rew_collision: -0.0351
          Mean episode rew_dof_acc: -0.0844
   Mean episode rew_dof_pos_limits: -0.0054
        Mean episode rew_lin_vel_z: -0.0205
          Mean episode rew_torques: -0.0395
 Mean episode rew_tracking_ang_vel: 0.2803
 Mean episode rew_tracking_lin_vel: 0.4848
            Mean episode rew_total: 0.4224
        Mean episode terrain_level: 0.9557
    Mean episode min_command_x_vel: -0.9989
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4992
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.4996
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 2.30s
                        Total time: 446.25s
                               ETA: 22558.8s

################################################################################
                     [1m Learning iteration 194/10000 [0m                     

                       Computation: 43124 steps/s (collection: 1.916s, learning 0.363s)
               Value function loss: 0.0089
           Forward prediction loss: 0.0000
                          CMT loss: 0.0047
                    Surrogate loss: 0.0003
                    Policy entropy: 10.6449
             Mean action noise std: 0.59
                       Mean reward: 7.69
               Mean episode length: 840.95
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0788
       Mean episode rew_ang_vel_xy: -0.0671
        Mean episode rew_collision: -0.0240
          Mean episode rew_dof_acc: -0.0812
   Mean episode rew_dof_pos_limits: -0.0053
        Mean episode rew_lin_vel_z: -0.0200
          Mean episode rew_torques: -0.0367
 Mean episode rew_tracking_ang_vel: 0.2541
 Mean episode rew_tracking_lin_vel: 0.4420
            Mean episode rew_total: 0.3830
        Mean episode terrain_level: 0.9471
    Mean episode min_command_x_vel: -0.9989
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4992
    Mean episode max_command_y_vel: 0.4998
  Mean episode min_command_yaw_vel: -0.4994
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 2.28s
                        Total time: 448.53s
                               ETA: 22555.4s

################################################################################
                     [1m Learning iteration 195/10000 [0m                     

                       Computation: 39847 steps/s (collection: 2.102s, learning 0.365s)
               Value function loss: 0.0089
           Forward prediction loss: 0.0000
                          CMT loss: 0.0048
                    Surrogate loss: 0.0013
                    Policy entropy: 10.6429
             Mean action noise std: 0.59
                       Mean reward: 8.28
               Mean episode length: 809.08
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0795
       Mean episode rew_ang_vel_xy: -0.0668
        Mean episode rew_collision: -0.0324
          Mean episode rew_dof_acc: -0.0803
   Mean episode rew_dof_pos_limits: -0.0053
        Mean episode rew_lin_vel_z: -0.0195
          Mean episode rew_torques: -0.0372
 Mean episode rew_tracking_ang_vel: 0.2540
 Mean episode rew_tracking_lin_vel: 0.4686
            Mean episode rew_total: 0.4015
        Mean episode terrain_level: 0.9403
    Mean episode min_command_x_vel: -0.9989
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4994
    Mean episode max_command_y_vel: 0.4998
  Mean episode min_command_yaw_vel: -0.4993
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 2.47s
                        Total time: 451.00s
                               ETA: 22561.5s

################################################################################
                     [1m Learning iteration 196/10000 [0m                     

                       Computation: 41677 steps/s (collection: 1.996s, learning 0.362s)
               Value function loss: 0.0079
           Forward prediction loss: 0.0000
                          CMT loss: 0.0047
                    Surrogate loss: 0.0005
                    Policy entropy: 10.6392
             Mean action noise std: 0.59
                       Mean reward: 8.87
               Mean episode length: 866.37
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0811
       Mean episode rew_ang_vel_xy: -0.0689
        Mean episode rew_collision: -0.0182
          Mean episode rew_dof_acc: -0.0816
   Mean episode rew_dof_pos_limits: -0.0054
        Mean episode rew_lin_vel_z: -0.0199
          Mean episode rew_torques: -0.0370
 Mean episode rew_tracking_ang_vel: 0.2639
 Mean episode rew_tracking_lin_vel: 0.4877
            Mean episode rew_total: 0.4395
        Mean episode terrain_level: 0.9329
    Mean episode min_command_x_vel: -0.9989
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4998
  Mean episode min_command_yaw_vel: -0.4991
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 2.36s
                        Total time: 453.36s
                               ETA: 22562.0s

################################################################################
                     [1m Learning iteration 197/10000 [0m                     

                       Computation: 42241 steps/s (collection: 1.965s, learning 0.363s)
               Value function loss: 0.0081
           Forward prediction loss: 0.0000
                          CMT loss: 0.0046
                    Surrogate loss: 0.0007
                    Policy entropy: 10.6324
             Mean action noise std: 0.59
                       Mean reward: 7.70
               Mean episode length: 773.05
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0737
       Mean episode rew_ang_vel_xy: -0.0628
        Mean episode rew_collision: -0.0258
          Mean episode rew_dof_acc: -0.0797
   Mean episode rew_dof_pos_limits: -0.0046
        Mean episode rew_lin_vel_z: -0.0202
          Mean episode rew_torques: -0.0345
 Mean episode rew_tracking_ang_vel: 0.2310
 Mean episode rew_tracking_lin_vel: 0.4505
            Mean episode rew_total: 0.3802
        Mean episode terrain_level: 0.9260
    Mean episode min_command_x_vel: -0.9971
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4998
  Mean episode min_command_yaw_vel: -0.4991
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 2.33s
                        Total time: 455.69s
                               ETA: 22561.0s

################################################################################
                     [1m Learning iteration 198/10000 [0m                     

                       Computation: 41352 steps/s (collection: 2.013s, learning 0.364s)
               Value function loss: 0.0074
           Forward prediction loss: 0.0000
                          CMT loss: 0.0047
                    Surrogate loss: 0.0016
                    Policy entropy: 10.6296
             Mean action noise std: 0.59
                       Mean reward: 7.65
               Mean episode length: 803.34
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0756
       Mean episode rew_ang_vel_xy: -0.0640
        Mean episode rew_collision: -0.0278
          Mean episode rew_dof_acc: -0.0766
   Mean episode rew_dof_pos_limits: -0.0054
        Mean episode rew_lin_vel_z: -0.0191
          Mean episode rew_torques: -0.0351
 Mean episode rew_tracking_ang_vel: 0.2471
 Mean episode rew_tracking_lin_vel: 0.4309
            Mean episode rew_total: 0.3745
        Mean episode terrain_level: 0.9195
    Mean episode min_command_x_vel: -0.9967
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4998
  Mean episode min_command_yaw_vel: -0.4991
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 2.38s
                        Total time: 458.06s
                               ETA: 22562.4s

################################################################################
                     [1m Learning iteration 199/10000 [0m                     

                       Computation: 40793 steps/s (collection: 2.046s, learning 0.363s)
               Value function loss: 0.0075
           Forward prediction loss: 0.0000
                          CMT loss: 0.0049
                    Surrogate loss: 0.0030
                    Policy entropy: 10.6275
             Mean action noise std: 0.59
                       Mean reward: 7.64
               Mean episode length: 794.79
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0787
       Mean episode rew_ang_vel_xy: -0.0665
        Mean episode rew_collision: -0.0307
          Mean episode rew_dof_acc: -0.0772
   Mean episode rew_dof_pos_limits: -0.0060
        Mean episode rew_lin_vel_z: -0.0196
          Mean episode rew_torques: -0.0368
 Mean episode rew_tracking_ang_vel: 0.2557
 Mean episode rew_tracking_lin_vel: 0.4673
            Mean episode rew_total: 0.4075
        Mean episode terrain_level: 0.9122
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4998
  Mean episode min_command_yaw_vel: -0.4991
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 2.41s
                        Total time: 460.47s
                               ETA: 22565.4s

################################################################################
                     [1m Learning iteration 200/10000 [0m                     

                       Computation: 42772 steps/s (collection: 1.936s, learning 0.363s)
               Value function loss: 0.0081
           Forward prediction loss: 0.0000
                          CMT loss: 0.0047
                    Surrogate loss: 0.0017
                    Policy entropy: 10.6219
             Mean action noise std: 0.59
                       Mean reward: 7.03
               Mean episode length: 792.46
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0768
       Mean episode rew_ang_vel_xy: -0.0673
        Mean episode rew_collision: -0.0897
          Mean episode rew_dof_acc: -0.0858
   Mean episode rew_dof_pos_limits: -0.0056
        Mean episode rew_lin_vel_z: -0.0197
          Mean episode rew_torques: -0.0352
 Mean episode rew_tracking_ang_vel: 0.2537
 Mean episode rew_tracking_lin_vel: 0.4257
            Mean episode rew_total: 0.2993
        Mean episode terrain_level: 0.9060
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4998
  Mean episode min_command_yaw_vel: -0.4993
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 2.30s
                        Total time: 462.77s
                               ETA: 22562.9s

################################################################################
                     [1m Learning iteration 201/10000 [0m                     

                       Computation: 43096 steps/s (collection: 1.916s, learning 0.365s)
               Value function loss: 0.0066
           Forward prediction loss: 0.0000
                          CMT loss: 0.0047
                    Surrogate loss: 0.0012
                    Policy entropy: 10.6187
             Mean action noise std: 0.59
                       Mean reward: 7.92
               Mean episode length: 839.50
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0830
       Mean episode rew_ang_vel_xy: -0.0710
        Mean episode rew_collision: -0.0475
          Mean episode rew_dof_acc: -0.0865
   Mean episode rew_dof_pos_limits: -0.0063
        Mean episode rew_lin_vel_z: -0.0202
          Mean episode rew_torques: -0.0385
 Mean episode rew_tracking_ang_vel: 0.2745
 Mean episode rew_tracking_lin_vel: 0.4820
            Mean episode rew_total: 0.4034
        Mean episode terrain_level: 0.9009
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4998
  Mean episode min_command_yaw_vel: -0.4993
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 2.28s
                        Total time: 465.05s
                               ETA: 22559.6s

################################################################################
                     [1m Learning iteration 202/10000 [0m                     

                       Computation: 40245 steps/s (collection: 2.076s, learning 0.367s)
               Value function loss: 0.0073
           Forward prediction loss: 0.0000
                          CMT loss: 0.0047
                    Surrogate loss: 0.0008
                    Policy entropy: 10.6143
             Mean action noise std: 0.59
                       Mean reward: 8.38
               Mean episode length: 843.07
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0819
       Mean episode rew_ang_vel_xy: -0.0691
        Mean episode rew_collision: -0.0432
          Mean episode rew_dof_acc: -0.0846
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0207
          Mean episode rew_torques: -0.0385
 Mean episode rew_tracking_ang_vel: 0.2670
 Mean episode rew_tracking_lin_vel: 0.4953
            Mean episode rew_total: 0.4181
        Mean episode terrain_level: 0.8959
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4998
  Mean episode min_command_yaw_vel: -0.4993
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 2.44s
                        Total time: 467.49s
                               ETA: 22564.1s

################################################################################
                     [1m Learning iteration 203/10000 [0m                     

                       Computation: 40736 steps/s (collection: 2.044s, learning 0.369s)
               Value function loss: 0.0082
           Forward prediction loss: 0.0000
                          CMT loss: 0.0047
                    Surrogate loss: 0.0005
                    Policy entropy: 10.6100
             Mean action noise std: 0.59
                       Mean reward: 8.36
               Mean episode length: 821.02
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0813
       Mean episode rew_ang_vel_xy: -0.0679
        Mean episode rew_collision: -0.0220
          Mean episode rew_dof_acc: -0.0850
   Mean episode rew_dof_pos_limits: -0.0062
        Mean episode rew_lin_vel_z: -0.0216
          Mean episode rew_torques: -0.0383
 Mean episode rew_tracking_ang_vel: 0.2623
 Mean episode rew_tracking_lin_vel: 0.4725
            Mean episode rew_total: 0.4125
        Mean episode terrain_level: 0.8908
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.4993
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 2.41s
                        Total time: 469.91s
                               ETA: 22567.1s

################################################################################
                     [1m Learning iteration 204/10000 [0m                     

                       Computation: 41679 steps/s (collection: 1.991s, learning 0.367s)
               Value function loss: 0.0067
           Forward prediction loss: 0.0000
                          CMT loss: 0.0047
                    Surrogate loss: 0.0016
                    Policy entropy: 10.6032
             Mean action noise std: 0.59
                       Mean reward: 7.62
               Mean episode length: 774.82
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0759
       Mean episode rew_ang_vel_xy: -0.0648
        Mean episode rew_collision: -0.0495
          Mean episode rew_dof_acc: -0.0833
   Mean episode rew_dof_pos_limits: -0.0053
        Mean episode rew_lin_vel_z: -0.0217
          Mean episode rew_torques: -0.0360
 Mean episode rew_tracking_ang_vel: 0.2402
 Mean episode rew_tracking_lin_vel: 0.4811
            Mean episode rew_total: 0.3848
        Mean episode terrain_level: 0.8873
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4993
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 2.36s
                        Total time: 472.27s
                               ETA: 22567.4s

################################################################################
                     [1m Learning iteration 205/10000 [0m                     

                       Computation: 41765 steps/s (collection: 1.986s, learning 0.367s)
               Value function loss: 0.0070
           Forward prediction loss: 0.0000
                          CMT loss: 0.0048
                    Surrogate loss: 0.0013
                    Policy entropy: 10.5960
             Mean action noise std: 0.59
                       Mean reward: 7.17
               Mean episode length: 745.05
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0706
       Mean episode rew_ang_vel_xy: -0.0612
        Mean episode rew_collision: -0.0221
          Mean episode rew_dof_acc: -0.0775
   Mean episode rew_dof_pos_limits: -0.0055
        Mean episode rew_lin_vel_z: -0.0197
          Mean episode rew_torques: -0.0330
 Mean episode rew_tracking_ang_vel: 0.2222
 Mean episode rew_tracking_lin_vel: 0.4254
            Mean episode rew_total: 0.3580
        Mean episode terrain_level: 0.8821
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4993
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 2.35s
                        Total time: 474.62s
                               ETA: 22567.5s

################################################################################
                     [1m Learning iteration 206/10000 [0m                     

                       Computation: 41247 steps/s (collection: 2.019s, learning 0.364s)
               Value function loss: 0.0070
           Forward prediction loss: 0.0000
                          CMT loss: 0.0047
                    Surrogate loss: 0.0007
                    Policy entropy: 10.5873
             Mean action noise std: 0.59
                       Mean reward: 8.29
               Mean episode length: 843.36
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0826
       Mean episode rew_ang_vel_xy: -0.0699
        Mean episode rew_collision: -0.0214
          Mean episode rew_dof_acc: -0.0857
   Mean episode rew_dof_pos_limits: -0.0073
        Mean episode rew_lin_vel_z: -0.0208
          Mean episode rew_torques: -0.0379
 Mean episode rew_tracking_ang_vel: 0.2723
 Mean episode rew_tracking_lin_vel: 0.4829
            Mean episode rew_total: 0.4295
        Mean episode terrain_level: 0.8767
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4997
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4993
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 2.38s
                        Total time: 477.00s
                               ETA: 22568.9s

################################################################################
                     [1m Learning iteration 207/10000 [0m                     

                       Computation: 42008 steps/s (collection: 1.977s, learning 0.363s)
               Value function loss: 0.0078
           Forward prediction loss: 0.0000
                          CMT loss: 0.0047
                    Surrogate loss: 0.0008
                    Policy entropy: 10.5828
             Mean action noise std: 0.59
                       Mean reward: 8.49
               Mean episode length: 829.22
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0821
       Mean episode rew_ang_vel_xy: -0.0701
        Mean episode rew_collision: -0.0358
          Mean episode rew_dof_acc: -0.0946
   Mean episode rew_dof_pos_limits: -0.0059
        Mean episode rew_lin_vel_z: -0.0219
          Mean episode rew_torques: -0.0378
 Mean episode rew_tracking_ang_vel: 0.2574
 Mean episode rew_tracking_lin_vel: 0.5033
            Mean episode rew_total: 0.4126
        Mean episode terrain_level: 0.8719
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4997
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4993
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.4978
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 2.34s
                        Total time: 479.34s
                               ETA: 22568.3s

################################################################################
                     [1m Learning iteration 208/10000 [0m                     

                       Computation: 34445 steps/s (collection: 2.487s, learning 0.367s)
               Value function loss: 0.0170
           Forward prediction loss: 0.0000
                          CMT loss: 0.0056
                    Surrogate loss: 0.0023
                    Policy entropy: 10.5810
             Mean action noise std: 0.59
                       Mean reward: 8.77
               Mean episode length: 952.99
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0746
       Mean episode rew_ang_vel_xy: -0.0628
        Mean episode rew_collision: -0.0247
          Mean episode rew_dof_acc: -0.0782
   Mean episode rew_dof_pos_limits: -0.0066
        Mean episode rew_lin_vel_z: -0.0197
          Mean episode rew_torques: -0.0347
 Mean episode rew_tracking_ang_vel: 0.2391
 Mean episode rew_tracking_lin_vel: 0.4470
            Mean episode rew_total: 0.3849
        Mean episode terrain_level: 0.8534
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4997
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4993
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.5676
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 2.85s
                        Total time: 482.20s
                               ETA: 22591.7s

################################################################################
                     [1m Learning iteration 209/10000 [0m                     

                       Computation: 40393 steps/s (collection: 2.069s, learning 0.364s)
               Value function loss: 0.0074
           Forward prediction loss: 0.0000
                          CMT loss: 0.0049
                    Surrogate loss: 0.0033
                    Policy entropy: 10.5806
             Mean action noise std: 0.59
                       Mean reward: 7.55
               Mean episode length: 777.03
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0786
       Mean episode rew_ang_vel_xy: -0.0649
        Mean episode rew_collision: -0.0268
          Mean episode rew_dof_acc: -0.0880
   Mean episode rew_dof_pos_limits: -0.0061
        Mean episode rew_lin_vel_z: -0.0215
          Mean episode rew_torques: -0.0367
 Mean episode rew_tracking_ang_vel: 0.2458
 Mean episode rew_tracking_lin_vel: 0.4595
            Mean episode rew_total: 0.3827
        Mean episode terrain_level: 0.8233
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4997
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4993
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 2.43s
                        Total time: 484.63s
                               ETA: 22595.3s

################################################################################
                     [1m Learning iteration 210/10000 [0m                     

                       Computation: 40617 steps/s (collection: 2.057s, learning 0.363s)
               Value function loss: 0.0077
           Forward prediction loss: 0.0000
                          CMT loss: 0.0049
                    Surrogate loss: 0.0021
                    Policy entropy: 10.5789
             Mean action noise std: 0.59
                       Mean reward: 6.57
               Mean episode length: 758.54
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0748
       Mean episode rew_ang_vel_xy: -0.0627
        Mean episode rew_collision: -0.0357
          Mean episode rew_dof_acc: -0.0802
   Mean episode rew_dof_pos_limits: -0.0067
        Mean episode rew_lin_vel_z: -0.0205
          Mean episode rew_torques: -0.0343
 Mean episode rew_tracking_ang_vel: 0.2365
 Mean episode rew_tracking_lin_vel: 0.4242
            Mean episode rew_total: 0.3457
        Mean episode terrain_level: 0.8192
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4997
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4993
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 2.42s
                        Total time: 487.05s
                               ETA: 22598.2s

################################################################################
                     [1m Learning iteration 211/10000 [0m                     

                       Computation: 38237 steps/s (collection: 2.204s, learning 0.367s)
               Value function loss: 0.0077
           Forward prediction loss: 0.0000
                          CMT loss: 0.0048
                    Surrogate loss: 0.0013
                    Policy entropy: 10.5754
             Mean action noise std: 0.59
                       Mean reward: 8.57
               Mean episode length: 853.72
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0801
       Mean episode rew_ang_vel_xy: -0.0675
        Mean episode rew_collision: -0.0196
          Mean episode rew_dof_acc: -0.0820
   Mean episode rew_dof_pos_limits: -0.0071
        Mean episode rew_lin_vel_z: -0.0205
          Mean episode rew_torques: -0.0366
 Mean episode rew_tracking_ang_vel: 0.2634
 Mean episode rew_tracking_lin_vel: 0.4646
            Mean episode rew_total: 0.4146
        Mean episode terrain_level: 0.8129
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.4997
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4993
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 2.57s
                        Total time: 489.62s
                               ETA: 22608.0s

################################################################################
                     [1m Learning iteration 212/10000 [0m                     

                       Computation: 38431 steps/s (collection: 2.194s, learning 0.364s)
               Value function loss: 0.0076
           Forward prediction loss: 0.0000
                          CMT loss: 0.0049
                    Surrogate loss: 0.0011
                    Policy entropy: 10.5707
             Mean action noise std: 0.59
                       Mean reward: 8.97
               Mean episode length: 881.81
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0841
       Mean episode rew_ang_vel_xy: -0.0699
        Mean episode rew_collision: -0.0255
          Mean episode rew_dof_acc: -0.0856
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0207
          Mean episode rew_torques: -0.0377
 Mean episode rew_tracking_ang_vel: 0.2777
 Mean episode rew_tracking_lin_vel: 0.5217
            Mean episode rew_total: 0.4679
        Mean episode terrain_level: 0.8051
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.5996
    Mean episode min_command_y_vel: -0.4997
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4993
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 2.56s
                        Total time: 492.18s
                               ETA: 22617.1s

################################################################################
                     [1m Learning iteration 213/10000 [0m                     

                       Computation: 39221 steps/s (collection: 2.140s, learning 0.366s)
               Value function loss: 0.0071
           Forward prediction loss: 0.0000
                          CMT loss: 0.0049
                    Surrogate loss: 0.0027
                    Policy entropy: 10.5675
             Mean action noise std: 0.59
                       Mean reward: 10.03
               Mean episode length: 911.18
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0855
       Mean episode rew_ang_vel_xy: -0.0707
        Mean episode rew_collision: -0.0309
          Mean episode rew_dof_acc: -0.0865
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0210
          Mean episode rew_torques: -0.0387
 Mean episode rew_tracking_ang_vel: 0.2831
 Mean episode rew_tracking_lin_vel: 0.5449
            Mean episode rew_total: 0.4864
        Mean episode terrain_level: 0.7977
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.6000
    Mean episode min_command_y_vel: -0.4997
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4993
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 2.51s
                        Total time: 494.69s
                               ETA: 22623.8s

################################################################################
                     [1m Learning iteration 214/10000 [0m                     

                       Computation: 40472 steps/s (collection: 2.065s, learning 0.364s)
               Value function loss: 0.0078
           Forward prediction loss: 0.0000
                          CMT loss: 0.0049
                    Surrogate loss: 0.0024
                    Policy entropy: 10.5637
             Mean action noise std: 0.59
                       Mean reward: 9.31
               Mean episode length: 887.98
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0800
       Mean episode rew_ang_vel_xy: -0.0671
        Mean episode rew_collision: -0.0285
          Mean episode rew_dof_acc: -0.0864
   Mean episode rew_dof_pos_limits: -0.0075
        Mean episode rew_lin_vel_z: -0.0208
          Mean episode rew_torques: -0.0360
 Mean episode rew_tracking_ang_vel: 0.2608
 Mean episode rew_tracking_lin_vel: 0.4831
            Mean episode rew_total: 0.4175
        Mean episode terrain_level: 0.7895
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.6000
    Mean episode min_command_y_vel: -0.4997
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4993
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 2.43s
                        Total time: 497.11s
                               ETA: 22626.8s

################################################################################
                     [1m Learning iteration 215/10000 [0m                     

                       Computation: 41438 steps/s (collection: 2.006s, learning 0.366s)
               Value function loss: 0.0066
           Forward prediction loss: 0.0000
                          CMT loss: 0.0049
                    Surrogate loss: 0.0014
                    Policy entropy: 10.5594
             Mean action noise std: 0.59
                       Mean reward: 8.57
               Mean episode length: 857.05
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0780
       Mean episode rew_ang_vel_xy: -0.0640
        Mean episode rew_collision: -0.0213
          Mean episode rew_dof_acc: -0.0823
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0205
          Mean episode rew_torques: -0.0352
 Mean episode rew_tracking_ang_vel: 0.2555
 Mean episode rew_tracking_lin_vel: 0.4546
            Mean episode rew_total: 0.4005
        Mean episode terrain_level: 0.7843
    Mean episode min_command_x_vel: -0.9969
    Mean episode max_command_x_vel: 0.6000
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 2.37s
                        Total time: 499.49s
                               ETA: 22627.2s

################################################################################
                     [1m Learning iteration 216/10000 [0m                     

                       Computation: 41759 steps/s (collection: 1.991s, learning 0.363s)
               Value function loss: 0.0079
           Forward prediction loss: 0.0000
                          CMT loss: 0.0048
                    Surrogate loss: 0.0018
                    Policy entropy: 10.5534
             Mean action noise std: 0.59
                       Mean reward: 8.49
               Mean episode length: 874.24
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0831
       Mean episode rew_ang_vel_xy: -0.0681
        Mean episode rew_collision: -0.0392
          Mean episode rew_dof_acc: -0.0827
   Mean episode rew_dof_pos_limits: -0.0091
        Mean episode rew_lin_vel_z: -0.0200
          Mean episode rew_torques: -0.0377
 Mean episode rew_tracking_ang_vel: 0.2792
 Mean episode rew_tracking_lin_vel: 0.4996
            Mean episode rew_total: 0.4388
        Mean episode terrain_level: 0.7811
    Mean episode min_command_x_vel: -0.9967
    Mean episode max_command_x_vel: 0.6000
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 2.35s
                        Total time: 501.84s
                               ETA: 22626.8s

################################################################################
                     [1m Learning iteration 217/10000 [0m                     

                       Computation: 39879 steps/s (collection: 2.100s, learning 0.365s)
               Value function loss: 0.0078
           Forward prediction loss: 0.0000
                          CMT loss: 0.0048
                    Surrogate loss: 0.0012
                    Policy entropy: 10.5503
             Mean action noise std: 0.58
                       Mean reward: 10.53
               Mean episode length: 920.22
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0858
       Mean episode rew_ang_vel_xy: -0.0715
        Mean episode rew_collision: -0.0392
          Mean episode rew_dof_acc: -0.0868
   Mean episode rew_dof_pos_limits: -0.0084
        Mean episode rew_lin_vel_z: -0.0205
          Mean episode rew_torques: -0.0380
 Mean episode rew_tracking_ang_vel: 0.2901
 Mean episode rew_tracking_lin_vel: 0.5751
            Mean episode rew_total: 0.5150
        Mean episode terrain_level: 0.7747
    Mean episode min_command_x_vel: -0.9966
    Mean episode max_command_x_vel: 0.6000
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 2.47s
                        Total time: 504.31s
                               ETA: 22631.3s

################################################################################
                     [1m Learning iteration 218/10000 [0m                     

                       Computation: 40326 steps/s (collection: 2.074s, learning 0.364s)
               Value function loss: 0.0064
           Forward prediction loss: 0.0000
                          CMT loss: 0.0049
                    Surrogate loss: 0.0022
                    Policy entropy: 10.5453
             Mean action noise std: 0.58
                       Mean reward: 9.61
               Mean episode length: 945.13
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0900
       Mean episode rew_ang_vel_xy: -0.0744
        Mean episode rew_collision: -0.0326
          Mean episode rew_dof_acc: -0.0938
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0226
          Mean episode rew_torques: -0.0401
 Mean episode rew_tracking_ang_vel: 0.2867
 Mean episode rew_tracking_lin_vel: 0.5406
            Mean episode rew_total: 0.4650
        Mean episode terrain_level: 0.7700
    Mean episode min_command_x_vel: -0.9959
    Mean episode max_command_x_vel: 0.6000
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 2.44s
                        Total time: 506.74s
                               ETA: 22634.6s

################################################################################
                     [1m Learning iteration 219/10000 [0m                     

                       Computation: 40550 steps/s (collection: 2.060s, learning 0.364s)
               Value function loss: 0.0063
           Forward prediction loss: 0.0000
                          CMT loss: 0.0049
                    Surrogate loss: 0.0012
                    Policy entropy: 10.5388
             Mean action noise std: 0.58
                       Mean reward: 8.56
               Mean episode length: 877.92
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0819
       Mean episode rew_ang_vel_xy: -0.0687
        Mean episode rew_collision: -0.0208
          Mean episode rew_dof_acc: -0.0926
   Mean episode rew_dof_pos_limits: -0.0083
        Mean episode rew_lin_vel_z: -0.0215
          Mean episode rew_torques: -0.0374
 Mean episode rew_tracking_ang_vel: 0.2609
 Mean episode rew_tracking_lin_vel: 0.4920
            Mean episode rew_total: 0.4217
        Mean episode terrain_level: 0.7659
    Mean episode min_command_x_vel: -0.9927
    Mean episode max_command_x_vel: 0.6000
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 2.42s
                        Total time: 509.17s
                               ETA: 22637.1s

################################################################################
                     [1m Learning iteration 220/10000 [0m                     

                       Computation: 39759 steps/s (collection: 2.109s, learning 0.363s)
               Value function loss: 0.0068
           Forward prediction loss: 0.0000
                          CMT loss: 0.0049
                    Surrogate loss: 0.0010
                    Policy entropy: 10.5313
             Mean action noise std: 0.58
                       Mean reward: 8.16
               Mean episode length: 835.49
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0788
       Mean episode rew_ang_vel_xy: -0.0668
        Mean episode rew_collision: -0.0274
          Mean episode rew_dof_acc: -0.0833
   Mean episode rew_dof_pos_limits: -0.0080
        Mean episode rew_lin_vel_z: -0.0204
          Mean episode rew_torques: -0.0353
 Mean episode rew_tracking_ang_vel: 0.2651
 Mean episode rew_tracking_lin_vel: 0.4807
            Mean episode rew_total: 0.4256
        Mean episode terrain_level: 0.7615
    Mean episode min_command_x_vel: -0.9929
    Mean episode max_command_x_vel: 0.6000
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 2.47s
                        Total time: 511.64s
                               ETA: 22641.8s

################################################################################
                     [1m Learning iteration 221/10000 [0m                     

                       Computation: 40018 steps/s (collection: 2.089s, learning 0.367s)
               Value function loss: 0.0067
           Forward prediction loss: 0.0000
                          CMT loss: 0.0048
                    Surrogate loss: 0.0016
                    Policy entropy: 10.5257
             Mean action noise std: 0.58
                       Mean reward: 8.25
               Mean episode length: 827.24
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0784
       Mean episode rew_ang_vel_xy: -0.0649
        Mean episode rew_collision: -0.0210
          Mean episode rew_dof_acc: -0.0819
   Mean episode rew_dof_pos_limits: -0.0090
        Mean episode rew_lin_vel_z: -0.0198
          Mean episode rew_torques: -0.0345
 Mean episode rew_tracking_ang_vel: 0.2584
 Mean episode rew_tracking_lin_vel: 0.4705
            Mean episode rew_total: 0.4194
        Mean episode terrain_level: 0.7575
    Mean episode min_command_x_vel: -0.9983
    Mean episode max_command_x_vel: 0.6000
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 2.46s
                        Total time: 514.10s
                               ETA: 22645.7s

################################################################################
                     [1m Learning iteration 222/10000 [0m                     

                       Computation: 40841 steps/s (collection: 2.039s, learning 0.368s)
               Value function loss: 0.0057
           Forward prediction loss: 0.0000
                          CMT loss: 0.0049
                    Surrogate loss: 0.0022
                    Policy entropy: 10.5223
             Mean action noise std: 0.58
                       Mean reward: 8.81
               Mean episode length: 853.47
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0807
       Mean episode rew_ang_vel_xy: -0.0661
        Mean episode rew_collision: -0.0276
          Mean episode rew_dof_acc: -0.0861
   Mean episode rew_dof_pos_limits: -0.0086
        Mean episode rew_lin_vel_z: -0.0206
          Mean episode rew_torques: -0.0358
 Mean episode rew_tracking_ang_vel: 0.2626
 Mean episode rew_tracking_lin_vel: 0.4915
            Mean episode rew_total: 0.4287
        Mean episode terrain_level: 0.7526
    Mean episode min_command_x_vel: -0.9983
    Mean episode max_command_x_vel: 0.6000
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 2.41s
                        Total time: 516.50s
                               ETA: 22647.4s

################################################################################
                     [1m Learning iteration 223/10000 [0m                     

                       Computation: 39619 steps/s (collection: 2.118s, learning 0.364s)
               Value function loss: 0.0065
           Forward prediction loss: 0.0000
                          CMT loss: 0.0048
                    Surrogate loss: 0.0013
                    Policy entropy: 10.5192
             Mean action noise std: 0.58
                       Mean reward: 8.85
               Mean episode length: 873.93
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0824
       Mean episode rew_ang_vel_xy: -0.0685
        Mean episode rew_collision: -0.0294
          Mean episode rew_dof_acc: -0.0884
   Mean episode rew_dof_pos_limits: -0.0091
        Mean episode rew_lin_vel_z: -0.0208
          Mean episode rew_torques: -0.0365
 Mean episode rew_tracking_ang_vel: 0.2663
 Mean episode rew_tracking_lin_vel: 0.5022
            Mean episode rew_total: 0.4334
        Mean episode terrain_level: 0.7485
    Mean episode min_command_x_vel: -0.9983
    Mean episode max_command_x_vel: 0.6000
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 2.48s
                        Total time: 518.99s
                               ETA: 22652.3s

################################################################################
                     [1m Learning iteration 224/10000 [0m                     

                       Computation: 40185 steps/s (collection: 2.083s, learning 0.363s)
               Value function loss: 0.0062
           Forward prediction loss: 0.0000
                          CMT loss: 0.0048
                    Surrogate loss: 0.0013
                    Policy entropy: 10.5145
             Mean action noise std: 0.58
                       Mean reward: 8.51
               Mean episode length: 866.50
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0832
       Mean episode rew_ang_vel_xy: -0.0687
        Mean episode rew_collision: -0.0376
          Mean episode rew_dof_acc: -0.0932
   Mean episode rew_dof_pos_limits: -0.0089
        Mean episode rew_lin_vel_z: -0.0218
          Mean episode rew_torques: -0.0369
 Mean episode rew_tracking_ang_vel: 0.2690
 Mean episode rew_tracking_lin_vel: 0.4883
            Mean episode rew_total: 0.4072
        Mean episode terrain_level: 0.7437
    Mean episode min_command_x_vel: -0.9983
    Mean episode max_command_x_vel: 0.6000
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 2.45s
                        Total time: 521.43s
                               ETA: 22655.6s

################################################################################
                     [1m Learning iteration 225/10000 [0m                     

                       Computation: 41684 steps/s (collection: 1.994s, learning 0.364s)
               Value function loss: 0.0049
           Forward prediction loss: 0.0000
                          CMT loss: 0.0049
                    Surrogate loss: 0.0034
                    Policy entropy: 10.5099
             Mean action noise std: 0.58
                       Mean reward: 8.89
               Mean episode length: 911.56
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0843
       Mean episode rew_ang_vel_xy: -0.0707
        Mean episode rew_collision: -0.0364
          Mean episode rew_dof_acc: -0.0916
   Mean episode rew_dof_pos_limits: -0.0095
        Mean episode rew_lin_vel_z: -0.0207
          Mean episode rew_torques: -0.0374
 Mean episode rew_tracking_ang_vel: 0.2799
 Mean episode rew_tracking_lin_vel: 0.5075
            Mean episode rew_total: 0.4366
        Mean episode terrain_level: 0.7381
    Mean episode min_command_x_vel: -0.9983
    Mean episode max_command_x_vel: 0.6000
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 2.36s
                        Total time: 523.79s
                               ETA: 22655.1s

################################################################################
                     [1m Learning iteration 226/10000 [0m                     

                       Computation: 41605 steps/s (collection: 1.999s, learning 0.364s)
               Value function loss: 0.0063
           Forward prediction loss: 0.0000
                          CMT loss: 0.0050
                    Surrogate loss: 0.0032
                    Policy entropy: 10.5060
             Mean action noise std: 0.58
                       Mean reward: 8.69
               Mean episode length: 904.61
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0853
       Mean episode rew_ang_vel_xy: -0.0699
        Mean episode rew_collision: -0.0366
          Mean episode rew_dof_acc: -0.0910
   Mean episode rew_dof_pos_limits: -0.0099
        Mean episode rew_lin_vel_z: -0.0216
          Mean episode rew_torques: -0.0379
 Mean episode rew_tracking_ang_vel: 0.2809
 Mean episode rew_tracking_lin_vel: 0.5167
            Mean episode rew_total: 0.4454
        Mean episode terrain_level: 0.7339
    Mean episode min_command_x_vel: -0.9983
    Mean episode max_command_x_vel: 0.6000
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 2.36s
                        Total time: 526.15s
                               ETA: 22654.7s

################################################################################
                     [1m Learning iteration 227/10000 [0m                     

                       Computation: 40835 steps/s (collection: 2.044s, learning 0.363s)
               Value function loss: 0.0062
           Forward prediction loss: 0.0000
                          CMT loss: 0.0050
                    Surrogate loss: 0.0026
                    Policy entropy: 10.5020
             Mean action noise std: 0.58
                       Mean reward: 9.21
               Mean episode length: 845.60
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0824
       Mean episode rew_ang_vel_xy: -0.0673
        Mean episode rew_collision: -0.0224
          Mean episode rew_dof_acc: -0.0847
   Mean episode rew_dof_pos_limits: -0.0098
        Mean episode rew_lin_vel_z: -0.0203
          Mean episode rew_torques: -0.0361
 Mean episode rew_tracking_ang_vel: 0.2750
 Mean episode rew_tracking_lin_vel: 0.5234
            Mean episode rew_total: 0.4754
        Mean episode terrain_level: 0.7293
    Mean episode min_command_x_vel: -0.9983
    Mean episode max_command_x_vel: 0.6000
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 2.41s
                        Total time: 528.56s
                               ETA: 22656.2s

################################################################################
                     [1m Learning iteration 228/10000 [0m                     

                       Computation: 42410 steps/s (collection: 1.953s, learning 0.365s)
               Value function loss: 0.0055
           Forward prediction loss: 0.0000
                          CMT loss: 0.0048
                    Surrogate loss: 0.0020
                    Policy entropy: 10.4986
             Mean action noise std: 0.58
                       Mean reward: 8.68
               Mean episode length: 886.32
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0850
       Mean episode rew_ang_vel_xy: -0.0718
        Mean episode rew_collision: -0.0441
          Mean episode rew_dof_acc: -0.0949
   Mean episode rew_dof_pos_limits: -0.0093
        Mean episode rew_lin_vel_z: -0.0229
          Mean episode rew_torques: -0.0373
 Mean episode rew_tracking_ang_vel: 0.2738
 Mean episode rew_tracking_lin_vel: 0.5010
            Mean episode rew_total: 0.4094
        Mean episode terrain_level: 0.7247
    Mean episode min_command_x_vel: -0.9983
    Mean episode max_command_x_vel: 0.6000
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.5000
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 2.32s
                        Total time: 530.88s
                               ETA: 22653.9s

################################################################################
                     [1m Learning iteration 229/10000 [0m                     

                       Computation: 39771 steps/s (collection: 2.110s, learning 0.361s)
               Value function loss: 0.0134
           Forward prediction loss: 0.0000
                          CMT loss: 0.0049
                    Surrogate loss: 0.0015
                    Policy entropy: 10.4945
             Mean action noise std: 0.58
                       Mean reward: 9.53
               Mean episode length: 920.70
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0869
       Mean episode rew_ang_vel_xy: -0.0710
        Mean episode rew_collision: -0.0314
          Mean episode rew_dof_acc: -0.0911
   Mean episode rew_dof_pos_limits: -0.0107
        Mean episode rew_lin_vel_z: -0.0214
          Mean episode rew_torques: -0.0376
 Mean episode rew_tracking_ang_vel: 0.2901
 Mean episode rew_tracking_lin_vel: 0.5332
            Mean episode rew_total: 0.4731
        Mean episode terrain_level: 0.7213
    Mean episode min_command_x_vel: -0.9983
    Mean episode max_command_x_vel: 0.6000
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 2.47s
                        Total time: 533.35s
                               ETA: 22658.1s

################################################################################
                     [1m Learning iteration 230/10000 [0m                     

                       Computation: 40038 steps/s (collection: 2.092s, learning 0.364s)
               Value function loss: 0.0063
           Forward prediction loss: 0.0000
                          CMT loss: 0.0049
                    Surrogate loss: 0.0021
                    Policy entropy: 10.4928
             Mean action noise std: 0.58
                       Mean reward: 9.01
               Mean episode length: 886.51
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0785
       Mean episode rew_ang_vel_xy: -0.0658
        Mean episode rew_collision: -0.0521
          Mean episode rew_dof_acc: -0.0852
   Mean episode rew_dof_pos_limits: -0.0095
        Mean episode rew_lin_vel_z: -0.0201
          Mean episode rew_torques: -0.0344
 Mean episode rew_tracking_ang_vel: 0.2548
 Mean episode rew_tracking_lin_vel: 0.4822
            Mean episode rew_total: 0.3913
        Mean episode terrain_level: 0.7192
    Mean episode min_command_x_vel: -0.9988
    Mean episode max_command_x_vel: 0.6000
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 2.46s
                        Total time: 535.80s
                               ETA: 22661.5s

################################################################################
                     [1m Learning iteration 231/10000 [0m                     

                       Computation: 41501 steps/s (collection: 2.006s, learning 0.363s)
               Value function loss: 0.0071
           Forward prediction loss: 0.0000
                          CMT loss: 0.0049
                    Surrogate loss: 0.0012
                    Policy entropy: 10.4901
             Mean action noise std: 0.58
                       Mean reward: 8.96
               Mean episode length: 905.77
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0837
       Mean episode rew_ang_vel_xy: -0.0687
        Mean episode rew_collision: -0.0466
          Mean episode rew_dof_acc: -0.0893
   Mean episode rew_dof_pos_limits: -0.0109
        Mean episode rew_lin_vel_z: -0.0207
          Mean episode rew_torques: -0.0365
 Mean episode rew_tracking_ang_vel: 0.2787
 Mean episode rew_tracking_lin_vel: 0.5182
            Mean episode rew_total: 0.4405
        Mean episode terrain_level: 0.7135
    Mean episode min_command_x_vel: -0.9999
    Mean episode max_command_x_vel: 0.6000
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 2.37s
                        Total time: 538.17s
                               ETA: 22661.3s

################################################################################
                     [1m Learning iteration 232/10000 [0m                     

                       Computation: 40614 steps/s (collection: 2.055s, learning 0.365s)
               Value function loss: 0.0078
           Forward prediction loss: 0.0000
                          CMT loss: 0.0049
                    Surrogate loss: 0.0016
                    Policy entropy: 10.4870
             Mean action noise std: 0.58
                       Mean reward: 9.33
               Mean episode length: 893.05
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0831
       Mean episode rew_ang_vel_xy: -0.0671
        Mean episode rew_collision: -0.0235
          Mean episode rew_dof_acc: -0.0847
   Mean episode rew_dof_pos_limits: -0.0112
        Mean episode rew_lin_vel_z: -0.0206
          Mean episode rew_torques: -0.0354
 Mean episode rew_tracking_ang_vel: 0.2828
 Mean episode rew_tracking_lin_vel: 0.5197
            Mean episode rew_total: 0.4766
        Mean episode terrain_level: 0.7058
    Mean episode min_command_x_vel: -0.9999
    Mean episode max_command_x_vel: 0.6000
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 2.42s
                        Total time: 540.59s
                               ETA: 22663.2s

################################################################################
                     [1m Learning iteration 233/10000 [0m                     

                       Computation: 41215 steps/s (collection: 2.021s, learning 0.364s)
               Value function loss: 0.0069
           Forward prediction loss: 0.0000
                          CMT loss: 0.0049
                    Surrogate loss: 0.0010
                    Policy entropy: 10.4831
             Mean action noise std: 0.58
                       Mean reward: 8.86
               Mean episode length: 859.97
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0800
       Mean episode rew_ang_vel_xy: -0.0660
        Mean episode rew_collision: -0.0370
          Mean episode rew_dof_acc: -0.0842
   Mean episode rew_dof_pos_limits: -0.0109
        Mean episode rew_lin_vel_z: -0.0202
          Mean episode rew_torques: -0.0346
 Mean episode rew_tracking_ang_vel: 0.2753
 Mean episode rew_tracking_lin_vel: 0.5103
            Mean episode rew_total: 0.4526
        Mean episode terrain_level: 0.7009
    Mean episode min_command_x_vel: -0.9999
    Mean episode max_command_x_vel: 0.5996
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 2.39s
                        Total time: 542.98s
                               ETA: 22663.6s

################################################################################
                     [1m Learning iteration 234/10000 [0m                     

                       Computation: 40722 steps/s (collection: 2.050s, learning 0.364s)
               Value function loss: 0.0063
           Forward prediction loss: 0.0000
                          CMT loss: 0.0048
                    Surrogate loss: 0.0004
                    Policy entropy: 10.4784
             Mean action noise std: 0.58
                       Mean reward: 8.61
               Mean episode length: 905.35
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0859
       Mean episode rew_ang_vel_xy: -0.0695
        Mean episode rew_collision: -0.0422
          Mean episode rew_dof_acc: -0.0908
   Mean episode rew_dof_pos_limits: -0.0104
        Mean episode rew_lin_vel_z: -0.0214
          Mean episode rew_torques: -0.0369
 Mean episode rew_tracking_ang_vel: 0.2830
 Mean episode rew_tracking_lin_vel: 0.5083
            Mean episode rew_total: 0.4342
        Mean episode terrain_level: 0.6981
    Mean episode min_command_x_vel: -0.9999
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4995
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 2.41s
                        Total time: 545.39s
                               ETA: 22665.1s

################################################################################
                     [1m Learning iteration 235/10000 [0m                     

                       Computation: 40724 steps/s (collection: 2.050s, learning 0.364s)
               Value function loss: 0.0066
           Forward prediction loss: 0.0000
                          CMT loss: 0.0050
                    Surrogate loss: 0.0027
                    Policy entropy: 10.4736
             Mean action noise std: 0.58
                       Mean reward: 9.24
               Mean episode length: 915.89
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0838
       Mean episode rew_ang_vel_xy: -0.0682
        Mean episode rew_collision: -0.0359
          Mean episode rew_dof_acc: -0.0846
   Mean episode rew_dof_pos_limits: -0.0113
        Mean episode rew_lin_vel_z: -0.0199
          Mean episode rew_torques: -0.0354
 Mean episode rew_tracking_ang_vel: 0.2756
 Mean episode rew_tracking_lin_vel: 0.5253
            Mean episode rew_total: 0.4618
        Mean episode terrain_level: 0.6941
    Mean episode min_command_x_vel: -0.9999
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4994
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 2.41s
                        Total time: 547.81s
                               ETA: 22666.7s

################################################################################
                     [1m Learning iteration 236/10000 [0m                     

                       Computation: 41440 steps/s (collection: 2.008s, learning 0.364s)
               Value function loss: 0.0064
           Forward prediction loss: 0.0000
                          CMT loss: 0.0049
                    Surrogate loss: 0.0016
                    Policy entropy: 10.4658
             Mean action noise std: 0.58
                       Mean reward: 9.23
               Mean episode length: 927.70
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0862
       Mean episode rew_ang_vel_xy: -0.0703
        Mean episode rew_collision: -0.0239
          Mean episode rew_dof_acc: -0.0917
   Mean episode rew_dof_pos_limits: -0.0117
        Mean episode rew_lin_vel_z: -0.0211
          Mean episode rew_torques: -0.0361
 Mean episode rew_tracking_ang_vel: 0.2877
 Mean episode rew_tracking_lin_vel: 0.5190
            Mean episode rew_total: 0.4658
        Mean episode terrain_level: 0.6900
    Mean episode min_command_x_vel: -0.9999
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4991
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 2.37s
                        Total time: 550.18s
                               ETA: 22666.4s

################################################################################
                     [1m Learning iteration 237/10000 [0m                     

                       Computation: 40670 steps/s (collection: 2.049s, learning 0.368s)
               Value function loss: 0.0076
           Forward prediction loss: 0.0000
                          CMT loss: 0.0050
                    Surrogate loss: 0.0030
                    Policy entropy: 10.4616
             Mean action noise std: 0.58
                       Mean reward: 9.68
               Mean episode length: 890.08
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0817
       Mean episode rew_ang_vel_xy: -0.0674
        Mean episode rew_collision: -0.0343
          Mean episode rew_dof_acc: -0.0850
   Mean episode rew_dof_pos_limits: -0.0111
        Mean episode rew_lin_vel_z: -0.0210
          Mean episode rew_torques: -0.0344
 Mean episode rew_tracking_ang_vel: 0.2743
 Mean episode rew_tracking_lin_vel: 0.5041
            Mean episode rew_total: 0.4435
        Mean episode terrain_level: 0.6867
    Mean episode min_command_x_vel: -0.9999
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4991
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 2.42s
                        Total time: 552.60s
                               ETA: 22668.0s

################################################################################
                     [1m Learning iteration 238/10000 [0m                     

                       Computation: 40680 steps/s (collection: 2.053s, learning 0.364s)
               Value function loss: 0.0064
           Forward prediction loss: 0.0000
                          CMT loss: 0.0050
                    Surrogate loss: 0.0016
                    Policy entropy: 10.4581
             Mean action noise std: 0.58
                       Mean reward: 9.25
               Mean episode length: 888.79
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0849
       Mean episode rew_ang_vel_xy: -0.0690
        Mean episode rew_collision: -0.0253
          Mean episode rew_dof_acc: -0.0917
   Mean episode rew_dof_pos_limits: -0.0114
        Mean episode rew_lin_vel_z: -0.0212
          Mean episode rew_torques: -0.0361
 Mean episode rew_tracking_ang_vel: 0.2758
 Mean episode rew_tracking_lin_vel: 0.5399
            Mean episode rew_total: 0.4761
        Mean episode terrain_level: 0.6810
    Mean episode min_command_x_vel: -0.9999
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4998
  Mean episode min_command_yaw_vel: -0.4993
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 2.42s
                        Total time: 555.01s
                               ETA: 22669.6s

################################################################################
                     [1m Learning iteration 239/10000 [0m                     

                       Computation: 40819 steps/s (collection: 2.045s, learning 0.363s)
               Value function loss: 0.0063
           Forward prediction loss: 0.0000
                          CMT loss: 0.0049
                    Surrogate loss: 0.0011
                    Policy entropy: 10.4541
             Mean action noise std: 0.58
                       Mean reward: 9.07
               Mean episode length: 905.24
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0840
       Mean episode rew_ang_vel_xy: -0.0670
        Mean episode rew_collision: -0.0478
          Mean episode rew_dof_acc: -0.0878
   Mean episode rew_dof_pos_limits: -0.0126
        Mean episode rew_lin_vel_z: -0.0211
          Mean episode rew_torques: -0.0358
 Mean episode rew_tracking_ang_vel: 0.2851
 Mean episode rew_tracking_lin_vel: 0.5268
            Mean episode rew_total: 0.4559
        Mean episode terrain_level: 0.6763
    Mean episode min_command_x_vel: -0.9999
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4993
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 2.41s
                        Total time: 557.42s
                               ETA: 22670.8s

################################################################################
                     [1m Learning iteration 240/10000 [0m                     

                       Computation: 40638 steps/s (collection: 2.052s, learning 0.367s)
               Value function loss: 0.0054
           Forward prediction loss: 0.0000
                          CMT loss: 0.0050
                    Surrogate loss: 0.0016
                    Policy entropy: 10.4480
             Mean action noise std: 0.58
                       Mean reward: 9.18
               Mean episode length: 920.02
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0864
       Mean episode rew_ang_vel_xy: -0.0712
        Mean episode rew_collision: -0.0581
          Mean episode rew_dof_acc: -0.0928
   Mean episode rew_dof_pos_limits: -0.0117
        Mean episode rew_lin_vel_z: -0.0215
          Mean episode rew_torques: -0.0365
 Mean episode rew_tracking_ang_vel: 0.2889
 Mean episode rew_tracking_lin_vel: 0.5499
            Mean episode rew_total: 0.4608
        Mean episode terrain_level: 0.6730
    Mean episode min_command_x_vel: -0.9999
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4997
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 2.42s
                        Total time: 559.84s
                               ETA: 22672.3s

################################################################################
                     [1m Learning iteration 241/10000 [0m                     

                       Computation: 40265 steps/s (collection: 2.077s, learning 0.365s)
               Value function loss: 0.0064
           Forward prediction loss: 0.0000
                          CMT loss: 0.0052
                    Surrogate loss: 0.0019
                    Policy entropy: 10.4392
             Mean action noise std: 0.58
                       Mean reward: 9.39
               Mean episode length: 883.18
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0783
       Mean episode rew_ang_vel_xy: -0.0633
        Mean episode rew_collision: -0.0202
          Mean episode rew_dof_acc: -0.0869
   Mean episode rew_dof_pos_limits: -0.0113
        Mean episode rew_lin_vel_z: -0.0213
          Mean episode rew_torques: -0.0331
 Mean episode rew_tracking_ang_vel: 0.2593
 Mean episode rew_tracking_lin_vel: 0.4816
            Mean episode rew_total: 0.4267
        Mean episode terrain_level: 0.6676
    Mean episode min_command_x_vel: -0.9999
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4997
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 2.44s
                        Total time: 562.28s
                               ETA: 22674.8s

################################################################################
                     [1m Learning iteration 242/10000 [0m                     

                       Computation: 40715 steps/s (collection: 2.050s, learning 0.364s)
               Value function loss: 0.0056
           Forward prediction loss: 0.0000
                          CMT loss: 0.0051
                    Surrogate loss: 0.0014
                    Policy entropy: 10.4333
             Mean action noise std: 0.58
                       Mean reward: 8.58
               Mean episode length: 878.49
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0847
       Mean episode rew_ang_vel_xy: -0.0690
        Mean episode rew_collision: -0.0196
          Mean episode rew_dof_acc: -0.0892
   Mean episode rew_dof_pos_limits: -0.0137
        Mean episode rew_lin_vel_z: -0.0209
          Mean episode rew_torques: -0.0352
 Mean episode rew_tracking_ang_vel: 0.2808
 Mean episode rew_tracking_lin_vel: 0.4913
            Mean episode rew_total: 0.4398
        Mean episode terrain_level: 0.6628
    Mean episode min_command_x_vel: -0.9999
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4997
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 2.41s
                        Total time: 564.70s
                               ETA: 22676.1s

################################################################################
                     [1m Learning iteration 243/10000 [0m                     

                       Computation: 40799 steps/s (collection: 2.046s, learning 0.364s)
               Value function loss: 0.0067
           Forward prediction loss: 0.0000
                          CMT loss: 0.0050
                    Surrogate loss: 0.0010
                    Policy entropy: 10.4292
             Mean action noise std: 0.58
                       Mean reward: 9.26
               Mean episode length: 881.50
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0829
       Mean episode rew_ang_vel_xy: -0.0653
        Mean episode rew_collision: -0.0149
          Mean episode rew_dof_acc: -0.0859
   Mean episode rew_dof_pos_limits: -0.0130
        Mean episode rew_lin_vel_z: -0.0208
          Mean episode rew_torques: -0.0351
 Mean episode rew_tracking_ang_vel: 0.2741
 Mean episode rew_tracking_lin_vel: 0.5027
            Mean episode rew_total: 0.4590
        Mean episode terrain_level: 0.6595
    Mean episode min_command_x_vel: -0.9999
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4997
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 2.41s
                        Total time: 567.10s
                               ETA: 22677.2s

################################################################################
                     [1m Learning iteration 244/10000 [0m                     

                       Computation: 40513 steps/s (collection: 2.060s, learning 0.366s)
               Value function loss: 0.0057
           Forward prediction loss: 0.0000
                          CMT loss: 0.0051
                    Surrogate loss: 0.0016
                    Policy entropy: 10.4257
             Mean action noise std: 0.58
                       Mean reward: 8.92
               Mean episode length: 923.26
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0852
       Mean episode rew_ang_vel_xy: -0.0703
        Mean episode rew_collision: -0.0220
          Mean episode rew_dof_acc: -0.1011
   Mean episode rew_dof_pos_limits: -0.0108
        Mean episode rew_lin_vel_z: -0.0226
          Mean episode rew_torques: -0.0364
 Mean episode rew_tracking_ang_vel: 0.2706
 Mean episode rew_tracking_lin_vel: 0.5048
            Mean episode rew_total: 0.4270
        Mean episode terrain_level: 0.6572
    Mean episode min_command_x_vel: -0.9999
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4997
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 2.43s
                        Total time: 569.53s
                               ETA: 22679.0s

################################################################################
                     [1m Learning iteration 245/10000 [0m                     

                       Computation: 40692 steps/s (collection: 2.060s, learning 0.356s)
               Value function loss: 0.0069
           Forward prediction loss: 0.0000
                          CMT loss: 0.0050
                    Surrogate loss: 0.0013
                    Policy entropy: 10.4208
             Mean action noise std: 0.58
                       Mean reward: 9.16
               Mean episode length: 867.82
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0791
       Mean episode rew_ang_vel_xy: -0.0639
        Mean episode rew_collision: -0.0341
          Mean episode rew_dof_acc: -0.0841
   Mean episode rew_dof_pos_limits: -0.0128
        Mean episode rew_lin_vel_z: -0.0200
          Mean episode rew_torques: -0.0331
 Mean episode rew_tracking_ang_vel: 0.2709
 Mean episode rew_tracking_lin_vel: 0.5068
            Mean episode rew_total: 0.4506
        Mean episode terrain_level: 0.6556
    Mean episode min_command_x_vel: -0.9999
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4997
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 2.42s
                        Total time: 571.95s
                               ETA: 22680.3s

################################################################################
                     [1m Learning iteration 246/10000 [0m                     

                       Computation: 41227 steps/s (collection: 2.027s, learning 0.357s)
               Value function loss: 0.0062
           Forward prediction loss: 0.0000
                          CMT loss: 0.0050
                    Surrogate loss: 0.0015
                    Policy entropy: 10.4141
             Mean action noise std: 0.58
                       Mean reward: 9.10
               Mean episode length: 889.45
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0843
       Mean episode rew_ang_vel_xy: -0.0675
        Mean episode rew_collision: -0.0265
          Mean episode rew_dof_acc: -0.0893
   Mean episode rew_dof_pos_limits: -0.0136
        Mean episode rew_lin_vel_z: -0.0205
          Mean episode rew_torques: -0.0356
 Mean episode rew_tracking_ang_vel: 0.2892
 Mean episode rew_tracking_lin_vel: 0.5382
            Mean episode rew_total: 0.4901
        Mean episode terrain_level: 0.6533
    Mean episode min_command_x_vel: -0.9999
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4997
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 2.38s
                        Total time: 574.33s
                               ETA: 22680.3s

################################################################################
                     [1m Learning iteration 247/10000 [0m                     

                       Computation: 40721 steps/s (collection: 2.050s, learning 0.364s)
               Value function loss: 0.0065
           Forward prediction loss: 0.0000
                          CMT loss: 0.0051
                    Surrogate loss: 0.0025
                    Policy entropy: 10.4102
             Mean action noise std: 0.58
                       Mean reward: 9.88
               Mean episode length: 915.49
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0860
       Mean episode rew_ang_vel_xy: -0.0697
        Mean episode rew_collision: -0.0226
          Mean episode rew_dof_acc: -0.0920
   Mean episode rew_dof_pos_limits: -0.0133
        Mean episode rew_lin_vel_z: -0.0209
          Mean episode rew_torques: -0.0363
 Mean episode rew_tracking_ang_vel: 0.2961
 Mean episode rew_tracking_lin_vel: 0.5489
            Mean episode rew_total: 0.5043
        Mean episode terrain_level: 0.6508
    Mean episode min_command_x_vel: -0.9999
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4997
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 2.41s
                        Total time: 576.75s
                               ETA: 22681.5s

################################################################################
                     [1m Learning iteration 248/10000 [0m                     

                       Computation: 40353 steps/s (collection: 2.072s, learning 0.364s)
               Value function loss: 0.0056
           Forward prediction loss: 0.0000
                          CMT loss: 0.0052
                    Surrogate loss: 0.0021
                    Policy entropy: 10.4078
             Mean action noise std: 0.58
                       Mean reward: 9.56
               Mean episode length: 905.05
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0831
       Mean episode rew_ang_vel_xy: -0.0662
        Mean episode rew_collision: -0.0236
          Mean episode rew_dof_acc: -0.0875
   Mean episode rew_dof_pos_limits: -0.0137
        Mean episode rew_lin_vel_z: -0.0206
          Mean episode rew_torques: -0.0344
 Mean episode rew_tracking_ang_vel: 0.2822
 Mean episode rew_tracking_lin_vel: 0.5242
            Mean episode rew_total: 0.4773
        Mean episode terrain_level: 0.6496
    Mean episode min_command_x_vel: -0.9999
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4997
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 2.44s
                        Total time: 579.18s
                               ETA: 22683.5s

################################################################################
                     [1m Learning iteration 249/10000 [0m                     

                       Computation: 41283 steps/s (collection: 2.025s, learning 0.356s)
               Value function loss: 0.0066
           Forward prediction loss: 0.0000
                          CMT loss: 0.0052
                    Surrogate loss: 0.0021
                    Policy entropy: 10.4035
             Mean action noise std: 0.58
                       Mean reward: 9.50
               Mean episode length: 899.09
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0846
       Mean episode rew_ang_vel_xy: -0.0685
        Mean episode rew_collision: -0.0288
          Mean episode rew_dof_acc: -0.0975
   Mean episode rew_dof_pos_limits: -0.0120
        Mean episode rew_lin_vel_z: -0.0216
          Mean episode rew_torques: -0.0355
 Mean episode rew_tracking_ang_vel: 0.2779
 Mean episode rew_tracking_lin_vel: 0.5159
            Mean episode rew_total: 0.4453
        Mean episode terrain_level: 0.6475
    Mean episode min_command_x_vel: -0.9999
    Mean episode max_command_x_vel: 0.5995
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4997
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.6267
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 2.38s
                        Total time: 581.56s
                               ETA: 22683.3s

################################################################################
                     [1m Learning iteration 250/10000 [0m                     

                       Computation: 37383 steps/s (collection: 2.269s, learning 0.360s)
               Value function loss: 0.0147
           Forward prediction loss: 0.0000
                          CMT loss: 0.0062
                    Surrogate loss: 0.0026
                    Policy entropy: 10.4014
             Mean action noise std: 0.58
                       Mean reward: 10.40
               Mean episode length: 985.46
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0870
       Mean episode rew_ang_vel_xy: -0.0686
        Mean episode rew_collision: -0.0187
          Mean episode rew_dof_acc: -0.0958
   Mean episode rew_dof_pos_limits: -0.0132
        Mean episode rew_lin_vel_z: -0.0213
          Mean episode rew_torques: -0.0365
 Mean episode rew_tracking_ang_vel: 0.2909
 Mean episode rew_tracking_lin_vel: 0.5375
            Mean episode rew_total: 0.4873
        Mean episode terrain_level: 0.6352
    Mean episode min_command_x_vel: -0.9999
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.4993
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4997
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.6933
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 2.63s
                        Total time: 584.19s
                               ETA: 22692.7s

################################################################################
                     [1m Learning iteration 251/10000 [0m                     

                       Computation: 40792 steps/s (collection: 2.043s, learning 0.367s)
               Value function loss: 0.0063
           Forward prediction loss: 0.0000
                          CMT loss: 0.0053
                    Surrogate loss: 0.0135
                    Policy entropy: 10.4003
             Mean action noise std: 0.58
                       Mean reward: 9.72
               Mean episode length: 920.25
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0890
       Mean episode rew_ang_vel_xy: -0.0715
        Mean episode rew_collision: -0.0284
          Mean episode rew_dof_acc: -0.1042
   Mean episode rew_dof_pos_limits: -0.0125
        Mean episode rew_lin_vel_z: -0.0228
          Mean episode rew_torques: -0.0377
 Mean episode rew_tracking_ang_vel: 0.2851
 Mean episode rew_tracking_lin_vel: 0.5749
            Mean episode rew_total: 0.4939
        Mean episode terrain_level: 0.6259
    Mean episode min_command_x_vel: -0.9963
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.4991
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4997
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 2.41s
                        Total time: 586.60s
                               ETA: 22693.6s

################################################################################
                     [1m Learning iteration 252/10000 [0m                     

                       Computation: 40217 steps/s (collection: 2.085s, learning 0.359s)
               Value function loss: 0.0079
           Forward prediction loss: 0.0000
                          CMT loss: 0.0055
                    Surrogate loss: 0.0015
                    Policy entropy: 10.3986
             Mean action noise std: 0.58
                       Mean reward: 9.60
               Mean episode length: 908.96
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0815
       Mean episode rew_ang_vel_xy: -0.0660
        Mean episode rew_collision: -0.0154
          Mean episode rew_dof_acc: -0.0876
   Mean episode rew_dof_pos_limits: -0.0137
        Mean episode rew_lin_vel_z: -0.0211
          Mean episode rew_torques: -0.0332
 Mean episode rew_tracking_ang_vel: 0.2759
 Mean episode rew_tracking_lin_vel: 0.5068
            Mean episode rew_total: 0.4641
        Mean episode terrain_level: 0.6250
    Mean episode min_command_x_vel: -0.9951
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.4992
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4998
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 2.44s
                        Total time: 589.05s
                               ETA: 22695.8s

################################################################################
                     [1m Learning iteration 253/10000 [0m                     

                       Computation: 41334 steps/s (collection: 2.022s, learning 0.356s)
               Value function loss: 0.0076
           Forward prediction loss: 0.0000
                          CMT loss: 0.0056
                    Surrogate loss: 0.0020
                    Policy entropy: 10.3974
             Mean action noise std: 0.58
                       Mean reward: 9.47
               Mean episode length: 906.05
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0827
       Mean episode rew_ang_vel_xy: -0.0658
        Mean episode rew_collision: -0.0179
          Mean episode rew_dof_acc: -0.0920
   Mean episode rew_dof_pos_limits: -0.0135
        Mean episode rew_lin_vel_z: -0.0211
          Mean episode rew_torques: -0.0338
 Mean episode rew_tracking_ang_vel: 0.2806
 Mean episode rew_tracking_lin_vel: 0.5164
            Mean episode rew_total: 0.4700
        Mean episode terrain_level: 0.6211
    Mean episode min_command_x_vel: -0.9993
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.4991
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.4997
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 2.38s
                        Total time: 591.43s
                               ETA: 22695.4s

################################################################################
                     [1m Learning iteration 254/10000 [0m                     

                       Computation: 41069 steps/s (collection: 2.027s, learning 0.367s)
               Value function loss: 0.0069
           Forward prediction loss: 0.0000
                          CMT loss: 0.0056
                    Surrogate loss: 0.0021
                    Policy entropy: 10.3963
             Mean action noise std: 0.58
                       Mean reward: 9.60
               Mean episode length: 931.56
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0855
       Mean episode rew_ang_vel_xy: -0.0679
        Mean episode rew_collision: -0.0269
          Mean episode rew_dof_acc: -0.0930
   Mean episode rew_dof_pos_limits: -0.0147
        Mean episode rew_lin_vel_z: -0.0212
          Mean episode rew_torques: -0.0352
 Mean episode rew_tracking_ang_vel: 0.2824
 Mean episode rew_tracking_lin_vel: 0.5325
            Mean episode rew_total: 0.4705
        Mean episode terrain_level: 0.6176
    Mean episode min_command_x_vel: -0.9996
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.4991
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4997
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 2.39s
                        Total time: 593.82s
                               ETA: 22695.5s

################################################################################
                     [1m Learning iteration 255/10000 [0m                     

                       Computation: 39108 steps/s (collection: 2.152s, learning 0.362s)
               Value function loss: 0.0077
           Forward prediction loss: 0.0000
                          CMT loss: 0.0056
                    Surrogate loss: 0.0018
                    Policy entropy: 10.3947
             Mean action noise std: 0.58
                       Mean reward: 9.50
               Mean episode length: 915.39
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0842
       Mean episode rew_ang_vel_xy: -0.0665
        Mean episode rew_collision: -0.0251
          Mean episode rew_dof_acc: -0.0896
   Mean episode rew_dof_pos_limits: -0.0150
        Mean episode rew_lin_vel_z: -0.0206
          Mean episode rew_torques: -0.0343
 Mean episode rew_tracking_ang_vel: 0.2900
 Mean episode rew_tracking_lin_vel: 0.5141
            Mean episode rew_total: 0.4687
        Mean episode terrain_level: 0.6142
    Mean episode min_command_x_vel: -0.9996
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.4991
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4997
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 2.51s
                        Total time: 596.33s
                               ETA: 22700.2s

################################################################################
                     [1m Learning iteration 256/10000 [0m                     

                       Computation: 39952 steps/s (collection: 2.098s, learning 0.363s)
               Value function loss: 0.0069
           Forward prediction loss: 0.0000
                          CMT loss: 0.0058
                    Surrogate loss: 0.0019
                    Policy entropy: 10.3930
             Mean action noise std: 0.58
                       Mean reward: 9.96
               Mean episode length: 903.62
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0821
       Mean episode rew_ang_vel_xy: -0.0651
        Mean episode rew_collision: -0.0209
          Mean episode rew_dof_acc: -0.0855
   Mean episode rew_dof_pos_limits: -0.0144
        Mean episode rew_lin_vel_z: -0.0198
          Mean episode rew_torques: -0.0332
 Mean episode rew_tracking_ang_vel: 0.2883
 Mean episode rew_tracking_lin_vel: 0.5376
            Mean episode rew_total: 0.5050
        Mean episode terrain_level: 0.6086
    Mean episode min_command_x_vel: -0.9996
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.4991
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4997
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 2.46s
                        Total time: 598.79s
                               ETA: 22702.9s

################################################################################
                     [1m Learning iteration 257/10000 [0m                     

                       Computation: 39728 steps/s (collection: 2.109s, learning 0.365s)
               Value function loss: 0.0066
           Forward prediction loss: 0.0000
                          CMT loss: 0.0059
                    Surrogate loss: 0.0083
                    Policy entropy: 10.3907
             Mean action noise std: 0.58
                       Mean reward: 9.23
               Mean episode length: 874.52
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0789
       Mean episode rew_ang_vel_xy: -0.0639
        Mean episode rew_collision: -0.0281
          Mean episode rew_dof_acc: -0.0856
   Mean episode rew_dof_pos_limits: -0.0134
        Mean episode rew_lin_vel_z: -0.0201
          Mean episode rew_torques: -0.0326
 Mean episode rew_tracking_ang_vel: 0.2678
 Mean episode rew_tracking_lin_vel: 0.5210
            Mean episode rew_total: 0.4662
        Mean episode terrain_level: 0.6048
    Mean episode min_command_x_vel: -0.9990
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.4991
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4997
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 2.47s
                        Total time: 601.27s
                               ETA: 22706.0s

################################################################################
                     [1m Learning iteration 258/10000 [0m                     

                       Computation: 38742 steps/s (collection: 2.172s, learning 0.365s)
               Value function loss: 0.0071
           Forward prediction loss: 0.0000
                          CMT loss: 0.0063
                    Surrogate loss: 0.0070
                    Policy entropy: 10.3887
             Mean action noise std: 0.58
                       Mean reward: 9.46
               Mean episode length: 888.20
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0822
       Mean episode rew_ang_vel_xy: -0.0650
        Mean episode rew_collision: -0.0144
          Mean episode rew_dof_acc: -0.0917
   Mean episode rew_dof_pos_limits: -0.0139
        Mean episode rew_lin_vel_z: -0.0210
          Mean episode rew_torques: -0.0333
 Mean episode rew_tracking_ang_vel: 0.2794
 Mean episode rew_tracking_lin_vel: 0.4938
            Mean episode rew_total: 0.4516
        Mean episode terrain_level: 0.6018
    Mean episode min_command_x_vel: -0.9987
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.4992
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4997
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 2.54s
                        Total time: 603.80s
                               ETA: 22711.4s

################################################################################
                     [1m Learning iteration 259/10000 [0m                     

                       Computation: 39737 steps/s (collection: 2.108s, learning 0.366s)
               Value function loss: 0.0066
           Forward prediction loss: 0.0000
                          CMT loss: 0.0063
                    Surrogate loss: 0.0025
                    Policy entropy: 10.3866
             Mean action noise std: 0.58
                       Mean reward: 9.76
               Mean episode length: 886.37
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0813
       Mean episode rew_ang_vel_xy: -0.0659
        Mean episode rew_collision: -0.0247
          Mean episode rew_dof_acc: -0.0861
   Mean episode rew_dof_pos_limits: -0.0143
        Mean episode rew_lin_vel_z: -0.0199
          Mean episode rew_torques: -0.0322
 Mean episode rew_tracking_ang_vel: 0.2808
 Mean episode rew_tracking_lin_vel: 0.5510
            Mean episode rew_total: 0.5074
        Mean episode terrain_level: 0.5972
    Mean episode min_command_x_vel: -0.9987
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.4994
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4997
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 2.47s
                        Total time: 606.28s
                               ETA: 22714.5s

################################################################################
                     [1m Learning iteration 260/10000 [0m                     

                       Computation: 40039 steps/s (collection: 2.086s, learning 0.369s)
               Value function loss: 0.0062
           Forward prediction loss: 0.0000
                          CMT loss: 0.0064
                    Surrogate loss: 0.0028
                    Policy entropy: 10.3842
             Mean action noise std: 0.58
                       Mean reward: 8.72
               Mean episode length: 871.79
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0796
       Mean episode rew_ang_vel_xy: -0.0637
        Mean episode rew_collision: -0.0219
          Mean episode rew_dof_acc: -0.0904
   Mean episode rew_dof_pos_limits: -0.0133
        Mean episode rew_lin_vel_z: -0.0208
          Mean episode rew_torques: -0.0329
 Mean episode rew_tracking_ang_vel: 0.2688
 Mean episode rew_tracking_lin_vel: 0.4834
            Mean episode rew_total: 0.4297
        Mean episode terrain_level: 0.5944
    Mean episode min_command_x_vel: -0.9987
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.4996
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4997
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 2.46s
                        Total time: 608.73s
                               ETA: 22716.7s

################################################################################
                     [1m Learning iteration 261/10000 [0m                     

                       Computation: 38722 steps/s (collection: 2.173s, learning 0.365s)
               Value function loss: 0.0067
           Forward prediction loss: 0.0000
                          CMT loss: 0.0064
                    Surrogate loss: 0.0028
                    Policy entropy: 10.3824
             Mean action noise std: 0.58
                       Mean reward: 10.24
               Mean episode length: 940.62
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0856
       Mean episode rew_ang_vel_xy: -0.0672
        Mean episode rew_collision: -0.0129
          Mean episode rew_dof_acc: -0.0935
   Mean episode rew_dof_pos_limits: -0.0149
        Mean episode rew_lin_vel_z: -0.0211
          Mean episode rew_torques: -0.0344
 Mean episode rew_tracking_ang_vel: 0.2928
 Mean episode rew_tracking_lin_vel: 0.5512
            Mean episode rew_total: 0.5145
        Mean episode terrain_level: 0.5918
    Mean episode min_command_x_vel: -0.9987
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.4996
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4997
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 2.54s
                        Total time: 611.27s
                               ETA: 22722.1s

################################################################################
                     [1m Learning iteration 262/10000 [0m                     

                       Computation: 39511 steps/s (collection: 2.125s, learning 0.363s)
               Value function loss: 0.0067
           Forward prediction loss: 0.0000
                          CMT loss: 0.0064
                    Surrogate loss: 0.0025
                    Policy entropy: 10.3805
             Mean action noise std: 0.58
                       Mean reward: 8.95
               Mean episode length: 877.25
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0783
       Mean episode rew_ang_vel_xy: -0.0612
        Mean episode rew_collision: -0.0300
          Mean episode rew_dof_acc: -0.0864
   Mean episode rew_dof_pos_limits: -0.0142
        Mean episode rew_lin_vel_z: -0.0197
          Mean episode rew_torques: -0.0308
 Mean episode rew_tracking_ang_vel: 0.2684
 Mean episode rew_tracking_lin_vel: 0.4880
            Mean episode rew_total: 0.4358
        Mean episode terrain_level: 0.5899
    Mean episode min_command_x_vel: -0.9987
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 2.49s
                        Total time: 613.76s
                               ETA: 22725.5s

################################################################################
                     [1m Learning iteration 263/10000 [0m                     

                       Computation: 39876 steps/s (collection: 2.102s, learning 0.363s)
               Value function loss: 0.0064
           Forward prediction loss: 0.0000
                          CMT loss: 0.0066
                    Surrogate loss: 0.0026
                    Policy entropy: 10.3792
             Mean action noise std: 0.58
                       Mean reward: 10.23
               Mean episode length: 886.16
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0825
       Mean episode rew_ang_vel_xy: -0.0649
        Mean episode rew_collision: -0.0184
          Mean episode rew_dof_acc: -0.0927
   Mean episode rew_dof_pos_limits: -0.0150
        Mean episode rew_lin_vel_z: -0.0204
          Mean episode rew_torques: -0.0333
 Mean episode rew_tracking_ang_vel: 0.2816
 Mean episode rew_tracking_lin_vel: 0.5424
            Mean episode rew_total: 0.4969
        Mean episode terrain_level: 0.5873
    Mean episode min_command_x_vel: -0.9987
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4998
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 2.47s
                        Total time: 616.23s
                               ETA: 22728.0s

################################################################################
                     [1m Learning iteration 264/10000 [0m                     

                       Computation: 40150 steps/s (collection: 2.083s, learning 0.365s)
               Value function loss: 0.0066
           Forward prediction loss: 0.0000
                          CMT loss: 0.0067
                    Surrogate loss: 0.0031
                    Policy entropy: 10.3774
             Mean action noise std: 0.58
                       Mean reward: 9.55
               Mean episode length: 887.81
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0798
       Mean episode rew_ang_vel_xy: -0.0625
        Mean episode rew_collision: -0.0194
          Mean episode rew_dof_acc: -0.0903
   Mean episode rew_dof_pos_limits: -0.0140
        Mean episode rew_lin_vel_z: -0.0202
          Mean episode rew_torques: -0.0322
 Mean episode rew_tracking_ang_vel: 0.2686
 Mean episode rew_tracking_lin_vel: 0.5248
            Mean episode rew_total: 0.4752
        Mean episode terrain_level: 0.5853
    Mean episode min_command_x_vel: -0.9987
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 2.45s
                        Total time: 618.67s
                               ETA: 22729.8s

################################################################################
                     [1m Learning iteration 265/10000 [0m                     

                       Computation: 40664 steps/s (collection: 2.050s, learning 0.368s)
               Value function loss: 0.0057
           Forward prediction loss: 0.0000
                          CMT loss: 0.0068
                    Surrogate loss: 0.0026
                    Policy entropy: 10.3756
             Mean action noise std: 0.58
                       Mean reward: 9.26
               Mean episode length: 885.51
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0803
       Mean episode rew_ang_vel_xy: -0.0624
        Mean episode rew_collision: -0.0224
          Mean episode rew_dof_acc: -0.0835
   Mean episode rew_dof_pos_limits: -0.0161
        Mean episode rew_lin_vel_z: -0.0201
          Mean episode rew_torques: -0.0315
 Mean episode rew_tracking_ang_vel: 0.2857
 Mean episode rew_tracking_lin_vel: 0.5160
            Mean episode rew_total: 0.4853
        Mean episode terrain_level: 0.5825
    Mean episode min_command_x_vel: -0.9987
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 2.42s
                        Total time: 621.09s
                               ETA: 22730.5s

################################################################################
                     [1m Learning iteration 266/10000 [0m                     

                       Computation: 39847 steps/s (collection: 2.102s, learning 0.365s)
               Value function loss: 0.0072
           Forward prediction loss: 0.0000
                          CMT loss: 0.0070
                    Surrogate loss: 0.0026
                    Policy entropy: 10.3733
             Mean action noise std: 0.58
                       Mean reward: 8.98
               Mean episode length: 862.93
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0780
       Mean episode rew_ang_vel_xy: -0.0607
        Mean episode rew_collision: -0.0204
          Mean episode rew_dof_acc: -0.0898
   Mean episode rew_dof_pos_limits: -0.0141
        Mean episode rew_lin_vel_z: -0.0206
          Mean episode rew_torques: -0.0310
 Mean episode rew_tracking_ang_vel: 0.2662
 Mean episode rew_tracking_lin_vel: 0.4873
            Mean episode rew_total: 0.4388
        Mean episode terrain_level: 0.5799
    Mean episode min_command_x_vel: -0.9987
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 2.47s
                        Total time: 623.56s
                               ETA: 22733.0s

################################################################################
                     [1m Learning iteration 267/10000 [0m                     

                       Computation: 40276 steps/s (collection: 2.079s, learning 0.362s)
               Value function loss: 0.0067
           Forward prediction loss: 0.0000
                          CMT loss: 0.0070
                    Surrogate loss: 0.0032
                    Policy entropy: 10.3713
             Mean action noise std: 0.58
                       Mean reward: 9.40
               Mean episode length: 875.34
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0800
       Mean episode rew_ang_vel_xy: -0.0633
        Mean episode rew_collision: -0.0406
          Mean episode rew_dof_acc: -0.0926
   Mean episode rew_dof_pos_limits: -0.0148
        Mean episode rew_lin_vel_z: -0.0210
          Mean episode rew_torques: -0.0323
 Mean episode rew_tracking_ang_vel: 0.2724
 Mean episode rew_tracking_lin_vel: 0.5351
            Mean episode rew_total: 0.4631
        Mean episode terrain_level: 0.5789
    Mean episode min_command_x_vel: -0.9987
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 2.44s
                        Total time: 626.00s
                               ETA: 22734.5s

################################################################################
                     [1m Learning iteration 268/10000 [0m                     

                       Computation: 39701 steps/s (collection: 2.111s, learning 0.365s)
               Value function loss: 0.0075
           Forward prediction loss: 0.0000
                          CMT loss: 0.0070
                    Surrogate loss: 0.0020
                    Policy entropy: 10.3699
             Mean action noise std: 0.58
                       Mean reward: 9.62
               Mean episode length: 870.60
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0778
       Mean episode rew_ang_vel_xy: -0.0624
        Mean episode rew_collision: -0.0303
          Mean episode rew_dof_acc: -0.0876
   Mean episode rew_dof_pos_limits: -0.0142
        Mean episode rew_lin_vel_z: -0.0196
          Mean episode rew_torques: -0.0319
 Mean episode rew_tracking_ang_vel: 0.2647
 Mean episode rew_tracking_lin_vel: 0.5119
            Mean episode rew_total: 0.4527
        Mean episode terrain_level: 0.5748
    Mean episode min_command_x_vel: -0.9988
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 2.48s
                        Total time: 628.48s
                               ETA: 22737.2s

################################################################################
                     [1m Learning iteration 269/10000 [0m                     

                       Computation: 40540 steps/s (collection: 2.058s, learning 0.367s)
               Value function loss: 0.0075
           Forward prediction loss: 0.0000
                          CMT loss: 0.0071
                    Surrogate loss: 0.0022
                    Policy entropy: 10.3688
             Mean action noise std: 0.58
                       Mean reward: 8.43
               Mean episode length: 818.63
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0773
       Mean episode rew_ang_vel_xy: -0.0600
        Mean episode rew_collision: -0.0127
          Mean episode rew_dof_acc: -0.0870
   Mean episode rew_dof_pos_limits: -0.0155
        Mean episode rew_lin_vel_z: -0.0203
          Mean episode rew_torques: -0.0309
 Mean episode rew_tracking_ang_vel: 0.2606
 Mean episode rew_tracking_lin_vel: 0.4727
            Mean episode rew_total: 0.4296
        Mean episode terrain_level: 0.5719
    Mean episode min_command_x_vel: -0.9991
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 2.42s
                        Total time: 630.90s
                               ETA: 22738.1s

################################################################################
                     [1m Learning iteration 270/10000 [0m                     

                       Computation: 39981 steps/s (collection: 2.094s, learning 0.365s)
               Value function loss: 0.0074
           Forward prediction loss: 0.0000
                          CMT loss: 0.0070
                    Surrogate loss: 0.0029
                    Policy entropy: 10.3676
             Mean action noise std: 0.58
                       Mean reward: 9.02
               Mean episode length: 859.70
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0788
       Mean episode rew_ang_vel_xy: -0.0613
        Mean episode rew_collision: -0.0205
          Mean episode rew_dof_acc: -0.0954
   Mean episode rew_dof_pos_limits: -0.0138
        Mean episode rew_lin_vel_z: -0.0215
          Mean episode rew_torques: -0.0315
 Mean episode rew_tracking_ang_vel: 0.2604
 Mean episode rew_tracking_lin_vel: 0.4951
            Mean episode rew_total: 0.4327
        Mean episode terrain_level: 0.5697
    Mean episode min_command_x_vel: -0.9992
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 2.46s
                        Total time: 633.36s
                               ETA: 22740.1s

################################################################################
                     [1m Learning iteration 271/10000 [0m                     

                       Computation: 39460 steps/s (collection: 2.125s, learning 0.367s)
               Value function loss: 0.0118
           Forward prediction loss: 0.0000
                          CMT loss: 0.0070
                    Surrogate loss: 0.0024
                    Policy entropy: 10.3665
             Mean action noise std: 0.58
                       Mean reward: 8.67
               Mean episode length: 858.27
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0797
       Mean episode rew_ang_vel_xy: -0.0618
        Mean episode rew_collision: -0.0282
          Mean episode rew_dof_acc: -0.0902
   Mean episode rew_dof_pos_limits: -0.0162
        Mean episode rew_lin_vel_z: -0.0210
          Mean episode rew_torques: -0.0313
 Mean episode rew_tracking_ang_vel: 0.2739
 Mean episode rew_tracking_lin_vel: 0.5074
            Mean episode rew_total: 0.4527
        Mean episode terrain_level: 0.5679
    Mean episode min_command_x_vel: -0.9992
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 2.49s
                        Total time: 635.85s
                               ETA: 22743.3s

################################################################################
                     [1m Learning iteration 272/10000 [0m                     

                       Computation: 40511 steps/s (collection: 2.062s, learning 0.365s)
               Value function loss: 0.0058
           Forward prediction loss: 0.0000
                          CMT loss: 0.0069
                    Surrogate loss: 0.0031
                    Policy entropy: 10.3661
             Mean action noise std: 0.58
                       Mean reward: 8.59
               Mean episode length: 872.24
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0830
       Mean episode rew_ang_vel_xy: -0.0654
        Mean episode rew_collision: -0.0252
          Mean episode rew_dof_acc: -0.1004
   Mean episode rew_dof_pos_limits: -0.0149
        Mean episode rew_lin_vel_z: -0.0217
          Mean episode rew_torques: -0.0329
 Mean episode rew_tracking_ang_vel: 0.2754
 Mean episode rew_tracking_lin_vel: 0.5077
            Mean episode rew_total: 0.4397
        Mean episode terrain_level: 0.5684
    Mean episode min_command_x_vel: -0.9992
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 2.43s
                        Total time: 638.28s
                               ETA: 22744.1s

################################################################################
                     [1m Learning iteration 273/10000 [0m                     

                       Computation: 39431 steps/s (collection: 2.130s, learning 0.363s)
               Value function loss: 0.0074
           Forward prediction loss: 0.0000
                          CMT loss: 0.0071
                    Surrogate loss: 0.0034
                    Policy entropy: 10.3648
             Mean action noise std: 0.58
                       Mean reward: 9.49
               Mean episode length: 867.74
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0796
       Mean episode rew_ang_vel_xy: -0.0616
        Mean episode rew_collision: -0.0161
          Mean episode rew_dof_acc: -0.0903
   Mean episode rew_dof_pos_limits: -0.0166
        Mean episode rew_lin_vel_z: -0.0206
          Mean episode rew_torques: -0.0307
 Mean episode rew_tracking_ang_vel: 0.2737
 Mean episode rew_tracking_lin_vel: 0.5203
            Mean episode rew_total: 0.4785
        Mean episode terrain_level: 0.5682
    Mean episode min_command_x_vel: -0.9992
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 2.49s
                        Total time: 640.77s
                               ETA: 22747.3s

################################################################################
                     [1m Learning iteration 274/10000 [0m                     

                       Computation: 40719 steps/s (collection: 2.053s, learning 0.362s)
               Value function loss: 0.0075
           Forward prediction loss: 0.0000
                          CMT loss: 0.0072
                    Surrogate loss: 0.0025
                    Policy entropy: 10.3637
             Mean action noise std: 0.58
                       Mean reward: 9.45
               Mean episode length: 914.64
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0816
       Mean episode rew_ang_vel_xy: -0.0631
        Mean episode rew_collision: -0.0303
          Mean episode rew_dof_acc: -0.0931
   Mean episode rew_dof_pos_limits: -0.0170
        Mean episode rew_lin_vel_z: -0.0218
          Mean episode rew_torques: -0.0312
 Mean episode rew_tracking_ang_vel: 0.2814
 Mean episode rew_tracking_lin_vel: 0.5215
            Mean episode rew_total: 0.4649
        Mean episode terrain_level: 0.5673
    Mean episode min_command_x_vel: -0.9992
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 2.41s
                        Total time: 643.18s
                               ETA: 22747.6s

################################################################################
                     [1m Learning iteration 275/10000 [0m                     

                       Computation: 42038 steps/s (collection: 1.974s, learning 0.365s)
               Value function loss: 0.0067
           Forward prediction loss: 0.0000
                          CMT loss: 0.0074
                    Surrogate loss: 0.0044
                    Policy entropy: 10.3623
             Mean action noise std: 0.58
                       Mean reward: 9.69
               Mean episode length: 895.79
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0806
       Mean episode rew_ang_vel_xy: -0.0620
        Mean episode rew_collision: -0.0176
          Mean episode rew_dof_acc: -0.0932
   Mean episode rew_dof_pos_limits: -0.0158
        Mean episode rew_lin_vel_z: -0.0211
          Mean episode rew_torques: -0.0316
 Mean episode rew_tracking_ang_vel: 0.2718
 Mean episode rew_tracking_lin_vel: 0.5163
            Mean episode rew_total: 0.4661
        Mean episode terrain_level: 0.5683
    Mean episode min_command_x_vel: -0.9992
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4993
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 2.34s
                        Total time: 645.52s
                               ETA: 22745.3s

################################################################################
                     [1m Learning iteration 276/10000 [0m                     

                       Computation: 39761 steps/s (collection: 2.106s, learning 0.367s)
               Value function loss: 0.0073
           Forward prediction loss: 0.0000
                          CMT loss: 0.0074
                    Surrogate loss: 0.0031
                    Policy entropy: 10.3617
             Mean action noise std: 0.58
                       Mean reward: 9.63
               Mean episode length: 905.39
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0802
       Mean episode rew_ang_vel_xy: -0.0626
        Mean episode rew_collision: -0.0226
          Mean episode rew_dof_acc: -0.0897
   Mean episode rew_dof_pos_limits: -0.0167
        Mean episode rew_lin_vel_z: -0.0207
          Mean episode rew_torques: -0.0306
 Mean episode rew_tracking_ang_vel: 0.2784
 Mean episode rew_tracking_lin_vel: 0.5230
            Mean episode rew_total: 0.4784
        Mean episode terrain_level: 0.5675
    Mean episode min_command_x_vel: -0.9992
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4993
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4995
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 2.47s
                        Total time: 647.99s
                               ETA: 22747.6s

################################################################################
                     [1m Learning iteration 277/10000 [0m                     

                       Computation: 39636 steps/s (collection: 2.118s, learning 0.362s)
               Value function loss: 0.0076
           Forward prediction loss: 0.0000
                          CMT loss: 0.0075
                    Surrogate loss: 0.0028
                    Policy entropy: 10.3610
             Mean action noise std: 0.58
                       Mean reward: 9.27
               Mean episode length: 900.91
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0835
       Mean episode rew_ang_vel_xy: -0.0647
        Mean episode rew_collision: -0.0185
          Mean episode rew_dof_acc: -0.0976
   Mean episode rew_dof_pos_limits: -0.0164
        Mean episode rew_lin_vel_z: -0.0215
          Mean episode rew_torques: -0.0321
 Mean episode rew_tracking_ang_vel: 0.2866
 Mean episode rew_tracking_lin_vel: 0.5361
            Mean episode rew_total: 0.4885
        Mean episode terrain_level: 0.5666
    Mean episode min_command_x_vel: -0.9992
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4994
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4994
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 2.48s
                        Total time: 650.47s
                               ETA: 22750.2s

################################################################################
                     [1m Learning iteration 278/10000 [0m                     

                       Computation: 40423 steps/s (collection: 2.069s, learning 0.362s)
               Value function loss: 0.0067
           Forward prediction loss: 0.0000
                          CMT loss: 0.0075
                    Surrogate loss: 0.0032
                    Policy entropy: 10.3600
             Mean action noise std: 0.58
                       Mean reward: 10.33
               Mean episode length: 908.74
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0817
       Mean episode rew_ang_vel_xy: -0.0622
        Mean episode rew_collision: -0.0225
          Mean episode rew_dof_acc: -0.0900
   Mean episode rew_dof_pos_limits: -0.0172
        Mean episode rew_lin_vel_z: -0.0209
          Mean episode rew_torques: -0.0307
 Mean episode rew_tracking_ang_vel: 0.2935
 Mean episode rew_tracking_lin_vel: 0.5567
            Mean episode rew_total: 0.5248
        Mean episode terrain_level: 0.5682
    Mean episode min_command_x_vel: -0.9992
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4995
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4993
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 2.43s
                        Total time: 652.91s
                               ETA: 22751.1s

################################################################################
                     [1m Learning iteration 279/10000 [0m                     

                       Computation: 39254 steps/s (collection: 2.138s, learning 0.366s)
               Value function loss: 0.0073
           Forward prediction loss: 0.0000
                          CMT loss: 0.0077
                    Surrogate loss: 0.0029
                    Policy entropy: 10.3589
             Mean action noise std: 0.58
                       Mean reward: 8.89
               Mean episode length: 879.99
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0807
       Mean episode rew_ang_vel_xy: -0.0624
        Mean episode rew_collision: -0.0418
          Mean episode rew_dof_acc: -0.0934
   Mean episode rew_dof_pos_limits: -0.0164
        Mean episode rew_lin_vel_z: -0.0209
          Mean episode rew_torques: -0.0307
 Mean episode rew_tracking_ang_vel: 0.2783
 Mean episode rew_tracking_lin_vel: 0.5129
            Mean episode rew_total: 0.4449
        Mean episode terrain_level: 0.5665
    Mean episode min_command_x_vel: -0.9992
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4995
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4992
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 2.50s
                        Total time: 655.41s
                               ETA: 22754.5s

################################################################################
                     [1m Learning iteration 280/10000 [0m                     

                       Computation: 38906 steps/s (collection: 2.165s, learning 0.362s)
               Value function loss: 0.0067
           Forward prediction loss: 0.0000
                          CMT loss: 0.0078
                    Surrogate loss: 0.0029
                    Policy entropy: 10.3578
             Mean action noise std: 0.58
                       Mean reward: 9.69
               Mean episode length: 912.14
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0860
       Mean episode rew_ang_vel_xy: -0.0659
        Mean episode rew_collision: -0.0132
          Mean episode rew_dof_acc: -0.1048
   Mean episode rew_dof_pos_limits: -0.0168
        Mean episode rew_lin_vel_z: -0.0221
          Mean episode rew_torques: -0.0331
 Mean episode rew_tracking_ang_vel: 0.2852
 Mean episode rew_tracking_lin_vel: 0.5337
            Mean episode rew_total: 0.4770
        Mean episode terrain_level: 0.5672
    Mean episode min_command_x_vel: -0.9992
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4995
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4993
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 2.53s
                        Total time: 657.94s
                               ETA: 22758.5s

################################################################################
                     [1m Learning iteration 281/10000 [0m                     

                       Computation: 41613 steps/s (collection: 2.000s, learning 0.363s)
               Value function loss: 0.0064
           Forward prediction loss: 0.0000
                          CMT loss: 0.0077
                    Surrogate loss: 0.0028
                    Policy entropy: 10.3569
             Mean action noise std: 0.58
                       Mean reward: 9.91
               Mean episode length: 890.29
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0838
       Mean episode rew_ang_vel_xy: -0.0653
        Mean episode rew_collision: -0.0124
          Mean episode rew_dof_acc: -0.1051
   Mean episode rew_dof_pos_limits: -0.0152
        Mean episode rew_lin_vel_z: -0.0221
          Mean episode rew_torques: -0.0328
 Mean episode rew_tracking_ang_vel: 0.2750
 Mean episode rew_tracking_lin_vel: 0.5504
            Mean episode rew_total: 0.4885
        Mean episode terrain_level: 0.5680
    Mean episode min_command_x_vel: -0.9992
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4995
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4995
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 2.36s
                        Total time: 660.30s
                               ETA: 22756.9s

################################################################################
                     [1m Learning iteration 282/10000 [0m                     

                       Computation: 39132 steps/s (collection: 2.148s, learning 0.364s)
               Value function loss: 0.0078
           Forward prediction loss: 0.0000
                          CMT loss: 0.0077
                    Surrogate loss: 0.0025
                    Policy entropy: 10.3557
             Mean action noise std: 0.58
                       Mean reward: 9.04
               Mean episode length: 860.08
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0814
       Mean episode rew_ang_vel_xy: -0.0630
        Mean episode rew_collision: -0.0210
          Mean episode rew_dof_acc: -0.1006
   Mean episode rew_dof_pos_limits: -0.0158
        Mean episode rew_lin_vel_z: -0.0214
          Mean episode rew_torques: -0.0318
 Mean episode rew_tracking_ang_vel: 0.2655
 Mean episode rew_tracking_lin_vel: 0.5182
            Mean episode rew_total: 0.4488
        Mean episode terrain_level: 0.5693
    Mean episode min_command_x_vel: -0.9992
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4995
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4995
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 2.51s
                        Total time: 662.81s
                               ETA: 22760.4s

################################################################################
                     [1m Learning iteration 283/10000 [0m                     

                       Computation: 40060 steps/s (collection: 2.090s, learning 0.364s)
               Value function loss: 0.0064
           Forward prediction loss: 0.0000
                          CMT loss: 0.0081
                    Surrogate loss: 0.0037
                    Policy entropy: 10.3541
             Mean action noise std: 0.58
                       Mean reward: 9.10
               Mean episode length: 855.80
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0773
       Mean episode rew_ang_vel_xy: -0.0601
        Mean episode rew_collision: -0.0226
          Mean episode rew_dof_acc: -0.0950
   Mean episode rew_dof_pos_limits: -0.0152
        Mean episode rew_lin_vel_z: -0.0210
          Mean episode rew_torques: -0.0294
 Mean episode rew_tracking_ang_vel: 0.2554
 Mean episode rew_tracking_lin_vel: 0.4961
            Mean episode rew_total: 0.4309
        Mean episode terrain_level: 0.5693
    Mean episode min_command_x_vel: -0.9991
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4995
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 2.45s
                        Total time: 665.27s
                               ETA: 22761.9s

################################################################################
                     [1m Learning iteration 284/10000 [0m                     

                       Computation: 40136 steps/s (collection: 2.083s, learning 0.367s)
               Value function loss: 0.0071
           Forward prediction loss: 0.0000
                          CMT loss: 0.0081
                    Surrogate loss: 0.0128
                    Policy entropy: 10.3521
             Mean action noise std: 0.58
                       Mean reward: 8.87
               Mean episode length: 860.97
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0778
       Mean episode rew_ang_vel_xy: -0.0609
        Mean episode rew_collision: -0.0179
          Mean episode rew_dof_acc: -0.0909
   Mean episode rew_dof_pos_limits: -0.0167
        Mean episode rew_lin_vel_z: -0.0206
          Mean episode rew_torques: -0.0297
 Mean episode rew_tracking_ang_vel: 0.2631
 Mean episode rew_tracking_lin_vel: 0.4747
            Mean episode rew_total: 0.4233
        Mean episode terrain_level: 0.5696
    Mean episode min_command_x_vel: -0.9991
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4995
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 2.45s
                        Total time: 667.71s
                               ETA: 22763.2s

################################################################################
                     [1m Learning iteration 285/10000 [0m                     

                       Computation: 40273 steps/s (collection: 2.078s, learning 0.363s)
               Value function loss: 0.0078
           Forward prediction loss: 0.0000
                          CMT loss: 0.0081
                    Surrogate loss: 0.0027
                    Policy entropy: 10.3510
             Mean action noise std: 0.58
                       Mean reward: 9.10
               Mean episode length: 869.13
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0767
       Mean episode rew_ang_vel_xy: -0.0582
        Mean episode rew_collision: -0.0296
          Mean episode rew_dof_acc: -0.0864
   Mean episode rew_dof_pos_limits: -0.0180
        Mean episode rew_lin_vel_z: -0.0203
          Mean episode rew_torques: -0.0289
 Mean episode rew_tracking_ang_vel: 0.2690
 Mean episode rew_tracking_lin_vel: 0.5120
            Mean episode rew_total: 0.4628
        Mean episode terrain_level: 0.5704
    Mean episode min_command_x_vel: -0.9993
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4995
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 2.44s
                        Total time: 670.16s
                               ETA: 22764.2s

################################################################################
                     [1m Learning iteration 286/10000 [0m                     

                       Computation: 39553 steps/s (collection: 2.124s, learning 0.362s)
               Value function loss: 0.0064
           Forward prediction loss: 0.0000
                          CMT loss: 0.0081
                    Surrogate loss: 0.0038
                    Policy entropy: 10.3504
             Mean action noise std: 0.58
                       Mean reward: 9.15
               Mean episode length: 900.27
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0846
       Mean episode rew_ang_vel_xy: -0.0663
        Mean episode rew_collision: -0.0362
          Mean episode rew_dof_acc: -0.1019
   Mean episode rew_dof_pos_limits: -0.0172
        Mean episode rew_lin_vel_z: -0.0216
          Mean episode rew_torques: -0.0325
 Mean episode rew_tracking_ang_vel: 0.2873
 Mean episode rew_tracking_lin_vel: 0.5388
            Mean episode rew_total: 0.4657
        Mean episode terrain_level: 0.5696
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 2.49s
                        Total time: 672.64s
                               ETA: 22766.7s

################################################################################
                     [1m Learning iteration 287/10000 [0m                     

                       Computation: 41000 steps/s (collection: 2.033s, learning 0.364s)
               Value function loss: 0.0066
           Forward prediction loss: 0.0000
                          CMT loss: 0.0081
                    Surrogate loss: 0.0036
                    Policy entropy: 10.3496
             Mean action noise std: 0.58
                       Mean reward: 9.20
               Mean episode length: 873.58
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0776
       Mean episode rew_ang_vel_xy: -0.0605
        Mean episode rew_collision: -0.0483
          Mean episode rew_dof_acc: -0.0901
   Mean episode rew_dof_pos_limits: -0.0184
        Mean episode rew_lin_vel_z: -0.0201
          Mean episode rew_torques: -0.0288
 Mean episode rew_tracking_ang_vel: 0.2786
 Mean episode rew_tracking_lin_vel: 0.5145
            Mean episode rew_total: 0.4493
        Mean episode terrain_level: 0.5700
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 2.40s
                        Total time: 675.04s
                               ETA: 22766.2s

################################################################################
                     [1m Learning iteration 288/10000 [0m                     

                       Computation: 40139 steps/s (collection: 2.087s, learning 0.362s)
               Value function loss: 0.0077
           Forward prediction loss: 0.0000
                          CMT loss: 0.0081
                    Surrogate loss: 0.0029
                    Policy entropy: 10.3484
             Mean action noise std: 0.58
                       Mean reward: 9.19
               Mean episode length: 861.08
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0816
       Mean episode rew_ang_vel_xy: -0.0610
        Mean episode rew_collision: -0.0251
          Mean episode rew_dof_acc: -0.0915
   Mean episode rew_dof_pos_limits: -0.0193
        Mean episode rew_lin_vel_z: -0.0209
          Mean episode rew_torques: -0.0301
 Mean episode rew_tracking_ang_vel: 0.2849
 Mean episode rew_tracking_lin_vel: 0.5317
            Mean episode rew_total: 0.4871
        Mean episode terrain_level: 0.5707
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 2.45s
                        Total time: 677.49s
                               ETA: 22767.3s

################################################################################
                     [1m Learning iteration 289/10000 [0m                     

                       Computation: 40287 steps/s (collection: 2.073s, learning 0.367s)
               Value function loss: 0.0076
           Forward prediction loss: 0.0000
                          CMT loss: 0.0081
                    Surrogate loss: 0.0043
                    Policy entropy: 10.3474
             Mean action noise std: 0.58
                       Mean reward: 9.63
               Mean episode length: 871.57
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0805
       Mean episode rew_ang_vel_xy: -0.0614
        Mean episode rew_collision: -0.0205
          Mean episode rew_dof_acc: -0.0987
   Mean episode rew_dof_pos_limits: -0.0170
        Mean episode rew_lin_vel_z: -0.0211
          Mean episode rew_torques: -0.0306
 Mean episode rew_tracking_ang_vel: 0.2679
 Mean episode rew_tracking_lin_vel: 0.5231
            Mean episode rew_total: 0.4612
        Mean episode terrain_level: 0.5709
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 2.44s
                        Total time: 679.93s
                               ETA: 22768.2s

################################################################################
                     [1m Learning iteration 290/10000 [0m                     

                       Computation: 40449 steps/s (collection: 2.066s, learning 0.365s)
               Value function loss: 0.0062
           Forward prediction loss: 0.0000
                          CMT loss: 0.0082
                    Surrogate loss: 0.0034
                    Policy entropy: 10.3468
             Mean action noise std: 0.58
                       Mean reward: 8.77
               Mean episode length: 888.13
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0839
       Mean episode rew_ang_vel_xy: -0.0646
        Mean episode rew_collision: -0.0294
          Mean episode rew_dof_acc: -0.1042
   Mean episode rew_dof_pos_limits: -0.0181
        Mean episode rew_lin_vel_z: -0.0223
          Mean episode rew_torques: -0.0316
 Mean episode rew_tracking_ang_vel: 0.2751
 Mean episode rew_tracking_lin_vel: 0.5123
            Mean episode rew_total: 0.4333
        Mean episode terrain_level: 0.5731
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7067
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 2.43s
                        Total time: 682.36s
                               ETA: 22768.7s

################################################################################
                     [1m Learning iteration 291/10000 [0m                     

                       Computation: 41486 steps/s (collection: 2.006s, learning 0.364s)
               Value function loss: 0.0063
           Forward prediction loss: 0.0000
                          CMT loss: 0.0083
                    Surrogate loss: 0.0032
                    Policy entropy: 10.3462
             Mean action noise std: 0.58
                       Mean reward: 9.47
               Mean episode length: 850.67
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0776
       Mean episode rew_ang_vel_xy: -0.0585
        Mean episode rew_collision: -0.0168
          Mean episode rew_dof_acc: -0.0913
   Mean episode rew_dof_pos_limits: -0.0174
        Mean episode rew_lin_vel_z: -0.0205
          Mean episode rew_torques: -0.0291
 Mean episode rew_tracking_ang_vel: 0.2645
 Mean episode rew_tracking_lin_vel: 0.5315
            Mean episode rew_total: 0.4849
        Mean episode terrain_level: 0.5711
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7122
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 2.37s
                        Total time: 684.73s
                               ETA: 22767.2s

################################################################################
                     [1m Learning iteration 292/10000 [0m                     

                       Computation: 34831 steps/s (collection: 2.457s, learning 0.365s)
               Value function loss: 0.0097
           Forward prediction loss: 0.0000
                          CMT loss: 0.0091
                    Surrogate loss: 0.0052
                    Policy entropy: 10.3456
             Mean action noise std: 0.58
                       Mean reward: 8.85
               Mean episode length: 873.70
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0815
       Mean episode rew_ang_vel_xy: -0.0619
        Mean episode rew_collision: -0.0140
          Mean episode rew_dof_acc: -0.1031
   Mean episode rew_dof_pos_limits: -0.0170
        Mean episode rew_lin_vel_z: -0.0216
          Mean episode rew_torques: -0.0306
 Mean episode rew_tracking_ang_vel: 0.2721
 Mean episode rew_tracking_lin_vel: 0.5312
            Mean episode rew_total: 0.4735
        Mean episode terrain_level: 0.5734
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 2.82s
                        Total time: 687.55s
                               ETA: 22780.7s

################################################################################
                     [1m Learning iteration 293/10000 [0m                     

                       Computation: 40489 steps/s (collection: 2.064s, learning 0.364s)
               Value function loss: 0.0070
           Forward prediction loss: 0.0000
                          CMT loss: 0.0082
                    Surrogate loss: 0.0047
                    Policy entropy: 10.3451
             Mean action noise std: 0.58
                       Mean reward: 8.54
               Mean episode length: 817.69
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0763
       Mean episode rew_ang_vel_xy: -0.0580
        Mean episode rew_collision: -0.0237
          Mean episode rew_dof_acc: -0.0980
   Mean episode rew_dof_pos_limits: -0.0157
        Mean episode rew_lin_vel_z: -0.0207
          Mean episode rew_torques: -0.0291
 Mean episode rew_tracking_ang_vel: 0.2524
 Mean episode rew_tracking_lin_vel: 0.4844
            Mean episode rew_total: 0.4155
        Mean episode terrain_level: 0.5759
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 2.43s
                        Total time: 689.98s
                               ETA: 22781.0s

################################################################################
                     [1m Learning iteration 294/10000 [0m                     

                       Computation: 38395 steps/s (collection: 2.194s, learning 0.367s)
               Value function loss: 0.0074
           Forward prediction loss: 0.0000
                          CMT loss: 0.0084
                    Surrogate loss: 0.0032
                    Policy entropy: 10.3446
             Mean action noise std: 0.58
                       Mean reward: 9.92
               Mean episode length: 881.37
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0786
       Mean episode rew_ang_vel_xy: -0.0604
        Mean episode rew_collision: -0.0193
          Mean episode rew_dof_acc: -0.0947
   Mean episode rew_dof_pos_limits: -0.0182
        Mean episode rew_lin_vel_z: -0.0207
          Mean episode rew_torques: -0.0290
 Mean episode rew_tracking_ang_vel: 0.2716
 Mean episode rew_tracking_lin_vel: 0.5209
            Mean episode rew_total: 0.4715
        Mean episode terrain_level: 0.5781
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 2.56s
                        Total time: 692.54s
                               ETA: 22785.7s

################################################################################
                     [1m Learning iteration 295/10000 [0m                     

                       Computation: 38494 steps/s (collection: 2.192s, learning 0.362s)
               Value function loss: 0.0079
           Forward prediction loss: 0.0000
                          CMT loss: 0.0086
                    Surrogate loss: 0.0039
                    Policy entropy: 10.3441
             Mean action noise std: 0.58
                       Mean reward: 9.24
               Mean episode length: 874.86
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0817
       Mean episode rew_ang_vel_xy: -0.0616
        Mean episode rew_collision: -0.0179
          Mean episode rew_dof_acc: -0.1011
   Mean episode rew_dof_pos_limits: -0.0184
        Mean episode rew_lin_vel_z: -0.0215
          Mean episode rew_torques: -0.0300
 Mean episode rew_tracking_ang_vel: 0.2727
 Mean episode rew_tracking_lin_vel: 0.5200
            Mean episode rew_total: 0.4605
        Mean episode terrain_level: 0.5795
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 2.55s
                        Total time: 695.09s
                               ETA: 22790.1s

################################################################################
                     [1m Learning iteration 296/10000 [0m                     

                       Computation: 39424 steps/s (collection: 2.131s, learning 0.363s)
               Value function loss: 0.0073
           Forward prediction loss: 0.0000
                          CMT loss: 0.0086
                    Surrogate loss: 0.0037
                    Policy entropy: 10.3431
             Mean action noise std: 0.58
                       Mean reward: 9.26
               Mean episode length: 861.17
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0792
       Mean episode rew_ang_vel_xy: -0.0591
        Mean episode rew_collision: -0.0121
          Mean episode rew_dof_acc: -0.0935
   Mean episode rew_dof_pos_limits: -0.0194
        Mean episode rew_lin_vel_z: -0.0205
          Mean episode rew_torques: -0.0288
 Mean episode rew_tracking_ang_vel: 0.2712
 Mean episode rew_tracking_lin_vel: 0.5158
            Mean episode rew_total: 0.4745
        Mean episode terrain_level: 0.5796
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 2.49s
                        Total time: 697.59s
                               ETA: 22792.5s

################################################################################
                     [1m Learning iteration 297/10000 [0m                     

                       Computation: 39790 steps/s (collection: 2.107s, learning 0.364s)
               Value function loss: 0.0066
           Forward prediction loss: 0.0000
                          CMT loss: 0.0085
                    Surrogate loss: 0.0043
                    Policy entropy: 10.3422
             Mean action noise std: 0.58
                       Mean reward: 9.58
               Mean episode length: 899.57
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0839
       Mean episode rew_ang_vel_xy: -0.0635
        Mean episode rew_collision: -0.0266
          Mean episode rew_dof_acc: -0.1060
   Mean episode rew_dof_pos_limits: -0.0187
        Mean episode rew_lin_vel_z: -0.0217
          Mean episode rew_torques: -0.0312
 Mean episode rew_tracking_ang_vel: 0.2770
 Mean episode rew_tracking_lin_vel: 0.5485
            Mean episode rew_total: 0.4738
        Mean episode terrain_level: 0.5778
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4998
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 2.47s
                        Total time: 700.06s
                               ETA: 22794.1s

################################################################################
                     [1m Learning iteration 298/10000 [0m                     

                       Computation: 38621 steps/s (collection: 2.183s, learning 0.363s)
               Value function loss: 0.0075
           Forward prediction loss: 0.0000
                          CMT loss: 0.0086
                    Surrogate loss: 0.0053
                    Policy entropy: 10.3418
             Mean action noise std: 0.58
                       Mean reward: 9.21
               Mean episode length: 889.24
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0788
       Mean episode rew_ang_vel_xy: -0.0587
        Mean episode rew_collision: -0.0159
          Mean episode rew_dof_acc: -0.0940
   Mean episode rew_dof_pos_limits: -0.0199
        Mean episode rew_lin_vel_z: -0.0210
          Mean episode rew_torques: -0.0275
 Mean episode rew_tracking_ang_vel: 0.2727
 Mean episode rew_tracking_lin_vel: 0.4969
            Mean episode rew_total: 0.4538
        Mean episode terrain_level: 0.5792
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4993
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 2.55s
                        Total time: 702.60s
                               ETA: 22798.1s

################################################################################
                     [1m Learning iteration 299/10000 [0m                     

                       Computation: 41005 steps/s (collection: 2.037s, learning 0.360s)
               Value function loss: 0.0072
           Forward prediction loss: 0.0000
                          CMT loss: 0.0088
                    Surrogate loss: 0.0031
                    Policy entropy: 10.3414
             Mean action noise std: 0.58
                       Mean reward: 8.50
               Mean episode length: 853.05
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0782
       Mean episode rew_ang_vel_xy: -0.0579
        Mean episode rew_collision: -0.0172
          Mean episode rew_dof_acc: -0.0949
   Mean episode rew_dof_pos_limits: -0.0190
        Mean episode rew_lin_vel_z: -0.0212
          Mean episode rew_torques: -0.0285
 Mean episode rew_tracking_ang_vel: 0.2604
 Mean episode rew_tracking_lin_vel: 0.4810
            Mean episode rew_total: 0.4245
        Mean episode terrain_level: 0.5803
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4993
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 2.40s
                        Total time: 705.00s
                               ETA: 22797.3s

################################################################################
                     [1m Learning iteration 300/10000 [0m                     

                       Computation: 39014 steps/s (collection: 2.156s, learning 0.364s)
               Value function loss: 0.0072
           Forward prediction loss: 0.0000
                          CMT loss: 0.0090
                    Surrogate loss: 0.0079
                    Policy entropy: 10.3404
             Mean action noise std: 0.58
                       Mean reward: 9.26
               Mean episode length: 868.76
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0820
       Mean episode rew_ang_vel_xy: -0.0617
        Mean episode rew_collision: -0.0216
          Mean episode rew_dof_acc: -0.1011
   Mean episode rew_dof_pos_limits: -0.0196
        Mean episode rew_lin_vel_z: -0.0212
          Mean episode rew_torques: -0.0299
 Mean episode rew_tracking_ang_vel: 0.2765
 Mean episode rew_tracking_lin_vel: 0.5433
            Mean episode rew_total: 0.4827
        Mean episode terrain_level: 0.5803
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4993
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 2.52s
                        Total time: 707.52s
                               ETA: 22800.4s

################################################################################
                     [1m Learning iteration 301/10000 [0m                     

                       Computation: 39585 steps/s (collection: 2.120s, learning 0.364s)
               Value function loss: 0.0071
           Forward prediction loss: 0.0000
                          CMT loss: 0.0090
                    Surrogate loss: 0.0037
                    Policy entropy: 10.3393
             Mean action noise std: 0.58
                       Mean reward: 8.47
               Mean episode length: 849.94
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0776
       Mean episode rew_ang_vel_xy: -0.0579
        Mean episode rew_collision: -0.0212
          Mean episode rew_dof_acc: -0.0954
   Mean episode rew_dof_pos_limits: -0.0190
        Mean episode rew_lin_vel_z: -0.0210
          Mean episode rew_torques: -0.0281
 Mean episode rew_tracking_ang_vel: 0.2622
 Mean episode rew_tracking_lin_vel: 0.4892
            Mean episode rew_total: 0.4314
        Mean episode terrain_level: 0.5814
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4997
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 2.48s
                        Total time: 710.00s
                               ETA: 22802.3s

################################################################################
                     [1m Learning iteration 302/10000 [0m                     

                       Computation: 40088 steps/s (collection: 2.088s, learning 0.364s)
               Value function loss: 0.0061
           Forward prediction loss: 0.0000
                          CMT loss: 0.0091
                    Surrogate loss: 0.0037
                    Policy entropy: 10.3379
             Mean action noise std: 0.58
                       Mean reward: 9.21
               Mean episode length: 887.71
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0804
       Mean episode rew_ang_vel_xy: -0.0600
        Mean episode rew_collision: -0.0176
          Mean episode rew_dof_acc: -0.1003
   Mean episode rew_dof_pos_limits: -0.0194
        Mean episode rew_lin_vel_z: -0.0214
          Mean episode rew_torques: -0.0290
 Mean episode rew_tracking_ang_vel: 0.2655
 Mean episode rew_tracking_lin_vel: 0.5115
            Mean episode rew_total: 0.4489
        Mean episode terrain_level: 0.5844
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4998
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 2.45s
                        Total time: 712.45s
                               ETA: 22803.2s

################################################################################
                     [1m Learning iteration 303/10000 [0m                     

                       Computation: 40443 steps/s (collection: 2.067s, learning 0.364s)
               Value function loss: 0.0077
           Forward prediction loss: 0.0000
                          CMT loss: 0.0093
                    Surrogate loss: 0.0029
                    Policy entropy: 10.3367
             Mean action noise std: 0.58
                       Mean reward: 8.45
               Mean episode length: 826.89
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0768
       Mean episode rew_ang_vel_xy: -0.0582
        Mean episode rew_collision: -0.0197
          Mean episode rew_dof_acc: -0.0984
   Mean episode rew_dof_pos_limits: -0.0185
        Mean episode rew_lin_vel_z: -0.0211
          Mean episode rew_torques: -0.0276
 Mean episode rew_tracking_ang_vel: 0.2545
 Mean episode rew_tracking_lin_vel: 0.4794
            Mean episode rew_total: 0.4135
        Mean episode terrain_level: 0.5877
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4998
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 2.43s
                        Total time: 714.88s
                               ETA: 22803.4s

################################################################################
                     [1m Learning iteration 304/10000 [0m                     

                       Computation: 39743 steps/s (collection: 2.111s, learning 0.363s)
               Value function loss: 0.0059
           Forward prediction loss: 0.0000
                          CMT loss: 0.0093
                    Surrogate loss: 0.0050
                    Policy entropy: 10.3355
             Mean action noise std: 0.57
                       Mean reward: 8.91
               Mean episode length: 866.23
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0769
       Mean episode rew_ang_vel_xy: -0.0561
        Mean episode rew_collision: -0.0165
          Mean episode rew_dof_acc: -0.0935
   Mean episode rew_dof_pos_limits: -0.0208
        Mean episode rew_lin_vel_z: -0.0209
          Mean episode rew_torques: -0.0266
 Mean episode rew_tracking_ang_vel: 0.2677
 Mean episode rew_tracking_lin_vel: 0.4848
            Mean episode rew_total: 0.4411
        Mean episode terrain_level: 0.5895
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4998
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 2.47s
                        Total time: 717.36s
                               ETA: 22804.9s

################################################################################
                     [1m Learning iteration 305/10000 [0m                     

                       Computation: 41087 steps/s (collection: 2.029s, learning 0.364s)
               Value function loss: 0.0066
           Forward prediction loss: 0.0000
                          CMT loss: 0.0093
                    Surrogate loss: 0.0044
                    Policy entropy: 10.3344
             Mean action noise std: 0.57
                       Mean reward: 9.32
               Mean episode length: 883.01
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0819
       Mean episode rew_ang_vel_xy: -0.0609
        Mean episode rew_collision: -0.0173
          Mean episode rew_dof_acc: -0.1061
   Mean episode rew_dof_pos_limits: -0.0187
        Mean episode rew_lin_vel_z: -0.0217
          Mean episode rew_torques: -0.0292
 Mean episode rew_tracking_ang_vel: 0.2645
 Mean episode rew_tracking_lin_vel: 0.5181
            Mean episode rew_total: 0.4467
        Mean episode terrain_level: 0.5919
    Mean episode min_command_x_vel: -0.9995
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.4998
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 2.39s
                        Total time: 719.75s
                               ETA: 22803.9s

################################################################################
                     [1m Learning iteration 306/10000 [0m                     

                       Computation: 40466 steps/s (collection: 2.065s, learning 0.364s)
               Value function loss: 0.0071
           Forward prediction loss: 0.0000
                          CMT loss: 0.0096
                    Surrogate loss: 0.0037
                    Policy entropy: 10.3334
             Mean action noise std: 0.57
                       Mean reward: 9.41
               Mean episode length: 879.51
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0805
       Mean episode rew_ang_vel_xy: -0.0595
        Mean episode rew_collision: -0.0112
          Mean episode rew_dof_acc: -0.1007
   Mean episode rew_dof_pos_limits: -0.0206
        Mean episode rew_lin_vel_z: -0.0218
          Mean episode rew_torques: -0.0287
 Mean episode rew_tracking_ang_vel: 0.2763
 Mean episode rew_tracking_lin_vel: 0.5358
            Mean episode rew_total: 0.4890
        Mean episode terrain_level: 0.5936
    Mean episode min_command_x_vel: -0.9994
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4995
  Mean episode min_command_yaw_vel: -0.4998
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 2.43s
                        Total time: 722.18s
                               ETA: 22803.9s

################################################################################
                     [1m Learning iteration 307/10000 [0m                     

                       Computation: 40820 steps/s (collection: 2.045s, learning 0.363s)
               Value function loss: 0.0073
           Forward prediction loss: 0.0000
                          CMT loss: 0.0096
                    Surrogate loss: 0.0042
                    Policy entropy: 10.3326
             Mean action noise std: 0.57
                       Mean reward: 8.82
               Mean episode length: 803.73
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0788
       Mean episode rew_ang_vel_xy: -0.0611
        Mean episode rew_collision: -0.0176
          Mean episode rew_dof_acc: -0.1063
   Mean episode rew_dof_pos_limits: -0.0172
        Mean episode rew_lin_vel_z: -0.0217
          Mean episode rew_torques: -0.0282
 Mean episode rew_tracking_ang_vel: 0.2571
 Mean episode rew_tracking_lin_vel: 0.5366
            Mean episode rew_total: 0.4627
        Mean episode terrain_level: 0.5968
    Mean episode min_command_x_vel: -0.9994
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4987
  Mean episode min_command_yaw_vel: -0.4998
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 2.41s
                        Total time: 724.59s
                               ETA: 22803.3s

################################################################################
                     [1m Learning iteration 308/10000 [0m                     

                       Computation: 39937 steps/s (collection: 2.097s, learning 0.364s)
               Value function loss: 0.0068
           Forward prediction loss: 0.0000
                          CMT loss: 0.0097
                    Surrogate loss: 0.0029
                    Policy entropy: 10.3318
             Mean action noise std: 0.57
                       Mean reward: 8.78
               Mean episode length: 846.69
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0807
       Mean episode rew_ang_vel_xy: -0.0612
        Mean episode rew_collision: -0.0183
          Mean episode rew_dof_acc: -0.1089
   Mean episode rew_dof_pos_limits: -0.0193
        Mean episode rew_lin_vel_z: -0.0222
          Mean episode rew_torques: -0.0290
 Mean episode rew_tracking_ang_vel: 0.2609
 Mean episode rew_tracking_lin_vel: 0.5172
            Mean episode rew_total: 0.4385
        Mean episode terrain_level: 0.5990
    Mean episode min_command_x_vel: -0.9998
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4988
  Mean episode min_command_yaw_vel: -0.4998
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 2.46s
                        Total time: 727.05s
                               ETA: 22804.4s

################################################################################
                     [1m Learning iteration 309/10000 [0m                     

                       Computation: 41094 steps/s (collection: 2.031s, learning 0.361s)
               Value function loss: 0.0069
           Forward prediction loss: 0.0000
                          CMT loss: 0.0098
                    Surrogate loss: 0.0043
                    Policy entropy: 10.3310
             Mean action noise std: 0.57
                       Mean reward: 8.99
               Mean episode length: 852.74
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0791
       Mean episode rew_ang_vel_xy: -0.0581
        Mean episode rew_collision: -0.0189
          Mean episode rew_dof_acc: -0.0979
   Mean episode rew_dof_pos_limits: -0.0209
        Mean episode rew_lin_vel_z: -0.0213
          Mean episode rew_torques: -0.0276
 Mean episode rew_tracking_ang_vel: 0.2704
 Mean episode rew_tracking_lin_vel: 0.5121
            Mean episode rew_total: 0.4587
        Mean episode terrain_level: 0.6020
    Mean episode min_command_x_vel: -0.9998
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4987
  Mean episode min_command_yaw_vel: -0.4998
  Mean episode max_command_yaw_vel: 0.4994
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 2.39s
                        Total time: 729.44s
                               ETA: 22803.3s

################################################################################
                     [1m Learning iteration 310/10000 [0m                     

                       Computation: 39544 steps/s (collection: 2.120s, learning 0.365s)
               Value function loss: 0.0083
           Forward prediction loss: 0.0000
                          CMT loss: 0.0098
                    Surrogate loss: 0.0034
                    Policy entropy: 10.3302
             Mean action noise std: 0.57
                       Mean reward: 8.43
               Mean episode length: 814.65
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0747
       Mean episode rew_ang_vel_xy: -0.0558
        Mean episode rew_collision: -0.0151
          Mean episode rew_dof_acc: -0.0974
   Mean episode rew_dof_pos_limits: -0.0180
        Mean episode rew_lin_vel_z: -0.0206
          Mean episode rew_torques: -0.0267
 Mean episode rew_tracking_ang_vel: 0.2482
 Mean episode rew_tracking_lin_vel: 0.4809
            Mean episode rew_total: 0.4207
        Mean episode terrain_level: 0.6032
    Mean episode min_command_x_vel: -0.9998
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4987
  Mean episode min_command_yaw_vel: -0.4998
  Mean episode max_command_yaw_vel: 0.4993
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 2.49s
                        Total time: 731.93s
                               ETA: 22805.1s

################################################################################
                     [1m Learning iteration 311/10000 [0m                     

                       Computation: 40317 steps/s (collection: 2.075s, learning 0.364s)
               Value function loss: 0.0077
           Forward prediction loss: 0.0000
                          CMT loss: 0.0099
                    Surrogate loss: 0.0036
                    Policy entropy: 10.3295
             Mean action noise std: 0.57
                       Mean reward: 8.98
               Mean episode length: 829.89
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0771
       Mean episode rew_ang_vel_xy: -0.0569
        Mean episode rew_collision: -0.0239
          Mean episode rew_dof_acc: -0.0973
   Mean episode rew_dof_pos_limits: -0.0203
        Mean episode rew_lin_vel_z: -0.0209
          Mean episode rew_torques: -0.0277
 Mean episode rew_tracking_ang_vel: 0.2632
 Mean episode rew_tracking_lin_vel: 0.5003
            Mean episode rew_total: 0.4395
        Mean episode terrain_level: 0.6057
    Mean episode min_command_x_vel: -0.9998
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4987
  Mean episode min_command_yaw_vel: -0.4998
  Mean episode max_command_yaw_vel: 0.4993
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 2.44s
                        Total time: 734.37s
                               ETA: 22805.3s

################################################################################
                     [1m Learning iteration 312/10000 [0m                     

                       Computation: 40644 steps/s (collection: 2.056s, learning 0.363s)
               Value function loss: 0.0070
           Forward prediction loss: 0.0000
                          CMT loss: 0.0098
                    Surrogate loss: 0.0034
                    Policy entropy: 10.3291
             Mean action noise std: 0.57
                       Mean reward: 8.68
               Mean episode length: 812.98
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0741
       Mean episode rew_ang_vel_xy: -0.0546
        Mean episode rew_collision: -0.0210
          Mean episode rew_dof_acc: -0.0929
   Mean episode rew_dof_pos_limits: -0.0196
        Mean episode rew_lin_vel_z: -0.0209
          Mean episode rew_torques: -0.0257
 Mean episode rew_tracking_ang_vel: 0.2564
 Mean episode rew_tracking_lin_vel: 0.4873
            Mean episode rew_total: 0.4349
        Mean episode terrain_level: 0.6049
    Mean episode min_command_x_vel: -0.9998
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4987
  Mean episode min_command_yaw_vel: -0.4998
  Mean episode max_command_yaw_vel: 0.4993
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 2.42s
                        Total time: 736.78s
                               ETA: 22805.0s

################################################################################
                     [1m Learning iteration 313/10000 [0m                     

                       Computation: 38599 steps/s (collection: 2.184s, learning 0.362s)
               Value function loss: 0.0073
           Forward prediction loss: 0.0000
                          CMT loss: 0.0098
                    Surrogate loss: 0.0067
                    Policy entropy: 10.3285
             Mean action noise std: 0.57
                       Mean reward: 9.35
               Mean episode length: 878.40
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0794
       Mean episode rew_ang_vel_xy: -0.0572
        Mean episode rew_collision: -0.0139
          Mean episode rew_dof_acc: -0.0978
   Mean episode rew_dof_pos_limits: -0.0220
        Mean episode rew_lin_vel_z: -0.0217
          Mean episode rew_torques: -0.0269
 Mean episode rew_tracking_ang_vel: 0.2741
 Mean episode rew_tracking_lin_vel: 0.5193
            Mean episode rew_total: 0.4744
        Mean episode terrain_level: 0.6053
    Mean episode min_command_x_vel: -0.9998
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4987
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4993
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 2.55s
                        Total time: 739.33s
                               ETA: 22808.6s

################################################################################
                     [1m Learning iteration 314/10000 [0m                     

                       Computation: 39460 steps/s (collection: 2.125s, learning 0.367s)
               Value function loss: 0.0060
           Forward prediction loss: 0.0000
                          CMT loss: 0.0097
                    Surrogate loss: 0.0038
                    Policy entropy: 10.3278
             Mean action noise std: 0.57
                       Mean reward: 9.40
               Mean episode length: 891.41
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0825
       Mean episode rew_ang_vel_xy: -0.0608
        Mean episode rew_collision: -0.0281
          Mean episode rew_dof_acc: -0.1088
   Mean episode rew_dof_pos_limits: -0.0204
        Mean episode rew_lin_vel_z: -0.0227
          Mean episode rew_torques: -0.0286
 Mean episode rew_tracking_ang_vel: 0.2752
 Mean episode rew_tracking_lin_vel: 0.5396
            Mean episode rew_total: 0.4629
        Mean episode terrain_level: 0.6084
    Mean episode min_command_x_vel: -0.9998
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4994
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4995
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 2.49s
                        Total time: 741.82s
                               ETA: 22810.4s

################################################################################
                     [1m Learning iteration 315/10000 [0m                     

                       Computation: 40350 steps/s (collection: 2.073s, learning 0.363s)
               Value function loss: 0.0077
           Forward prediction loss: 0.0000
                          CMT loss: 0.0097
                    Surrogate loss: 0.0033
                    Policy entropy: 10.3264
             Mean action noise std: 0.57
                       Mean reward: 9.32
               Mean episode length: 867.74
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0780
       Mean episode rew_ang_vel_xy: -0.0573
        Mean episode rew_collision: -0.0141
          Mean episode rew_dof_acc: -0.1003
   Mean episode rew_dof_pos_limits: -0.0200
        Mean episode rew_lin_vel_z: -0.0215
          Mean episode rew_torques: -0.0270
 Mean episode rew_tracking_ang_vel: 0.2679
 Mean episode rew_tracking_lin_vel: 0.5078
            Mean episode rew_total: 0.4575
        Mean episode terrain_level: 0.6095
    Mean episode min_command_x_vel: -0.9993
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4994
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4995
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 2.44s
                        Total time: 744.26s
                               ETA: 22810.6s

################################################################################
                     [1m Learning iteration 316/10000 [0m                     

                       Computation: 39126 steps/s (collection: 2.149s, learning 0.363s)
               Value function loss: 0.0079
           Forward prediction loss: 0.0000
                          CMT loss: 0.0096
                    Surrogate loss: 0.0033
                    Policy entropy: 10.3255
             Mean action noise std: 0.57
                       Mean reward: 9.64
               Mean episode length: 896.73
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0807
       Mean episode rew_ang_vel_xy: -0.0594
        Mean episode rew_collision: -0.0138
          Mean episode rew_dof_acc: -0.1005
   Mean episode rew_dof_pos_limits: -0.0216
        Mean episode rew_lin_vel_z: -0.0218
          Mean episode rew_torques: -0.0274
 Mean episode rew_tracking_ang_vel: 0.2740
 Mean episode rew_tracking_lin_vel: 0.5370
            Mean episode rew_total: 0.4858
        Mean episode terrain_level: 0.6100
    Mean episode min_command_x_vel: -0.9989
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4994
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4995
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 2.51s
                        Total time: 746.77s
                               ETA: 22813.0s

################################################################################
                     [1m Learning iteration 317/10000 [0m                     

                       Computation: 41121 steps/s (collection: 2.027s, learning 0.364s)
               Value function loss: 0.0068
           Forward prediction loss: 0.0000
                          CMT loss: 0.0096
                    Surrogate loss: 0.0040
                    Policy entropy: 10.3248
             Mean action noise std: 0.57
                       Mean reward: 9.01
               Mean episode length: 823.96
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0762
       Mean episode rew_ang_vel_xy: -0.0553
        Mean episode rew_collision: -0.0146
          Mean episode rew_dof_acc: -0.0965
   Mean episode rew_dof_pos_limits: -0.0213
        Mean episode rew_lin_vel_z: -0.0216
          Mean episode rew_torques: -0.0260
 Mean episode rew_tracking_ang_vel: 0.2607
 Mean episode rew_tracking_lin_vel: 0.4880
            Mean episode rew_total: 0.4372
        Mean episode terrain_level: 0.6122
    Mean episode min_command_x_vel: -0.9989
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4994
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4995
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 2.39s
                        Total time: 749.16s
                               ETA: 22811.7s

################################################################################
                     [1m Learning iteration 318/10000 [0m                     

                       Computation: 39883 steps/s (collection: 2.099s, learning 0.365s)
               Value function loss: 0.0071
           Forward prediction loss: 0.0000
                          CMT loss: 0.0097
                    Surrogate loss: 0.0040
                    Policy entropy: 10.3244
             Mean action noise std: 0.57
                       Mean reward: 9.12
               Mean episode length: 898.78
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0836
       Mean episode rew_ang_vel_xy: -0.0609
        Mean episode rew_collision: -0.0145
          Mean episode rew_dof_acc: -0.1116
   Mean episode rew_dof_pos_limits: -0.0203
        Mean episode rew_lin_vel_z: -0.0226
          Mean episode rew_torques: -0.0287
 Mean episode rew_tracking_ang_vel: 0.2734
 Mean episode rew_tracking_lin_vel: 0.5069
            Mean episode rew_total: 0.4381
        Mean episode terrain_level: 0.6151
    Mean episode min_command_x_vel: -0.9989
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4994
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4995
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 2.46s
                        Total time: 751.63s
                               ETA: 22812.7s

################################################################################
                     [1m Learning iteration 319/10000 [0m                     

                       Computation: 39901 steps/s (collection: 2.098s, learning 0.366s)
               Value function loss: 0.0073
           Forward prediction loss: 0.0000
                          CMT loss: 0.0097
                    Surrogate loss: 0.0039
                    Policy entropy: 10.3238
             Mean action noise std: 0.57
                       Mean reward: 9.30
               Mean episode length: 882.71
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0813
       Mean episode rew_ang_vel_xy: -0.0598
        Mean episode rew_collision: -0.0152
          Mean episode rew_dof_acc: -0.1063
   Mean episode rew_dof_pos_limits: -0.0210
        Mean episode rew_lin_vel_z: -0.0218
          Mean episode rew_torques: -0.0282
 Mean episode rew_tracking_ang_vel: 0.2704
 Mean episode rew_tracking_lin_vel: 0.5360
            Mean episode rew_total: 0.4727
        Mean episode terrain_level: 0.6183
    Mean episode min_command_x_vel: -0.9989
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4994
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4996
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 2.46s
                        Total time: 754.09s
                               ETA: 22813.6s

################################################################################
                     [1m Learning iteration 320/10000 [0m                     

                       Computation: 40552 steps/s (collection: 2.062s, learning 0.362s)
               Value function loss: 0.0063
           Forward prediction loss: 0.0000
                          CMT loss: 0.0097
                    Surrogate loss: 0.0042
                    Policy entropy: 10.3233
             Mean action noise std: 0.57
                       Mean reward: 10.21
               Mean episode length: 903.54
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0834
       Mean episode rew_ang_vel_xy: -0.0616
        Mean episode rew_collision: -0.0094
          Mean episode rew_dof_acc: -0.1115
   Mean episode rew_dof_pos_limits: -0.0201
        Mean episode rew_lin_vel_z: -0.0221
          Mean episode rew_torques: -0.0290
 Mean episode rew_tracking_ang_vel: 0.2706
 Mean episode rew_tracking_lin_vel: 0.5547
            Mean episode rew_total: 0.4882
        Mean episode terrain_level: 0.6194
    Mean episode min_command_x_vel: -0.9989
    Mean episode max_command_x_vel: 0.5999
    Mean episode min_command_y_vel: -0.5000
    Mean episode max_command_y_vel: 0.4994
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4996
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 2.42s
                        Total time: 756.51s
                               ETA: 22813.3s

################################################################################
                     [1m Learning iteration 321/10000 [0m                     

                       Computation: 40621 steps/s (collection: 2.057s, learning 0.363s)
               Value function loss: 0.0078
           Forward prediction loss: 0.0000
                          CMT loss: 0.0101
                    Surrogate loss: 0.0041
                    Policy entropy: 10.3225
             Mean action noise std: 0.57
                       Mean reward: 8.93
               Mean episode length: 875.89
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0833
       Mean episode rew_ang_vel_xy: -0.0613
        Mean episode rew_collision: -0.0123
          Mean episode rew_dof_acc: -0.1120
   Mean episode rew_dof_pos_limits: -0.0209
        Mean episode rew_lin_vel_z: -0.0223
          Mean episode rew_torques: -0.0290
 Mean episode rew_tracking_ang_vel: 0.2711
 Mean episode rew_tracking_lin_vel: 0.5081
            Mean episode rew_total: 0.4381
        Mean episode terrain_level: 0.6234
    Mean episode min_command_x_vel: -0.9989
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4999
    Mean episode max_command_y_vel: 0.4994
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4996
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 2.42s
                        Total time: 758.93s
                               ETA: 22812.8s

################################################################################
                     [1m Learning iteration 322/10000 [0m                     

                       Computation: 39305 steps/s (collection: 2.134s, learning 0.367s)
               Value function loss: 0.0068
           Forward prediction loss: 0.0000
                          CMT loss: 0.0099
                    Surrogate loss: 0.0039
                    Policy entropy: 10.3217
             Mean action noise std: 0.57
                       Mean reward: 8.49
               Mean episode length: 878.68
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0853
       Mean episode rew_ang_vel_xy: -0.0637
        Mean episode rew_collision: -0.0225
          Mean episode rew_dof_acc: -0.1158
   Mean episode rew_dof_pos_limits: -0.0210
        Mean episode rew_lin_vel_z: -0.0231
          Mean episode rew_torques: -0.0297
 Mean episode rew_tracking_ang_vel: 0.2777
 Mean episode rew_tracking_lin_vel: 0.5335
            Mean episode rew_total: 0.4500
        Mean episode terrain_level: 0.6267
    Mean episode min_command_x_vel: -0.9991
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4994
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4996
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 2.50s
                        Total time: 761.44s
                               ETA: 22814.8s

################################################################################
                     [1m Learning iteration 323/10000 [0m                     

                       Computation: 41174 steps/s (collection: 2.024s, learning 0.364s)
               Value function loss: 0.0068
           Forward prediction loss: 0.0000
                          CMT loss: 0.0099
                    Surrogate loss: 0.0035
                    Policy entropy: 10.3207
             Mean action noise std: 0.57
                       Mean reward: 9.19
               Mean episode length: 856.81
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0802
       Mean episode rew_ang_vel_xy: -0.0601
        Mean episode rew_collision: -0.0144
          Mean episode rew_dof_acc: -0.1084
   Mean episode rew_dof_pos_limits: -0.0210
        Mean episode rew_lin_vel_z: -0.0228
          Mean episode rew_torques: -0.0272
 Mean episode rew_tracking_ang_vel: 0.2655
 Mean episode rew_tracking_lin_vel: 0.5235
            Mean episode rew_total: 0.4550
        Mean episode terrain_level: 0.6299
    Mean episode min_command_x_vel: -0.9994
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4994
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4996
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 2.39s
                        Total time: 763.82s
                               ETA: 22813.3s

################################################################################
                     [1m Learning iteration 324/10000 [0m                     

                       Computation: 40905 steps/s (collection: 2.040s, learning 0.363s)
               Value function loss: 0.0062
           Forward prediction loss: 0.0000
                          CMT loss: 0.0098
                    Surrogate loss: 0.0037
                    Policy entropy: 10.3197
             Mean action noise std: 0.57
                       Mean reward: 10.24
               Mean episode length: 899.77
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0831
       Mean episode rew_ang_vel_xy: -0.0590
        Mean episode rew_collision: -0.0103
          Mean episode rew_dof_acc: -0.1058
   Mean episode rew_dof_pos_limits: -0.0235
        Mean episode rew_lin_vel_z: -0.0217
          Mean episode rew_torques: -0.0282
 Mean episode rew_tracking_ang_vel: 0.2848
 Mean episode rew_tracking_lin_vel: 0.5551
            Mean episode rew_total: 0.5083
        Mean episode terrain_level: 0.6310
    Mean episode min_command_x_vel: -0.9994
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4994
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4996
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 2.40s
                        Total time: 766.23s
                               ETA: 22812.3s

################################################################################
                     [1m Learning iteration 325/10000 [0m                     

                       Computation: 40688 steps/s (collection: 2.051s, learning 0.365s)
               Value function loss: 0.0065
           Forward prediction loss: 0.0000
                          CMT loss: 0.0100
                    Surrogate loss: 0.0039
                    Policy entropy: 10.3183
             Mean action noise std: 0.57
                       Mean reward: 8.82
               Mean episode length: 841.72
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0797
       Mean episode rew_ang_vel_xy: -0.0590
        Mean episode rew_collision: -0.0129
          Mean episode rew_dof_acc: -0.1093
   Mean episode rew_dof_pos_limits: -0.0205
        Mean episode rew_lin_vel_z: -0.0221
          Mean episode rew_torques: -0.0275
 Mean episode rew_tracking_ang_vel: 0.2666
 Mean episode rew_tracking_lin_vel: 0.5184
            Mean episode rew_total: 0.4541
        Mean episode terrain_level: 0.6335
    Mean episode min_command_x_vel: -0.9994
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4996
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 2.42s
                        Total time: 768.64s
                               ETA: 22811.7s

################################################################################
                     [1m Learning iteration 326/10000 [0m                     

                       Computation: 40460 steps/s (collection: 2.068s, learning 0.362s)
               Value function loss: 0.0064
           Forward prediction loss: 0.0000
                          CMT loss: 0.0100
                    Surrogate loss: 0.0036
                    Policy entropy: 10.3170
             Mean action noise std: 0.57
                       Mean reward: 10.01
               Mean episode length: 877.31
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0824
       Mean episode rew_ang_vel_xy: -0.0603
        Mean episode rew_collision: -0.0169
          Mean episode rew_dof_acc: -0.1084
   Mean episode rew_dof_pos_limits: -0.0221
        Mean episode rew_lin_vel_z: -0.0223
          Mean episode rew_torques: -0.0290
 Mean episode rew_tracking_ang_vel: 0.2736
 Mean episode rew_tracking_lin_vel: 0.5584
            Mean episode rew_total: 0.4906
        Mean episode terrain_level: 0.6347
    Mean episode min_command_x_vel: -0.9994
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4996
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4996
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 2.43s
                        Total time: 771.07s
                               ETA: 22811.5s

################################################################################
                     [1m Learning iteration 327/10000 [0m                     

                       Computation: 41995 steps/s (collection: 1.979s, learning 0.362s)
               Value function loss: 0.0070
           Forward prediction loss: 0.0000
                          CMT loss: 0.0100
                    Surrogate loss: 0.0033
                    Policy entropy: 10.3159
             Mean action noise std: 0.57
                       Mean reward: 10.31
               Mean episode length: 896.20
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0830
       Mean episode rew_ang_vel_xy: -0.0592
        Mean episode rew_collision: -0.0102
          Mean episode rew_dof_acc: -0.1053
   Mean episode rew_dof_pos_limits: -0.0233
        Mean episode rew_lin_vel_z: -0.0218
          Mean episode rew_torques: -0.0280
 Mean episode rew_tracking_ang_vel: 0.2937
 Mean episode rew_tracking_lin_vel: 0.5499
            Mean episode rew_total: 0.5129
        Mean episode terrain_level: 0.6366
    Mean episode min_command_x_vel: -0.9994
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4996
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 2.34s
                        Total time: 773.41s
                               ETA: 22808.6s

################################################################################
                     [1m Learning iteration 328/10000 [0m                     

                       Computation: 41098 steps/s (collection: 2.027s, learning 0.364s)
               Value function loss: 0.0069
           Forward prediction loss: 0.0000
                          CMT loss: 0.0100
                    Surrogate loss: 0.0038
                    Policy entropy: 10.3152
             Mean action noise std: 0.57
                       Mean reward: 9.31
               Mean episode length: 866.43
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0826
       Mean episode rew_ang_vel_xy: -0.0590
        Mean episode rew_collision: -0.0154
          Mean episode rew_dof_acc: -0.1102
   Mean episode rew_dof_pos_limits: -0.0217
        Mean episode rew_lin_vel_z: -0.0222
          Mean episode rew_torques: -0.0282
 Mean episode rew_tracking_ang_vel: 0.2701
 Mean episode rew_tracking_lin_vel: 0.5442
            Mean episode rew_total: 0.4750
        Mean episode terrain_level: 0.6390
    Mean episode min_command_x_vel: -0.9994
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4996
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 2.39s
                        Total time: 775.80s
                               ETA: 22807.2s

################################################################################
                     [1m Learning iteration 329/10000 [0m                     

                       Computation: 40614 steps/s (collection: 2.057s, learning 0.364s)
               Value function loss: 0.0076
           Forward prediction loss: 0.0000
                          CMT loss: 0.0100
                    Surrogate loss: 0.0030
                    Policy entropy: 10.3145
             Mean action noise std: 0.57
                       Mean reward: 9.35
               Mean episode length: 874.99
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0817
       Mean episode rew_ang_vel_xy: -0.0601
        Mean episode rew_collision: -0.0115
          Mean episode rew_dof_acc: -0.1097
   Mean episode rew_dof_pos_limits: -0.0212
        Mean episode rew_lin_vel_z: -0.0223
          Mean episode rew_torques: -0.0279
 Mean episode rew_tracking_ang_vel: 0.2696
 Mean episode rew_tracking_lin_vel: 0.5395
            Mean episode rew_total: 0.4748
        Mean episode terrain_level: 0.6416
    Mean episode min_command_x_vel: -0.9994
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4996
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 2.42s
                        Total time: 778.22s
                               ETA: 22806.7s

################################################################################
                     [1m Learning iteration 330/10000 [0m                     

                       Computation: 41339 steps/s (collection: 2.015s, learning 0.363s)
               Value function loss: 0.0064
           Forward prediction loss: 0.0000
                          CMT loss: 0.0102
                    Surrogate loss: 0.0062
                    Policy entropy: 10.3142
             Mean action noise std: 0.57
                       Mean reward: 8.98
               Mean episode length: 870.88
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0766
       Mean episode rew_ang_vel_xy: -0.0557
        Mean episode rew_collision: -0.0195
          Mean episode rew_dof_acc: -0.1034
   Mean episode rew_dof_pos_limits: -0.0212
        Mean episode rew_lin_vel_z: -0.0215
          Mean episode rew_torques: -0.0260
 Mean episode rew_tracking_ang_vel: 0.2550
 Mean episode rew_tracking_lin_vel: 0.4861
            Mean episode rew_total: 0.4172
        Mean episode terrain_level: 0.6465
    Mean episode min_command_x_vel: -0.9996
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4997
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4996
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 2.38s
                        Total time: 780.60s
                               ETA: 22804.9s

################################################################################
                     [1m Learning iteration 331/10000 [0m                     

                       Computation: 40503 steps/s (collection: 2.059s, learning 0.368s)
               Value function loss: 0.0080
           Forward prediction loss: 0.0000
                          CMT loss: 0.0102
                    Surrogate loss: 0.0043
                    Policy entropy: 10.3136
             Mean action noise std: 0.57
                       Mean reward: 7.65
               Mean episode length: 775.63
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0724
       Mean episode rew_ang_vel_xy: -0.0538
        Mean episode rew_collision: -0.0164
          Mean episode rew_dof_acc: -0.0981
   Mean episode rew_dof_pos_limits: -0.0197
        Mean episode rew_lin_vel_z: -0.0209
          Mean episode rew_torques: -0.0245
 Mean episode rew_tracking_ang_vel: 0.2374
 Mean episode rew_tracking_lin_vel: 0.4521
            Mean episode rew_total: 0.3837
        Mean episode terrain_level: 0.6509
    Mean episode min_command_x_vel: -0.9996
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4998
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4996
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 2.43s
                        Total time: 783.03s
                               ETA: 22804.6s

################################################################################
                     [1m Learning iteration 332/10000 [0m                     

                       Computation: 40320 steps/s (collection: 2.070s, learning 0.368s)
               Value function loss: 0.0076
           Forward prediction loss: 0.0000
                          CMT loss: 0.0101
                    Surrogate loss: 0.0029
                    Policy entropy: 10.3135
             Mean action noise std: 0.57
                       Mean reward: 8.30
               Mean episode length: 803.50
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0763
       Mean episode rew_ang_vel_xy: -0.0560
        Mean episode rew_collision: -0.0071
          Mean episode rew_dof_acc: -0.1070
   Mean episode rew_dof_pos_limits: -0.0198
        Mean episode rew_lin_vel_z: -0.0215
          Mean episode rew_torques: -0.0263
 Mean episode rew_tracking_ang_vel: 0.2465
 Mean episode rew_tracking_lin_vel: 0.4727
            Mean episode rew_total: 0.4053
        Mean episode terrain_level: 0.6543
    Mean episode min_command_x_vel: -0.9996
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4996
         Mean episode command_area: 0.7733
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 2.44s
                        Total time: 785.47s
                               ETA: 22804.5s

################################################################################
                     [1m Learning iteration 333/10000 [0m                     

                       Computation: 37558 steps/s (collection: 2.255s, learning 0.362s)
               Value function loss: 0.0123
           Forward prediction loss: 0.0000
                          CMT loss: 0.0100
                    Surrogate loss: 0.0037
                    Policy entropy: 10.3133
             Mean action noise std: 0.57
                       Mean reward: 11.05
               Mean episode length: 994.42
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0852
       Mean episode rew_ang_vel_xy: -0.0637
        Mean episode rew_collision: -0.0159
          Mean episode rew_dof_acc: -0.1198
   Mean episode rew_dof_pos_limits: -0.0216
        Mean episode rew_lin_vel_z: -0.0229
          Mean episode rew_torques: -0.0299
 Mean episode rew_tracking_ang_vel: 0.2768
 Mean episode rew_tracking_lin_vel: 0.5384
            Mean episode rew_total: 0.4563
        Mean episode terrain_level: 0.6589
    Mean episode min_command_x_vel: -0.9996
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.5000
  Mean episode max_command_yaw_vel: 0.4996
         Mean episode command_area: 0.7789
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 2.62s
                        Total time: 788.08s
                               ETA: 22809.6s

################################################################################
                     [1m Learning iteration 334/10000 [0m                     

                       Computation: 40637 steps/s (collection: 2.057s, learning 0.362s)
               Value function loss: 0.0072
           Forward prediction loss: 0.0000
                          CMT loss: 0.0106
                    Surrogate loss: 0.0064
                    Policy entropy: 10.3132
             Mean action noise std: 0.57
                       Mean reward: 8.76
               Mean episode length: 833.07
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0809
       Mean episode rew_ang_vel_xy: -0.0590
        Mean episode rew_collision: -0.0100
          Mean episode rew_dof_acc: -0.1097
   Mean episode rew_dof_pos_limits: -0.0218
        Mean episode rew_lin_vel_z: -0.0223
          Mean episode rew_torques: -0.0276
 Mean episode rew_tracking_ang_vel: 0.2675
 Mean episode rew_tracking_lin_vel: 0.5210
            Mean episode rew_total: 0.4572
        Mean episode terrain_level: 0.6635
    Mean episode min_command_x_vel: -0.9996
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4996
         Mean episode command_area: 0.7867
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 2.42s
                        Total time: 790.50s
                               ETA: 22809.0s

################################################################################
                     [1m Learning iteration 335/10000 [0m                     

                       Computation: 39844 steps/s (collection: 2.098s, learning 0.370s)
               Value function loss: 0.0060
           Forward prediction loss: 0.0000
                          CMT loss: 0.0098
                    Surrogate loss: 0.0043
                    Policy entropy: 10.3124
             Mean action noise std: 0.57
                       Mean reward: 8.79
               Mean episode length: 843.29
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0817
       Mean episode rew_ang_vel_xy: -0.0614
        Mean episode rew_collision: -0.0105
          Mean episode rew_dof_acc: -0.1190
   Mean episode rew_dof_pos_limits: -0.0203
        Mean episode rew_lin_vel_z: -0.0225
          Mean episode rew_torques: -0.0288
 Mean episode rew_tracking_ang_vel: 0.2555
 Mean episode rew_tracking_lin_vel: 0.5189
            Mean episode rew_total: 0.4302
        Mean episode terrain_level: 0.6672
    Mean episode min_command_x_vel: -0.9996
    Mean episode max_command_x_vel: 0.5994
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4996
         Mean episode command_area: 0.7867
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 2.47s
                        Total time: 792.97s
                               ETA: 22809.7s

################################################################################
                     [1m Learning iteration 336/10000 [0m                     

                       Computation: 39619 steps/s (collection: 2.119s, learning 0.363s)
               Value function loss: 0.0075
           Forward prediction loss: 0.0000
                          CMT loss: 0.0098
                    Surrogate loss: 0.0037
                    Policy entropy: 10.3116
             Mean action noise std: 0.57
                       Mean reward: 9.74
               Mean episode length: 904.92
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0880
       Mean episode rew_ang_vel_xy: -0.0656
        Mean episode rew_collision: -0.0089
          Mean episode rew_dof_acc: -0.1249
   Mean episode rew_dof_pos_limits: -0.0216
        Mean episode rew_lin_vel_z: -0.0235
          Mean episode rew_torques: -0.0304
 Mean episode rew_tracking_ang_vel: 0.2844
 Mean episode rew_tracking_lin_vel: 0.5617
            Mean episode rew_total: 0.4832
        Mean episode terrain_level: 0.6726
    Mean episode min_command_x_vel: -0.9996
    Mean episode max_command_x_vel: 0.5993
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4996
         Mean episode command_area: 0.7867
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 2.48s
                        Total time: 795.45s
                               ETA: 22810.8s

################################################################################
                     [1m Learning iteration 337/10000 [0m                     

                       Computation: 40144 steps/s (collection: 2.082s, learning 0.367s)
               Value function loss: 0.0068
           Forward prediction loss: 0.0000
                          CMT loss: 0.0097
                    Surrogate loss: 0.0025
                    Policy entropy: 10.3112
             Mean action noise std: 0.57
                       Mean reward: 9.65
               Mean episode length: 883.00
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0805
       Mean episode rew_ang_vel_xy: -0.0583
        Mean episode rew_collision: -0.0104
          Mean episode rew_dof_acc: -0.1063
   Mean episode rew_dof_pos_limits: -0.0230
        Mean episode rew_lin_vel_z: -0.0220
          Mean episode rew_torques: -0.0268
 Mean episode rew_tracking_ang_vel: 0.2780
 Mean episode rew_tracking_lin_vel: 0.5417
            Mean episode rew_total: 0.4925
        Mean episode terrain_level: 0.6770
    Mean episode min_command_x_vel: -0.9996
    Mean episode max_command_x_vel: 0.5993
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4996
         Mean episode command_area: 0.7867
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 2.45s
                        Total time: 797.90s
                               ETA: 22811.0s

################################################################################
                     [1m Learning iteration 338/10000 [0m                     

                       Computation: 39878 steps/s (collection: 2.105s, learning 0.360s)
               Value function loss: 0.0064
           Forward prediction loss: 0.0000
                          CMT loss: 0.0098
                    Surrogate loss: 0.0037
                    Policy entropy: 10.3103
             Mean action noise std: 0.57
                       Mean reward: 8.94
               Mean episode length: 832.46
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0803
       Mean episode rew_ang_vel_xy: -0.0578
        Mean episode rew_collision: -0.0090
          Mean episode rew_dof_acc: -0.1070
   Mean episode rew_dof_pos_limits: -0.0227
        Mean episode rew_lin_vel_z: -0.0219
          Mean episode rew_torques: -0.0274
 Mean episode rew_tracking_ang_vel: 0.2654
 Mean episode rew_tracking_lin_vel: 0.5197
            Mean episode rew_total: 0.4590
        Mean episode terrain_level: 0.6784
    Mean episode min_command_x_vel: -0.9997
    Mean episode max_command_x_vel: 0.5993
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4996
         Mean episode command_area: 0.7867
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 2.47s
                        Total time: 800.37s
                               ETA: 22811.6s

################################################################################
                     [1m Learning iteration 339/10000 [0m                     

                       Computation: 40735 steps/s (collection: 2.046s, learning 0.367s)
               Value function loss: 0.0075
           Forward prediction loss: 0.0000
                          CMT loss: 0.0099
                    Surrogate loss: 0.0040
                    Policy entropy: 10.3097
             Mean action noise std: 0.57
                       Mean reward: 8.79
               Mean episode length: 819.92
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0728
       Mean episode rew_ang_vel_xy: -0.0520
        Mean episode rew_collision: -0.0109
          Mean episode rew_dof_acc: -0.0953
   Mean episode rew_dof_pos_limits: -0.0213
        Mean episode rew_lin_vel_z: -0.0202
          Mean episode rew_torques: -0.0243
 Mean episode rew_tracking_ang_vel: 0.2533
 Mean episode rew_tracking_lin_vel: 0.4718
            Mean episode rew_total: 0.4282
        Mean episode terrain_level: 0.6797
    Mean episode min_command_x_vel: -0.9997
    Mean episode max_command_x_vel: 0.5993
    Mean episode min_command_y_vel: -0.4998
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4996
         Mean episode command_area: 0.7867
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 2.41s
                        Total time: 802.78s
                               ETA: 22810.7s

################################################################################
                     [1m Learning iteration 340/10000 [0m                     

                       Computation: 40588 steps/s (collection: 2.056s, learning 0.366s)
               Value function loss: 0.0068
           Forward prediction loss: 0.0000
                          CMT loss: 0.0100
                    Surrogate loss: 0.0036
                    Policy entropy: 10.3090
             Mean action noise std: 0.57
                       Mean reward: 9.04
               Mean episode length: 854.03
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0839
       Mean episode rew_ang_vel_xy: -0.0643
        Mean episode rew_collision: -0.0206
          Mean episode rew_dof_acc: -0.1222
   Mean episode rew_dof_pos_limits: -0.0208
        Mean episode rew_lin_vel_z: -0.0230
          Mean episode rew_torques: -0.0293
 Mean episode rew_tracking_ang_vel: 0.2663
 Mean episode rew_tracking_lin_vel: 0.5524
            Mean episode rew_total: 0.4546
        Mean episode terrain_level: 0.6826
    Mean episode min_command_x_vel: -0.9997
    Mean episode max_command_x_vel: 0.5993
    Mean episode min_command_y_vel: -0.4997
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4996
         Mean episode command_area: 0.7867
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 2.42s
                        Total time: 805.20s
                               ETA: 22810.1s

################################################################################
                     [1m Learning iteration 341/10000 [0m                     

                       Computation: 40739 steps/s (collection: 2.049s, learning 0.364s)
               Value function loss: 0.0064
           Forward prediction loss: 0.0000
                          CMT loss: 0.0098
                    Surrogate loss: 0.0043
                    Policy entropy: 10.3084
             Mean action noise std: 0.57
                       Mean reward: 9.41
               Mean episode length: 868.58
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0755
       Mean episode rew_ang_vel_xy: -0.0558
        Mean episode rew_collision: -0.0082
          Mean episode rew_dof_acc: -0.1031
   Mean episode rew_dof_pos_limits: -0.0209
        Mean episode rew_lin_vel_z: -0.0215
          Mean episode rew_torques: -0.0254
 Mean episode rew_tracking_ang_vel: 0.2518
 Mean episode rew_tracking_lin_vel: 0.4965
            Mean episode rew_total: 0.4379
        Mean episode terrain_level: 0.6866
    Mean episode min_command_x_vel: -0.9997
    Mean episode max_command_x_vel: 0.5989
    Mean episode min_command_y_vel: -0.4996
    Mean episode max_command_y_vel: 0.4999
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4996
         Mean episode command_area: 0.7867
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 2.41s
                        Total time: 807.61s
                               ETA: 22809.2s

################################################################################
                     [1m Learning iteration 342/10000 [0m                     

                       Computation: 41074 steps/s (collection: 2.030s, learning 0.363s)
               Value function loss: 0.0081
           Forward prediction loss: 0.0000
                          CMT loss: 0.0099
                    Surrogate loss: 0.0032
                    Policy entropy: 10.3081
             Mean action noise std: 0.57
                       Mean reward: 9.52
               Mean episode length: 862.40
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0795
       Mean episode rew_ang_vel_xy: -0.0573
        Mean episode rew_collision: -0.0207
          Mean episode rew_dof_acc: -0.1047
   Mean episode rew_dof_pos_limits: -0.0226
        Mean episode rew_lin_vel_z: -0.0214
          Mean episode rew_torques: -0.0266
 Mean episode rew_tracking_ang_vel: 0.2734
 Mean episode rew_tracking_lin_vel: 0.5414
            Mean episode rew_total: 0.4820
        Mean episode terrain_level: 0.6901
    Mean episode min_command_x_vel: -0.9997
    Mean episode max_command_x_vel: 0.5978
    Mean episode min_command_y_vel: -0.4996
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4996
         Mean episode command_area: 0.7867
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 2.39s
                        Total time: 810.01s
                               ETA: 22807.7s

################################################################################
                     [1m Learning iteration 343/10000 [0m                     

                       Computation: 41719 steps/s (collection: 1.992s, learning 0.364s)
               Value function loss: 0.0063
           Forward prediction loss: 0.0000
                          CMT loss: 0.0099
                    Surrogate loss: 0.0036
                    Policy entropy: 10.3077
             Mean action noise std: 0.57
                       Mean reward: 8.91
               Mean episode length: 868.34
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0806
       Mean episode rew_ang_vel_xy: -0.0587
        Mean episode rew_collision: -0.0188
          Mean episode rew_dof_acc: -0.1063
   Mean episode rew_dof_pos_limits: -0.0237
        Mean episode rew_lin_vel_z: -0.0219
          Mean episode rew_torques: -0.0276
 Mean episode rew_tracking_ang_vel: 0.2727
 Mean episode rew_tracking_lin_vel: 0.5445
            Mean episode rew_total: 0.4797
        Mean episode terrain_level: 0.6912
    Mean episode min_command_x_vel: -0.9998
    Mean episode max_command_x_vel: 0.5979
    Mean episode min_command_y_vel: -0.4996
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4996
         Mean episode command_area: 0.7867
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 2.36s
                        Total time: 812.36s
                               ETA: 22805.2s

################################################################################
                     [1m Learning iteration 344/10000 [0m                     

                       Computation: 41253 steps/s (collection: 2.020s, learning 0.363s)
               Value function loss: 0.0069
           Forward prediction loss: 0.0000
                          CMT loss: 0.0100
                    Surrogate loss: 0.0030
                    Policy entropy: 10.3069
             Mean action noise std: 0.57
                       Mean reward: 8.46
               Mean episode length: 812.00
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0784
       Mean episode rew_ang_vel_xy: -0.0587
        Mean episode rew_collision: -0.0119
          Mean episode rew_dof_acc: -0.1121
   Mean episode rew_dof_pos_limits: -0.0208
        Mean episode rew_lin_vel_z: -0.0220
          Mean episode rew_torques: -0.0276
 Mean episode rew_tracking_ang_vel: 0.2471
 Mean episode rew_tracking_lin_vel: 0.4988
            Mean episode rew_total: 0.4144
        Mean episode terrain_level: 0.6932
    Mean episode min_command_x_vel: -0.9998
    Mean episode max_command_x_vel: 0.5980
    Mean episode min_command_y_vel: -0.4996
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4996
         Mean episode command_area: 0.7867
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 2.38s
                        Total time: 814.75s
                               ETA: 22803.5s

################################################################################
                     [1m Learning iteration 345/10000 [0m                     

                       Computation: 41536 steps/s (collection: 2.004s, learning 0.362s)
               Value function loss: 0.0072
           Forward prediction loss: 0.0000
                          CMT loss: 0.0100
                    Surrogate loss: 0.0042
                    Policy entropy: 10.3062
             Mean action noise std: 0.57
                       Mean reward: 8.76
               Mean episode length: 826.95
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0800
       Mean episode rew_ang_vel_xy: -0.0594
        Mean episode rew_collision: -0.0139
          Mean episode rew_dof_acc: -0.1112
   Mean episode rew_dof_pos_limits: -0.0211
        Mean episode rew_lin_vel_z: -0.0218
          Mean episode rew_torques: -0.0276
 Mean episode rew_tracking_ang_vel: 0.2601
 Mean episode rew_tracking_lin_vel: 0.5371
            Mean episode rew_total: 0.4623
        Mean episode terrain_level: 0.6973
    Mean episode min_command_x_vel: -0.9998
    Mean episode max_command_x_vel: 0.5988
    Mean episode min_command_y_vel: -0.4996
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4997
         Mean episode command_area: 0.7867
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 2.37s
                        Total time: 817.11s
                               ETA: 22801.2s

################################################################################
                     [1m Learning iteration 346/10000 [0m                     

                       Computation: 41833 steps/s (collection: 1.987s, learning 0.363s)
               Value function loss: 0.0067
           Forward prediction loss: 0.0000
                          CMT loss: 0.0100
                    Surrogate loss: 0.0035
                    Policy entropy: 10.3055
             Mean action noise std: 0.57
                       Mean reward: 8.98
               Mean episode length: 852.95
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0777
       Mean episode rew_ang_vel_xy: -0.0554
        Mean episode rew_collision: -0.0154
          Mean episode rew_dof_acc: -0.1024
   Mean episode rew_dof_pos_limits: -0.0230
        Mean episode rew_lin_vel_z: -0.0213
          Mean episode rew_torques: -0.0262
 Mean episode rew_tracking_ang_vel: 0.2612
 Mean episode rew_tracking_lin_vel: 0.5014
            Mean episode rew_total: 0.4411
        Mean episode terrain_level: 0.6992
    Mean episode min_command_x_vel: -0.9998
    Mean episode max_command_x_vel: 0.5988
    Mean episode min_command_y_vel: -0.4996
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.7867
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 2.35s
                        Total time: 819.46s
                               ETA: 22798.6s

################################################################################
                     [1m Learning iteration 347/10000 [0m                     

                       Computation: 40217 steps/s (collection: 2.079s, learning 0.365s)
               Value function loss: 0.0066
           Forward prediction loss: 0.0000
                          CMT loss: 0.0101
                    Surrogate loss: 0.0039
                    Policy entropy: 10.3046
             Mean action noise std: 0.57
                       Mean reward: 8.25
               Mean episode length: 808.04
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0776
       Mean episode rew_ang_vel_xy: -0.0554
        Mean episode rew_collision: -0.0129
          Mean episode rew_dof_acc: -0.1032
   Mean episode rew_dof_pos_limits: -0.0230
        Mean episode rew_lin_vel_z: -0.0215
          Mean episode rew_torques: -0.0259
 Mean episode rew_tracking_ang_vel: 0.2644
 Mean episode rew_tracking_lin_vel: 0.4897
            Mean episode rew_total: 0.4346
        Mean episode terrain_level: 0.7000
    Mean episode min_command_x_vel: -0.9998
    Mean episode max_command_x_vel: 0.5997
    Mean episode min_command_y_vel: -0.4996
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.7867
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 2.44s
                        Total time: 821.91s
                               ETA: 22798.5s

################################################################################
                     [1m Learning iteration 348/10000 [0m                     

                       Computation: 41033 steps/s (collection: 2.032s, learning 0.364s)
               Value function loss: 0.0067
           Forward prediction loss: 0.0000
                          CMT loss: 0.0102
                    Surrogate loss: 0.0036
                    Policy entropy: 10.3035
             Mean action noise std: 0.57
                       Mean reward: 9.87
               Mean episode length: 878.01
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0812
       Mean episode rew_ang_vel_xy: -0.0587
        Mean episode rew_collision: -0.0096
          Mean episode rew_dof_acc: -0.1108
   Mean episode rew_dof_pos_limits: -0.0223
        Mean episode rew_lin_vel_z: -0.0218
          Mean episode rew_torques: -0.0279
 Mean episode rew_tracking_ang_vel: 0.2718
 Mean episode rew_tracking_lin_vel: 0.5559
            Mean episode rew_total: 0.4954
        Mean episode terrain_level: 0.7030
    Mean episode min_command_x_vel: -0.9998
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.7867
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 2.40s
                        Total time: 824.30s
                               ETA: 22797.1s

################################################################################
                     [1m Learning iteration 349/10000 [0m                     

                       Computation: 41889 steps/s (collection: 1.984s, learning 0.363s)
               Value function loss: 0.0062
           Forward prediction loss: 0.0000
                          CMT loss: 0.0102
                    Surrogate loss: 0.0041
                    Policy entropy: 10.3025
             Mean action noise std: 0.57
                       Mean reward: 9.63
               Mean episode length: 892.26
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0826
       Mean episode rew_ang_vel_xy: -0.0593
        Mean episode rew_collision: -0.0158
          Mean episode rew_dof_acc: -0.1127
   Mean episode rew_dof_pos_limits: -0.0237
        Mean episode rew_lin_vel_z: -0.0217
          Mean episode rew_torques: -0.0281
 Mean episode rew_tracking_ang_vel: 0.2819
 Mean episode rew_tracking_lin_vel: 0.5481
            Mean episode rew_total: 0.4859
        Mean episode terrain_level: 0.7066
    Mean episode min_command_x_vel: -0.9998
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.7867
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 2.35s
                        Total time: 826.65s
                               ETA: 22794.3s

################################################################################
                     [1m Learning iteration 350/10000 [0m                     

                       Computation: 41318 steps/s (collection: 2.016s, learning 0.363s)
               Value function loss: 0.0064
           Forward prediction loss: 0.0000
                          CMT loss: 0.0103
                    Surrogate loss: 0.0041
                    Policy entropy: 10.3009
             Mean action noise std: 0.57
                       Mean reward: 9.75
               Mean episode length: 885.18
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0832
       Mean episode rew_ang_vel_xy: -0.0610
        Mean episode rew_collision: -0.0102
          Mean episode rew_dof_acc: -0.1139
   Mean episode rew_dof_pos_limits: -0.0237
        Mean episode rew_lin_vel_z: -0.0222
          Mean episode rew_torques: -0.0287
 Mean episode rew_tracking_ang_vel: 0.2721
 Mean episode rew_tracking_lin_vel: 0.5522
            Mean episode rew_total: 0.4816
        Mean episode terrain_level: 0.7097
    Mean episode min_command_x_vel: -0.9998
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.7867
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 2.38s
                        Total time: 829.03s
                               ETA: 22792.4s

################################################################################
                     [1m Learning iteration 351/10000 [0m                     

                       Computation: 40243 steps/s (collection: 2.079s, learning 0.363s)
               Value function loss: 0.0069
           Forward prediction loss: 0.0000
                          CMT loss: 0.0103
                    Surrogate loss: 0.0042
                    Policy entropy: 10.2995
             Mean action noise std: 0.57
                       Mean reward: 8.99
               Mean episode length: 842.98
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0786
       Mean episode rew_ang_vel_xy: -0.0586
        Mean episode rew_collision: -0.0117
          Mean episode rew_dof_acc: -0.1114
   Mean episode rew_dof_pos_limits: -0.0208
        Mean episode rew_lin_vel_z: -0.0215
          Mean episode rew_torques: -0.0275
 Mean episode rew_tracking_ang_vel: 0.2559
 Mean episode rew_tracking_lin_vel: 0.5149
            Mean episode rew_total: 0.4406
        Mean episode terrain_level: 0.7131
    Mean episode min_command_x_vel: -0.9998
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.7867
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 2.44s
                        Total time: 831.47s
                               ETA: 22792.3s

Loading extension module gymtorch...
/home/jijingtian/miniconda3/envs/terrain/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if not hasattr(tensorboard, "__version__") or LooseVersion(
/home/jijingtian/project/MetaRobotics/legged_gym/utils/terrain_lib.py:83: DeprecationWarning: `interp2d` is deprecated!
`interp2d` is deprecated in SciPy 1.10 and will be removed in SciPy 1.12.0.

For legacy code, nearly bug-for-bug compatible replacements are
`RectBivariateSpline` on regular grids, and `bisplrep`/`bisplev` for
scattered 2D data.

In new code, for regular grids use `RegularGridInterpolator` instead.
For scattered data, prefer `LinearNDInterpolator` or
`CloughTocher2DInterpolator`.

For more details see
`https://gist.github.com/ev-br/8544371b40f414b7eaf3fe6217209bff`

  f = interpolate.interp2d(y, x, height_field_downsampled, kind='linear')
/home/jijingtian/project/MetaRobotics/legged_gym/utils/terrain_lib.py:87: DeprecationWarning:         `interp2d` is deprecated!
        `interp2d` is deprecated in SciPy 1.10 and will be removed in SciPy 1.12.0.

        For legacy code, nearly bug-for-bug compatible replacements are
        `RectBivariateSpline` on regular grids, and `bisplrep`/`bisplev` for
        scattered 2D data.

        In new code, for regular grids use `RegularGridInterpolator` instead.
        For scattered data, prefer `LinearNDInterpolator` or
        `CloughTocher2DInterpolator`.

        For more details see
        `https://gist.github.com/ev-br/8544371b40f414b7eaf3fe6217209bff`

  z_upsampled = np.rint(f(y_upsampled, x_upsampled))
################################################################################
                     [1m Learning iteration 352/10000 [0m                     

                       Computation: 42248 steps/s (collection: 1.964s, learning 0.363s)
               Value function loss: 0.0078
           Forward prediction loss: 0.0000
                          CMT loss: 0.0104
                    Surrogate loss: 0.0041
                    Policy entropy: 10.2983
             Mean action noise std: 0.57
                       Mean reward: 9.35
               Mean episode length: 877.37
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0822
       Mean episode rew_ang_vel_xy: -0.0589
        Mean episode rew_collision: -0.0170
          Mean episode rew_dof_acc: -0.1130
   Mean episode rew_dof_pos_limits: -0.0223
        Mean episode rew_lin_vel_z: -0.0219
          Mean episode rew_torques: -0.0289
 Mean episode rew_tracking_ang_vel: 0.2689
 Mean episode rew_tracking_lin_vel: 0.5425
            Mean episode rew_total: 0.4671
        Mean episode terrain_level: 0.7193
    Mean episode min_command_x_vel: -0.9998
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.7867
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 2.33s
                        Total time: 833.80s
                               ETA: 22788.9s

################################################################################
                     [1m Learning iteration 353/10000 [0m                     

                       Computation: 41043 steps/s (collection: 2.032s, learning 0.363s)
               Value function loss: 0.0076
           Forward prediction loss: 0.0000
                          CMT loss: 0.0103
                    Surrogate loss: 0.0032
                    Policy entropy: 10.2969
             Mean action noise std: 0.57
                       Mean reward: 8.97
               Mean episode length: 832.74
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0784
       Mean episode rew_ang_vel_xy: -0.0573
        Mean episode rew_collision: -0.0127
          Mean episode rew_dof_acc: -0.1070
   Mean episode rew_dof_pos_limits: -0.0227
        Mean episode rew_lin_vel_z: -0.0216
          Mean episode rew_torques: -0.0272
 Mean episode rew_tracking_ang_vel: 0.2643
 Mean episode rew_tracking_lin_vel: 0.5114
            Mean episode rew_total: 0.4489
        Mean episode terrain_level: 0.7236
    Mean episode min_command_x_vel: -0.9998
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.7867
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 2.40s
                        Total time: 836.19s
                               ETA: 22787.5s

################################################################################
                     [1m Learning iteration 354/10000 [0m                     

                       Computation: 40301 steps/s (collection: 2.076s, learning 0.363s)
               Value function loss: 0.0122
           Forward prediction loss: 0.0000
                          CMT loss: 0.0102
                    Surrogate loss: 0.0033
                    Policy entropy: 10.2960
             Mean action noise std: 0.57
                       Mean reward: 8.90
               Mean episode length: 836.96
        Mean episode max_command_x: 1.0000
      Mean episode rew_action_rate: -0.0777
       Mean episode rew_ang_vel_xy: -0.0572
        Mean episode rew_collision: -0.0452
          Mean episode rew_dof_acc: -0.1070
   Mean episode rew_dof_pos_limits: -0.0222
        Mean episode rew_lin_vel_z: -0.0212
          Mean episode rew_torques: -0.0269
 Mean episode rew_tracking_ang_vel: 0.2600
 Mean episode rew_tracking_lin_vel: 0.5509
            Mean episode rew_total: 0.4536
        Mean episode terrain_level: 0.7277
    Mean episode min_command_x_vel: -0.9998
    Mean episode max_command_x_vel: 0.5998
    Mean episode min_command_y_vel: -0.4995
    Mean episode max_command_y_vel: 0.5000
  Mean episode min_command_yaw_vel: -0.4999
  Mean episode max_command_yaw_vel: 0.4999
         Mean episode command_area: 0.7867
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 2.44s
                        Total time: 838.63s
                               ETA: 22787.2s

Traceback (most recent call last):
  File "run_vq.py", line 75, in <module>
    runner.learn(num_learning_iterations=10000)
  File "/home/jijingtian/project/MetaRobotics/OnlineAdaptation/runners/vq_onpolicy_runner.py", line 75, in learn
    ret = self.env.step(actions)
  File "/home/jijingtian/project/MetaRobotics/legged_gym/envs/wrapper/history_wrapper.py", line 22, in step
    obs,privileged_obs, rew, done, info = self.env.step(action)
  File "/home/jijingtian/project/MetaRobotics/legged_gym/envs/Go1/legged_robot.py", line 106, in step
    self.post_physics_step()
  File "/home/jijingtian/project/MetaRobotics/legged_gym/envs/Go1/legged_robot.py", line 151, in post_physics_step
    self.reset_idx(env_ids)
  File "/home/jijingtian/project/MetaRobotics/legged_gym/envs/Go1/legged_robot.py", line 209, in reset_idx
    self._call_train_eval(self.refresh_actor_rigid_shape_props, env_ids)
  File "/home/jijingtian/project/MetaRobotics/legged_gym/envs/Go1/legged_robot.py", line 577, in _call_train_eval
    ret = func(env_ids_train, self.cfg)
  File "/home/jijingtian/project/MetaRobotics/legged_gym/envs/Go1/legged_robot.py", line 679, in refresh_actor_rigid_shape_props
    rigid_shape_props[i].restitution = self.restitutions[env_id, 0]
KeyboardInterrupt
